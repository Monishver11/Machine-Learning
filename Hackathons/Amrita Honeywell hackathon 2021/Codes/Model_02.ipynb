{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "successful-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "editorial-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "from keras.layers import Dense,BatchNormalization,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras import regularizers\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "textile-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r'C:\\Users\\monis\\Desktop\\ML-Ang\\Datasets\\HoneyWell_Hackathon\\OneDrive_1_2-26-2021\\Voltage_L1_train.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\monis\\Desktop\\ML-Ang\\Datasets\\HoneyWell_Hackathon\\OneDrive_1_2-26-2021\\Voltage_L1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convenient-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = np.arange(len(df1.columns))\n",
    "df2.columns = np.arange(len(df2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "endangered-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5999, 256)\n",
      "(3599, 256)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>573.652486</td>\n",
       "      <td>1003.343736</td>\n",
       "      <td>1588.404525</td>\n",
       "      <td>2317.576741</td>\n",
       "      <td>2804.364311</td>\n",
       "      <td>3225.322510</td>\n",
       "      <td>3662.821690</td>\n",
       "      <td>4174.627969</td>\n",
       "      <td>4656.244143</td>\n",
       "      <td>4939.070130</td>\n",
       "      <td>...</td>\n",
       "      <td>-4650.282434</td>\n",
       "      <td>-4228.581226</td>\n",
       "      <td>-3865.609932</td>\n",
       "      <td>-3395.654756</td>\n",
       "      <td>-2933.680470</td>\n",
       "      <td>-2322.450904</td>\n",
       "      <td>-1841.562453</td>\n",
       "      <td>-1282.042025</td>\n",
       "      <td>-601.968217</td>\n",
       "      <td>-156.848367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4757.365183</td>\n",
       "      <td>5264.598912</td>\n",
       "      <td>5428.642486</td>\n",
       "      <td>5650.413073</td>\n",
       "      <td>5939.710012</td>\n",
       "      <td>5911.948067</td>\n",
       "      <td>6147.642171</td>\n",
       "      <td>6076.921501</td>\n",
       "      <td>5958.797444</td>\n",
       "      <td>6053.817701</td>\n",
       "      <td>...</td>\n",
       "      <td>-280.360872</td>\n",
       "      <td>323.325836</td>\n",
       "      <td>861.103019</td>\n",
       "      <td>1415.929276</td>\n",
       "      <td>2007.692919</td>\n",
       "      <td>2561.130303</td>\n",
       "      <td>2960.282598</td>\n",
       "      <td>3619.932691</td>\n",
       "      <td>4008.288701</td>\n",
       "      <td>4422.229911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4242.144824</td>\n",
       "      <td>4644.679402</td>\n",
       "      <td>5013.356532</td>\n",
       "      <td>5229.417051</td>\n",
       "      <td>5534.898007</td>\n",
       "      <td>5797.190678</td>\n",
       "      <td>5930.658682</td>\n",
       "      <td>5960.014599</td>\n",
       "      <td>6055.336310</td>\n",
       "      <td>6103.707793</td>\n",
       "      <td>...</td>\n",
       "      <td>-1256.270585</td>\n",
       "      <td>-616.527428</td>\n",
       "      <td>-67.068193</td>\n",
       "      <td>549.016676</td>\n",
       "      <td>1099.652199</td>\n",
       "      <td>1697.572166</td>\n",
       "      <td>2239.961604</td>\n",
       "      <td>2776.876479</td>\n",
       "      <td>3248.638662</td>\n",
       "      <td>3807.665149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2077.819247</td>\n",
       "      <td>2561.679246</td>\n",
       "      <td>3085.653813</td>\n",
       "      <td>3545.905160</td>\n",
       "      <td>4023.421592</td>\n",
       "      <td>4496.705157</td>\n",
       "      <td>4809.079868</td>\n",
       "      <td>5186.298840</td>\n",
       "      <td>5453.627533</td>\n",
       "      <td>5737.354699</td>\n",
       "      <td>...</td>\n",
       "      <td>-3557.345152</td>\n",
       "      <td>-3017.951179</td>\n",
       "      <td>-2596.647329</td>\n",
       "      <td>-1996.266675</td>\n",
       "      <td>-1467.203661</td>\n",
       "      <td>-885.101101</td>\n",
       "      <td>-329.685256</td>\n",
       "      <td>304.222722</td>\n",
       "      <td>935.528504</td>\n",
       "      <td>1460.127297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3599.645319</td>\n",
       "      <td>4099.944762</td>\n",
       "      <td>4499.282469</td>\n",
       "      <td>4897.875855</td>\n",
       "      <td>5120.077118</td>\n",
       "      <td>5402.227743</td>\n",
       "      <td>5694.801362</td>\n",
       "      <td>5928.683099</td>\n",
       "      <td>5981.616502</td>\n",
       "      <td>6052.006904</td>\n",
       "      <td>...</td>\n",
       "      <td>-2020.240712</td>\n",
       "      <td>-1388.704968</td>\n",
       "      <td>-849.731284</td>\n",
       "      <td>-232.632694</td>\n",
       "      <td>341.406093</td>\n",
       "      <td>854.579135</td>\n",
       "      <td>1528.023058</td>\n",
       "      <td>2002.557438</td>\n",
       "      <td>2576.468343</td>\n",
       "      <td>3036.303600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0            1            2            3            4    \\\n",
       "0   573.652486  1003.343736  1588.404525  2317.576741  2804.364311   \n",
       "1  4757.365183  5264.598912  5428.642486  5650.413073  5939.710012   \n",
       "2  4242.144824  4644.679402  5013.356532  5229.417051  5534.898007   \n",
       "3  2077.819247  2561.679246  3085.653813  3545.905160  4023.421592   \n",
       "4  3599.645319  4099.944762  4499.282469  4897.875855  5120.077118   \n",
       "\n",
       "           5            6            7            8            9    ...  \\\n",
       "0  3225.322510  3662.821690  4174.627969  4656.244143  4939.070130  ...   \n",
       "1  5911.948067  6147.642171  6076.921501  5958.797444  6053.817701  ...   \n",
       "2  5797.190678  5930.658682  5960.014599  6055.336310  6103.707793  ...   \n",
       "3  4496.705157  4809.079868  5186.298840  5453.627533  5737.354699  ...   \n",
       "4  5402.227743  5694.801362  5928.683099  5981.616502  6052.006904  ...   \n",
       "\n",
       "           246          247          248          249          250  \\\n",
       "0 -4650.282434 -4228.581226 -3865.609932 -3395.654756 -2933.680470   \n",
       "1  -280.360872   323.325836   861.103019  1415.929276  2007.692919   \n",
       "2 -1256.270585  -616.527428   -67.068193   549.016676  1099.652199   \n",
       "3 -3557.345152 -3017.951179 -2596.647329 -1996.266675 -1467.203661   \n",
       "4 -2020.240712 -1388.704968  -849.731284  -232.632694   341.406093   \n",
       "\n",
       "           251          252          253          254          255  \n",
       "0 -2322.450904 -1841.562453 -1282.042025  -601.968217  -156.848367  \n",
       "1  2561.130303  2960.282598  3619.932691  4008.288701  4422.229911  \n",
       "2  1697.572166  2239.961604  2776.876479  3248.638662  3807.665149  \n",
       "3  -885.101101  -329.685256   304.222722   935.528504  1460.127297  \n",
       "4   854.579135  1528.023058  2002.557438  2576.468343  3036.303600  \n",
       "\n",
       "[5 rows x 256 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dropna(how='all', inplace = True)\n",
    "#df2.dropna(how='all', inplace = True)\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developing-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(r'C:\\Users\\monis\\Desktop\\ML-Ang\\Datasets\\HoneyWell_Hackathon\\OneDrive_1_2-26-2021\\output_train.csv')\n",
    "t2 = pd.read_csv(r'C:\\Users\\monis\\Desktop\\ML-Ang\\Datasets\\HoneyWell_Hackathon\\OneDrive_1_2-26-2021\\output_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reverse-married",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5999, 1)\n",
      "(3599, 1)\n"
     ]
    }
   ],
   "source": [
    "t1.columns = ['target']\n",
    "t2.columns = ['target']\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "df1['target'] = t1\n",
    "df2['target'] = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polish-horizontal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2724.849156</td>\n",
       "      <td>3185.770469</td>\n",
       "      <td>3544.969353</td>\n",
       "      <td>3947.709357</td>\n",
       "      <td>4249.735481</td>\n",
       "      <td>4509.632122</td>\n",
       "      <td>4802.947871</td>\n",
       "      <td>5114.016305</td>\n",
       "      <td>5132.770284</td>\n",
       "      <td>5346.220080</td>\n",
       "      <td>...</td>\n",
       "      <td>-1945.342559</td>\n",
       "      <td>-1415.727240</td>\n",
       "      <td>-1023.829156</td>\n",
       "      <td>-327.571997</td>\n",
       "      <td>94.179788</td>\n",
       "      <td>614.550399</td>\n",
       "      <td>1088.761067</td>\n",
       "      <td>1662.400798</td>\n",
       "      <td>2074.986987</td>\n",
       "      <td>3.500417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1401.312059</td>\n",
       "      <td>1323.660192</td>\n",
       "      <td>1251.228291</td>\n",
       "      <td>1141.184078</td>\n",
       "      <td>1047.381443</td>\n",
       "      <td>942.996830</td>\n",
       "      <td>817.425047</td>\n",
       "      <td>676.144046</td>\n",
       "      <td>620.244368</td>\n",
       "      <td>472.550458</td>\n",
       "      <td>...</td>\n",
       "      <td>1487.105965</td>\n",
       "      <td>1536.695688</td>\n",
       "      <td>1564.588316</td>\n",
       "      <td>1590.101809</td>\n",
       "      <td>1593.744809</td>\n",
       "      <td>1591.564465</td>\n",
       "      <td>1564.580127</td>\n",
       "      <td>1526.235162</td>\n",
       "      <td>1476.272926</td>\n",
       "      <td>1.707805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.262860</td>\n",
       "      <td>385.980891</td>\n",
       "      <td>775.625243</td>\n",
       "      <td>1385.756620</td>\n",
       "      <td>1740.391212</td>\n",
       "      <td>2234.895090</td>\n",
       "      <td>2767.024727</td>\n",
       "      <td>3264.000188</td>\n",
       "      <td>3524.459163</td>\n",
       "      <td>3900.544325</td>\n",
       "      <td>...</td>\n",
       "      <td>-4629.714032</td>\n",
       "      <td>-4233.265710</td>\n",
       "      <td>-3792.399698</td>\n",
       "      <td>-3326.994658</td>\n",
       "      <td>-2820.922481</td>\n",
       "      <td>-2356.270922</td>\n",
       "      <td>-1892.557687</td>\n",
       "      <td>-1384.976676</td>\n",
       "      <td>-802.062281</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1557.738178</td>\n",
       "      <td>2107.644962</td>\n",
       "      <td>2532.216839</td>\n",
       "      <td>3049.940982</td>\n",
       "      <td>3443.401039</td>\n",
       "      <td>3797.774653</td>\n",
       "      <td>4210.798635</td>\n",
       "      <td>4628.580980</td>\n",
       "      <td>4744.722755</td>\n",
       "      <td>5044.231724</td>\n",
       "      <td>...</td>\n",
       "      <td>-3229.576837</td>\n",
       "      <td>-2749.029955</td>\n",
       "      <td>-2362.655083</td>\n",
       "      <td>-1683.959393</td>\n",
       "      <td>-1272.724943</td>\n",
       "      <td>-752.721400</td>\n",
       "      <td>-225.447922</td>\n",
       "      <td>377.091717</td>\n",
       "      <td>857.418113</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2826.465290</td>\n",
       "      <td>3293.945828</td>\n",
       "      <td>3666.253960</td>\n",
       "      <td>4093.769284</td>\n",
       "      <td>4387.454585</td>\n",
       "      <td>4654.817178</td>\n",
       "      <td>4951.354994</td>\n",
       "      <td>5225.126313</td>\n",
       "      <td>5147.138692</td>\n",
       "      <td>5382.547693</td>\n",
       "      <td>...</td>\n",
       "      <td>-2041.445383</td>\n",
       "      <td>-1475.021663</td>\n",
       "      <td>-1073.214096</td>\n",
       "      <td>-355.157286</td>\n",
       "      <td>73.283126</td>\n",
       "      <td>635.897381</td>\n",
       "      <td>1098.156111</td>\n",
       "      <td>1716.946597</td>\n",
       "      <td>2133.012880</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3963.225857</td>\n",
       "      <td>4355.132232</td>\n",
       "      <td>4620.156306</td>\n",
       "      <td>4929.741884</td>\n",
       "      <td>5102.596151</td>\n",
       "      <td>5232.110349</td>\n",
       "      <td>5386.120990</td>\n",
       "      <td>5627.831236</td>\n",
       "      <td>5599.086993</td>\n",
       "      <td>5680.702877</td>\n",
       "      <td>...</td>\n",
       "      <td>-670.709048</td>\n",
       "      <td>-90.623127</td>\n",
       "      <td>314.137602</td>\n",
       "      <td>1032.457475</td>\n",
       "      <td>1476.878236</td>\n",
       "      <td>1988.458274</td>\n",
       "      <td>2454.675182</td>\n",
       "      <td>2990.544060</td>\n",
       "      <td>3363.288496</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6411.869978</td>\n",
       "      <td>6091.892089</td>\n",
       "      <td>8200.995314</td>\n",
       "      <td>8037.622359</td>\n",
       "      <td>8208.441229</td>\n",
       "      <td>8303.706356</td>\n",
       "      <td>8573.907487</td>\n",
       "      <td>8738.820320</td>\n",
       "      <td>8838.097165</td>\n",
       "      <td>8421.712927</td>\n",
       "      <td>...</td>\n",
       "      <td>2095.599446</td>\n",
       "      <td>3408.422940</td>\n",
       "      <td>3997.133868</td>\n",
       "      <td>3974.105045</td>\n",
       "      <td>4222.552116</td>\n",
       "      <td>4169.650955</td>\n",
       "      <td>6525.291747</td>\n",
       "      <td>4804.683362</td>\n",
       "      <td>4893.099198</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2            3            4  \\\n",
       "count  5999.000000  5999.000000  5999.000000  5999.000000  5999.000000   \n",
       "mean   2724.849156  3185.770469  3544.969353  3947.709357  4249.735481   \n",
       "std    1401.312059  1323.660192  1251.228291  1141.184078  1047.381443   \n",
       "min       7.262860   385.980891   775.625243  1385.756620  1740.391212   \n",
       "25%    1557.738178  2107.644962  2532.216839  3049.940982  3443.401039   \n",
       "50%    2826.465290  3293.945828  3666.253960  4093.769284  4387.454585   \n",
       "75%    3963.225857  4355.132232  4620.156306  4929.741884  5102.596151   \n",
       "max    6411.869978  6091.892089  8200.995314  8037.622359  8208.441229   \n",
       "\n",
       "                 5            6            7            8            9  ...  \\\n",
       "count  5999.000000  5999.000000  5999.000000  5999.000000  5999.000000  ...   \n",
       "mean   4509.632122  4802.947871  5114.016305  5132.770284  5346.220080  ...   \n",
       "std     942.996830   817.425047   676.144046   620.244368   472.550458  ...   \n",
       "min    2234.895090  2767.024727  3264.000188  3524.459163  3900.544325  ...   \n",
       "25%    3797.774653  4210.798635  4628.580980  4744.722755  5044.231724  ...   \n",
       "50%    4654.817178  4951.354994  5225.126313  5147.138692  5382.547693  ...   \n",
       "75%    5232.110349  5386.120990  5627.831236  5599.086993  5680.702877  ...   \n",
       "max    8303.706356  8573.907487  8738.820320  8838.097165  8421.712927  ...   \n",
       "\n",
       "               247          248          249          250          251  \\\n",
       "count  5999.000000  5999.000000  5999.000000  5999.000000  5999.000000   \n",
       "mean  -1945.342559 -1415.727240 -1023.829156  -327.571997    94.179788   \n",
       "std    1487.105965  1536.695688  1564.588316  1590.101809  1593.744809   \n",
       "min   -4629.714032 -4233.265710 -3792.399698 -3326.994658 -2820.922481   \n",
       "25%   -3229.576837 -2749.029955 -2362.655083 -1683.959393 -1272.724943   \n",
       "50%   -2041.445383 -1475.021663 -1073.214096  -355.157286    73.283126   \n",
       "75%    -670.709048   -90.623127   314.137602  1032.457475  1476.878236   \n",
       "max    2095.599446  3408.422940  3997.133868  3974.105045  4222.552116   \n",
       "\n",
       "               252          253          254          255       target  \n",
       "count  5999.000000  5999.000000  5999.000000  5999.000000  5999.000000  \n",
       "mean    614.550399  1088.761067  1662.400798  2074.986987     3.500417  \n",
       "std    1591.564465  1564.580127  1526.235162  1476.272926     1.707805  \n",
       "min   -2356.270922 -1892.557687 -1384.976676  -802.062281     1.000000  \n",
       "25%    -752.721400  -225.447922   377.091717   857.418113     2.000000  \n",
       "50%     635.897381  1098.156111  1716.946597  2133.012880     4.000000  \n",
       "75%    1988.458274  2454.675182  2990.544060  3363.288496     5.000000  \n",
       "max    4169.650955  6525.291747  4804.683362  4893.099198     6.000000  \n",
       "\n",
       "[8 rows x 257 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "finnish-syndication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9598, 257)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>573.652486</td>\n",
       "      <td>1003.343736</td>\n",
       "      <td>1588.404525</td>\n",
       "      <td>2317.576741</td>\n",
       "      <td>2804.364311</td>\n",
       "      <td>3225.322510</td>\n",
       "      <td>3662.821690</td>\n",
       "      <td>4174.627969</td>\n",
       "      <td>4656.244143</td>\n",
       "      <td>4939.070130</td>\n",
       "      <td>...</td>\n",
       "      <td>-4228.581226</td>\n",
       "      <td>-3865.609932</td>\n",
       "      <td>-3395.654756</td>\n",
       "      <td>-2933.680470</td>\n",
       "      <td>-2322.450904</td>\n",
       "      <td>-1841.562453</td>\n",
       "      <td>-1282.042025</td>\n",
       "      <td>-601.968217</td>\n",
       "      <td>-156.848367</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4757.365183</td>\n",
       "      <td>5264.598912</td>\n",
       "      <td>5428.642486</td>\n",
       "      <td>5650.413073</td>\n",
       "      <td>5939.710012</td>\n",
       "      <td>5911.948067</td>\n",
       "      <td>6147.642171</td>\n",
       "      <td>6076.921501</td>\n",
       "      <td>5958.797444</td>\n",
       "      <td>6053.817701</td>\n",
       "      <td>...</td>\n",
       "      <td>323.325836</td>\n",
       "      <td>861.103019</td>\n",
       "      <td>1415.929276</td>\n",
       "      <td>2007.692919</td>\n",
       "      <td>2561.130303</td>\n",
       "      <td>2960.282598</td>\n",
       "      <td>3619.932691</td>\n",
       "      <td>4008.288701</td>\n",
       "      <td>4422.229911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4242.144824</td>\n",
       "      <td>4644.679402</td>\n",
       "      <td>5013.356532</td>\n",
       "      <td>5229.417051</td>\n",
       "      <td>5534.898007</td>\n",
       "      <td>5797.190678</td>\n",
       "      <td>5930.658682</td>\n",
       "      <td>5960.014599</td>\n",
       "      <td>6055.336310</td>\n",
       "      <td>6103.707793</td>\n",
       "      <td>...</td>\n",
       "      <td>-616.527428</td>\n",
       "      <td>-67.068193</td>\n",
       "      <td>549.016676</td>\n",
       "      <td>1099.652199</td>\n",
       "      <td>1697.572166</td>\n",
       "      <td>2239.961604</td>\n",
       "      <td>2776.876479</td>\n",
       "      <td>3248.638662</td>\n",
       "      <td>3807.665149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2077.819247</td>\n",
       "      <td>2561.679246</td>\n",
       "      <td>3085.653813</td>\n",
       "      <td>3545.905160</td>\n",
       "      <td>4023.421592</td>\n",
       "      <td>4496.705157</td>\n",
       "      <td>4809.079868</td>\n",
       "      <td>5186.298840</td>\n",
       "      <td>5453.627533</td>\n",
       "      <td>5737.354699</td>\n",
       "      <td>...</td>\n",
       "      <td>-3017.951179</td>\n",
       "      <td>-2596.647329</td>\n",
       "      <td>-1996.266675</td>\n",
       "      <td>-1467.203661</td>\n",
       "      <td>-885.101101</td>\n",
       "      <td>-329.685256</td>\n",
       "      <td>304.222722</td>\n",
       "      <td>935.528504</td>\n",
       "      <td>1460.127297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3599.645319</td>\n",
       "      <td>4099.944762</td>\n",
       "      <td>4499.282469</td>\n",
       "      <td>4897.875855</td>\n",
       "      <td>5120.077118</td>\n",
       "      <td>5402.227743</td>\n",
       "      <td>5694.801362</td>\n",
       "      <td>5928.683099</td>\n",
       "      <td>5981.616502</td>\n",
       "      <td>6052.006904</td>\n",
       "      <td>...</td>\n",
       "      <td>-1388.704968</td>\n",
       "      <td>-849.731284</td>\n",
       "      <td>-232.632694</td>\n",
       "      <td>341.406093</td>\n",
       "      <td>854.579135</td>\n",
       "      <td>1528.023058</td>\n",
       "      <td>2002.557438</td>\n",
       "      <td>2576.468343</td>\n",
       "      <td>3036.303600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1            2            3            4  \\\n",
       "0   573.652486  1003.343736  1588.404525  2317.576741  2804.364311   \n",
       "1  4757.365183  5264.598912  5428.642486  5650.413073  5939.710012   \n",
       "2  4242.144824  4644.679402  5013.356532  5229.417051  5534.898007   \n",
       "3  2077.819247  2561.679246  3085.653813  3545.905160  4023.421592   \n",
       "4  3599.645319  4099.944762  4499.282469  4897.875855  5120.077118   \n",
       "\n",
       "             5            6            7            8            9  ...  \\\n",
       "0  3225.322510  3662.821690  4174.627969  4656.244143  4939.070130  ...   \n",
       "1  5911.948067  6147.642171  6076.921501  5958.797444  6053.817701  ...   \n",
       "2  5797.190678  5930.658682  5960.014599  6055.336310  6103.707793  ...   \n",
       "3  4496.705157  4809.079868  5186.298840  5453.627533  5737.354699  ...   \n",
       "4  5402.227743  5694.801362  5928.683099  5981.616502  6052.006904  ...   \n",
       "\n",
       "           247          248          249          250          251  \\\n",
       "0 -4228.581226 -3865.609932 -3395.654756 -2933.680470 -2322.450904   \n",
       "1   323.325836   861.103019  1415.929276  2007.692919  2561.130303   \n",
       "2  -616.527428   -67.068193   549.016676  1099.652199  1697.572166   \n",
       "3 -3017.951179 -2596.647329 -1996.266675 -1467.203661  -885.101101   \n",
       "4 -1388.704968  -849.731284  -232.632694   341.406093   854.579135   \n",
       "\n",
       "           252          253          254          255  target  \n",
       "0 -1841.562453 -1282.042025  -601.968217  -156.848367       1  \n",
       "1  2960.282598  3619.932691  4008.288701  4422.229911       1  \n",
       "2  2239.961604  2776.876479  3248.638662  3807.665149       1  \n",
       "3  -329.685256   304.222722   935.528504  1460.127297       1  \n",
       "4  1528.023058  2002.557438  2576.468343  3036.303600       1  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df1.append(df2, ignore_index=True)\n",
    "class1 = df3[df3['target']==1]\n",
    "class2 = df3[df3['target']==2]\n",
    "class3 = df3[df3['target']==3]\n",
    "class4 = df3[df3['target']==4]\n",
    "class5 = df3[df3['target']==5]\n",
    "print(df3.shape)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acute-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df3.iloc[:,0:256]\n",
    "y = df3['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "boolean-result",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>573.652486</td>\n",
       "      <td>1003.343736</td>\n",
       "      <td>1588.404525</td>\n",
       "      <td>2317.576741</td>\n",
       "      <td>2804.364311</td>\n",
       "      <td>3225.322510</td>\n",
       "      <td>3662.821690</td>\n",
       "      <td>4174.627969</td>\n",
       "      <td>4656.244143</td>\n",
       "      <td>4939.070130</td>\n",
       "      <td>...</td>\n",
       "      <td>-4650.282434</td>\n",
       "      <td>-4228.581226</td>\n",
       "      <td>-3865.609932</td>\n",
       "      <td>-3395.654756</td>\n",
       "      <td>-2933.680470</td>\n",
       "      <td>-2322.450904</td>\n",
       "      <td>-1841.562453</td>\n",
       "      <td>-1282.042025</td>\n",
       "      <td>-601.968217</td>\n",
       "      <td>-156.848367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4757.365183</td>\n",
       "      <td>5264.598912</td>\n",
       "      <td>5428.642486</td>\n",
       "      <td>5650.413073</td>\n",
       "      <td>5939.710012</td>\n",
       "      <td>5911.948067</td>\n",
       "      <td>6147.642171</td>\n",
       "      <td>6076.921501</td>\n",
       "      <td>5958.797444</td>\n",
       "      <td>6053.817701</td>\n",
       "      <td>...</td>\n",
       "      <td>-280.360872</td>\n",
       "      <td>323.325836</td>\n",
       "      <td>861.103019</td>\n",
       "      <td>1415.929276</td>\n",
       "      <td>2007.692919</td>\n",
       "      <td>2561.130303</td>\n",
       "      <td>2960.282598</td>\n",
       "      <td>3619.932691</td>\n",
       "      <td>4008.288701</td>\n",
       "      <td>4422.229911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4242.144824</td>\n",
       "      <td>4644.679402</td>\n",
       "      <td>5013.356532</td>\n",
       "      <td>5229.417051</td>\n",
       "      <td>5534.898007</td>\n",
       "      <td>5797.190678</td>\n",
       "      <td>5930.658682</td>\n",
       "      <td>5960.014599</td>\n",
       "      <td>6055.336310</td>\n",
       "      <td>6103.707793</td>\n",
       "      <td>...</td>\n",
       "      <td>-1256.270585</td>\n",
       "      <td>-616.527428</td>\n",
       "      <td>-67.068193</td>\n",
       "      <td>549.016676</td>\n",
       "      <td>1099.652199</td>\n",
       "      <td>1697.572166</td>\n",
       "      <td>2239.961604</td>\n",
       "      <td>2776.876479</td>\n",
       "      <td>3248.638662</td>\n",
       "      <td>3807.665149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2077.819247</td>\n",
       "      <td>2561.679246</td>\n",
       "      <td>3085.653813</td>\n",
       "      <td>3545.905160</td>\n",
       "      <td>4023.421592</td>\n",
       "      <td>4496.705157</td>\n",
       "      <td>4809.079868</td>\n",
       "      <td>5186.298840</td>\n",
       "      <td>5453.627533</td>\n",
       "      <td>5737.354699</td>\n",
       "      <td>...</td>\n",
       "      <td>-3557.345152</td>\n",
       "      <td>-3017.951179</td>\n",
       "      <td>-2596.647329</td>\n",
       "      <td>-1996.266675</td>\n",
       "      <td>-1467.203661</td>\n",
       "      <td>-885.101101</td>\n",
       "      <td>-329.685256</td>\n",
       "      <td>304.222722</td>\n",
       "      <td>935.528504</td>\n",
       "      <td>1460.127297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3599.645319</td>\n",
       "      <td>4099.944762</td>\n",
       "      <td>4499.282469</td>\n",
       "      <td>4897.875855</td>\n",
       "      <td>5120.077118</td>\n",
       "      <td>5402.227743</td>\n",
       "      <td>5694.801362</td>\n",
       "      <td>5928.683099</td>\n",
       "      <td>5981.616502</td>\n",
       "      <td>6052.006904</td>\n",
       "      <td>...</td>\n",
       "      <td>-2020.240712</td>\n",
       "      <td>-1388.704968</td>\n",
       "      <td>-849.731284</td>\n",
       "      <td>-232.632694</td>\n",
       "      <td>341.406093</td>\n",
       "      <td>854.579135</td>\n",
       "      <td>1528.023058</td>\n",
       "      <td>2002.557438</td>\n",
       "      <td>2576.468343</td>\n",
       "      <td>3036.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9593</th>\n",
       "      <td>3295.219128</td>\n",
       "      <td>3512.821536</td>\n",
       "      <td>3983.990167</td>\n",
       "      <td>4371.573909</td>\n",
       "      <td>4571.739459</td>\n",
       "      <td>5017.246619</td>\n",
       "      <td>4914.430756</td>\n",
       "      <td>5433.849900</td>\n",
       "      <td>5146.884770</td>\n",
       "      <td>5651.071399</td>\n",
       "      <td>...</td>\n",
       "      <td>-1751.407093</td>\n",
       "      <td>-1558.771542</td>\n",
       "      <td>-644.706578</td>\n",
       "      <td>-529.887934</td>\n",
       "      <td>470.696999</td>\n",
       "      <td>473.316400</td>\n",
       "      <td>1483.781317</td>\n",
       "      <td>1587.883246</td>\n",
       "      <td>2458.081893</td>\n",
       "      <td>2576.241291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>3295.219128</td>\n",
       "      <td>3512.821536</td>\n",
       "      <td>3983.990167</td>\n",
       "      <td>4371.573909</td>\n",
       "      <td>4571.739459</td>\n",
       "      <td>5017.246619</td>\n",
       "      <td>4914.430756</td>\n",
       "      <td>5433.849900</td>\n",
       "      <td>5146.884770</td>\n",
       "      <td>5651.071399</td>\n",
       "      <td>...</td>\n",
       "      <td>-1751.407093</td>\n",
       "      <td>-1558.771542</td>\n",
       "      <td>-644.706578</td>\n",
       "      <td>-529.887934</td>\n",
       "      <td>470.696999</td>\n",
       "      <td>473.316400</td>\n",
       "      <td>1483.781317</td>\n",
       "      <td>1587.883246</td>\n",
       "      <td>2458.081893</td>\n",
       "      <td>2576.241291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>3295.219128</td>\n",
       "      <td>3512.821536</td>\n",
       "      <td>3983.990167</td>\n",
       "      <td>4371.573909</td>\n",
       "      <td>4571.739459</td>\n",
       "      <td>5017.246619</td>\n",
       "      <td>4914.430756</td>\n",
       "      <td>5433.849900</td>\n",
       "      <td>5146.884770</td>\n",
       "      <td>5651.071399</td>\n",
       "      <td>...</td>\n",
       "      <td>-1751.407093</td>\n",
       "      <td>-1558.771542</td>\n",
       "      <td>-644.706578</td>\n",
       "      <td>-529.887934</td>\n",
       "      <td>470.696999</td>\n",
       "      <td>473.316400</td>\n",
       "      <td>1483.781317</td>\n",
       "      <td>1587.883246</td>\n",
       "      <td>2458.081893</td>\n",
       "      <td>2576.241291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9596</th>\n",
       "      <td>3295.219128</td>\n",
       "      <td>3512.821536</td>\n",
       "      <td>3983.990167</td>\n",
       "      <td>4371.573909</td>\n",
       "      <td>4571.739459</td>\n",
       "      <td>5017.246619</td>\n",
       "      <td>4914.430756</td>\n",
       "      <td>5433.849900</td>\n",
       "      <td>5146.884770</td>\n",
       "      <td>5651.071399</td>\n",
       "      <td>...</td>\n",
       "      <td>-1751.407093</td>\n",
       "      <td>-1558.771542</td>\n",
       "      <td>-644.706578</td>\n",
       "      <td>-529.887934</td>\n",
       "      <td>470.696999</td>\n",
       "      <td>473.316400</td>\n",
       "      <td>1483.781317</td>\n",
       "      <td>1587.883246</td>\n",
       "      <td>2458.081893</td>\n",
       "      <td>2576.241291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>3295.219128</td>\n",
       "      <td>3512.821536</td>\n",
       "      <td>3983.990167</td>\n",
       "      <td>4371.573909</td>\n",
       "      <td>4571.739459</td>\n",
       "      <td>5017.246619</td>\n",
       "      <td>4914.430756</td>\n",
       "      <td>5433.849900</td>\n",
       "      <td>5146.884770</td>\n",
       "      <td>5651.071399</td>\n",
       "      <td>...</td>\n",
       "      <td>-1751.407093</td>\n",
       "      <td>-1558.771542</td>\n",
       "      <td>-644.706578</td>\n",
       "      <td>-529.887934</td>\n",
       "      <td>470.696999</td>\n",
       "      <td>473.316400</td>\n",
       "      <td>1483.781317</td>\n",
       "      <td>1587.883246</td>\n",
       "      <td>2458.081893</td>\n",
       "      <td>2576.241291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9598 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1            2            3            4    \\\n",
       "0      573.652486  1003.343736  1588.404525  2317.576741  2804.364311   \n",
       "1     4757.365183  5264.598912  5428.642486  5650.413073  5939.710012   \n",
       "2     4242.144824  4644.679402  5013.356532  5229.417051  5534.898007   \n",
       "3     2077.819247  2561.679246  3085.653813  3545.905160  4023.421592   \n",
       "4     3599.645319  4099.944762  4499.282469  4897.875855  5120.077118   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "9593  3295.219128  3512.821536  3983.990167  4371.573909  4571.739459   \n",
       "9594  3295.219128  3512.821536  3983.990167  4371.573909  4571.739459   \n",
       "9595  3295.219128  3512.821536  3983.990167  4371.573909  4571.739459   \n",
       "9596  3295.219128  3512.821536  3983.990167  4371.573909  4571.739459   \n",
       "9597  3295.219128  3512.821536  3983.990167  4371.573909  4571.739459   \n",
       "\n",
       "              5            6            7            8            9    ...  \\\n",
       "0     3225.322510  3662.821690  4174.627969  4656.244143  4939.070130  ...   \n",
       "1     5911.948067  6147.642171  6076.921501  5958.797444  6053.817701  ...   \n",
       "2     5797.190678  5930.658682  5960.014599  6055.336310  6103.707793  ...   \n",
       "3     4496.705157  4809.079868  5186.298840  5453.627533  5737.354699  ...   \n",
       "4     5402.227743  5694.801362  5928.683099  5981.616502  6052.006904  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "9593  5017.246619  4914.430756  5433.849900  5146.884770  5651.071399  ...   \n",
       "9594  5017.246619  4914.430756  5433.849900  5146.884770  5651.071399  ...   \n",
       "9595  5017.246619  4914.430756  5433.849900  5146.884770  5651.071399  ...   \n",
       "9596  5017.246619  4914.430756  5433.849900  5146.884770  5651.071399  ...   \n",
       "9597  5017.246619  4914.430756  5433.849900  5146.884770  5651.071399  ...   \n",
       "\n",
       "              246          247          248          249          250  \\\n",
       "0    -4650.282434 -4228.581226 -3865.609932 -3395.654756 -2933.680470   \n",
       "1     -280.360872   323.325836   861.103019  1415.929276  2007.692919   \n",
       "2    -1256.270585  -616.527428   -67.068193   549.016676  1099.652199   \n",
       "3    -3557.345152 -3017.951179 -2596.647329 -1996.266675 -1467.203661   \n",
       "4    -2020.240712 -1388.704968  -849.731284  -232.632694   341.406093   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "9593 -1751.407093 -1558.771542  -644.706578  -529.887934   470.696999   \n",
       "9594 -1751.407093 -1558.771542  -644.706578  -529.887934   470.696999   \n",
       "9595 -1751.407093 -1558.771542  -644.706578  -529.887934   470.696999   \n",
       "9596 -1751.407093 -1558.771542  -644.706578  -529.887934   470.696999   \n",
       "9597 -1751.407093 -1558.771542  -644.706578  -529.887934   470.696999   \n",
       "\n",
       "              251          252          253          254          255  \n",
       "0    -2322.450904 -1841.562453 -1282.042025  -601.968217  -156.848367  \n",
       "1     2561.130303  2960.282598  3619.932691  4008.288701  4422.229911  \n",
       "2     1697.572166  2239.961604  2776.876479  3248.638662  3807.665149  \n",
       "3     -885.101101  -329.685256   304.222722   935.528504  1460.127297  \n",
       "4      854.579135  1528.023058  2002.557438  2576.468343  3036.303600  \n",
       "...           ...          ...          ...          ...          ...  \n",
       "9593   473.316400  1483.781317  1587.883246  2458.081893  2576.241291  \n",
       "9594   473.316400  1483.781317  1587.883246  2458.081893  2576.241291  \n",
       "9595   473.316400  1483.781317  1587.883246  2458.081893  2576.241291  \n",
       "9596   473.316400  1483.781317  1587.883246  2458.081893  2576.241291  \n",
       "9597   473.316400  1483.781317  1587.883246  2458.081893  2576.241291  \n",
       "\n",
       "[9598 rows x 256 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "attended-aggregate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9593</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9598 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1  2  3  4  5  6\n",
       "0     1  0  0  0  0  0\n",
       "1     1  0  0  0  0  0\n",
       "2     1  0  0  0  0  0\n",
       "3     1  0  0  0  0  0\n",
       "4     1  0  0  0  0  0\n",
       "...  .. .. .. .. .. ..\n",
       "9593  0  0  0  0  0  1\n",
       "9594  0  0  0  0  0  1\n",
       "9595  0  0  0  0  0  1\n",
       "9596  0  0  0  0  0  1\n",
       "9597  0  0  0  0  0  1\n",
       "\n",
       "[9598 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.get_dummies(y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "happy-maldives",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.38)\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.38)\n",
    "\n",
    "num_classes = len(np.unique(y_t))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "alike-cemetery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1598, 257)\n",
      "(1600, 257)\n",
      "(1600, 257)\n",
      "(1600, 257)\n",
      "(1600, 257)\n"
     ]
    }
   ],
   "source": [
    "print(class1.shape)\n",
    "print(class2.shape)\n",
    "print(class3.shape)\n",
    "print(class4.shape)\n",
    "print(class5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adjusted-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7920453798618684\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.65       229\n",
      "           1       1.00      1.00      1.00       265\n",
      "           2       1.00      0.04      0.08       252\n",
      "           3       1.00      1.00      1.00       245\n",
      "           4       1.00      1.00      1.00       268\n",
      "           5       1.00      1.00      1.00       241\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1500\n",
      "   macro avg       0.91      0.84      0.79      1500\n",
      "weighted avg       0.92      0.84      0.79      1500\n",
      " samples avg       0.84      0.84      0.84      1500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9041067734026035"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3,  leaf_size=30)\n",
    "knn.fit(X_train,y_train)\n",
    "y_knn = knn.predict(X_test)\n",
    "print(f1_score(y_test,y_knn, average='weighted'))\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "blind-assumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(knn, 'knn.pkl') \n",
    "  \n",
    "# Load the model from the file \n",
    "knn_from_joblib = joblib.load('knn.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "knn_from_joblib.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "headed-craps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8941087541905807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.630     0.892     0.739       241\n",
      "           2      1.000     1.000     1.000       264\n",
      "           3      0.816     0.477     0.602       241\n",
      "           4      1.000     1.000     1.000       245\n",
      "           5      1.000     1.000     1.000       250\n",
      "           6      1.000     1.000     1.000       259\n",
      "\n",
      "    accuracy                          0.899      1500\n",
      "   macro avg      0.908     0.895     0.890      1500\n",
      "weighted avg      0.911     0.899     0.894      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_tr, y_tr)\n",
    "y_pred = classifier.predict(X_t)\n",
    "print(f1_score(y_t,y_pred, average='weighted'))\n",
    "\n",
    "#print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_t, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adapted-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:38:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9490660737625177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.812     0.916     0.861       511\n",
      "           2      0.985     0.939     0.962       495\n",
      "           3      0.966     0.949     0.957       719\n",
      "           4      1.000     1.000     1.000       497\n",
      "           5      0.977     0.941     0.959       778\n",
      "\n",
      "    accuracy                          0.948      3000\n",
      "   macro avg      0.948     0.949     0.948      3000\n",
      "weighted avg      0.952     0.948     0.949      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbcl = XGBClassifier(n_estimators=100)\n",
    "xgbcl.fit(X_tr,y_tr)\n",
    "y_xgbcl = xgbcl.predict(X_t)\n",
    "print(f1_score(y_t,y_xgbcl, average='weighted'))\n",
    "print(metrics.classification_report(y_t, y_xgbcl, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "engaging-latino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9490660737625177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.812     0.916     0.861       511\n",
      "           2      0.985     0.939     0.962       495\n",
      "           3      0.966     0.949     0.957       719\n",
      "           4      1.000     1.000     1.000       497\n",
      "           5      0.977     0.941     0.959       778\n",
      "\n",
      "    accuracy                          0.948      3000\n",
      "   macro avg      0.948     0.949     0.948      3000\n",
      "weighted avg      0.952     0.948     0.949      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)\n",
    "bagging.fit(X_tr,y_tr)\n",
    "y_bagging = xgbcl.predict(X_t)\n",
    "print(f1_score(y_t,y_bagging, average='weighted'))\n",
    "print(metrics.classification_report(y_t, y_bagging, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "minimal-danger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:54:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:54:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:55:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:56:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:57:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:57:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:58:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:59:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:59:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "result1 = cross_val_score(knn, X_tr, y_tr, cv=kfold)\n",
    "result2 = cross_val_score(classifier, X_tr, y_tr, cv=kfold)\n",
    "result3 = cross_val_score(xgbcl, X_tr, y_tr, cv=kfold)\n",
    "result4 = cross_val_score(bagging, X_tr, y_tr, cv=kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adjacent-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Knn Model =  0.9594336917562722\n",
      "Accuracy of Random forest Model =  0.914979977753059\n",
      "Accuracy of XGBoost Machine =  0.9459857866765542\n",
      "Accuracy of Bagging Model =  0.8534107032505253\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Knn Model = ',result1.mean())\n",
    "print('Accuracy of Random forest Model = ',result2.mean())\n",
    "print('Accuracy of XGBoost Machine = ',result3.mean())\n",
    "print('Accuracy of Bagging Model = ',result4.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "current-symbol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9353778770968793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.751     0.978     0.850       511\n",
      "           2      0.983     0.958     0.970       495\n",
      "           3      0.965     0.951     0.958       719\n",
      "           4      1.000     1.000     1.000       497\n",
      "           5      1.000     0.830     0.907       778\n",
      "\n",
      "    accuracy                          0.934      3000\n",
      "   macro avg      0.940     0.944     0.937      3000\n",
      "weighted avg      0.946     0.934     0.935      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting = VotingClassifier(estimators=[('knn', knn), ('rf', classifier)], voting='hard')\n",
    "voting.fit(X_tr,y_tr)\n",
    "voting.score(X_t,y_t)\n",
    "y_voting = voting.predict(X_t)\n",
    "print(f1_score(y_t,y_voting, average='weighted'))\n",
    "print(metrics.classification_report(y_t, y_voting, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "opposite-stockholm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2 0.8 0.  0.  0. ]\n",
      " [0.8 0.  0.1 0.  0.1]\n",
      " [0.2 0.  0.1 0.  0.7]\n",
      " ...\n",
      " [0.  0.  0.  0.  1. ]\n",
      " [0.7 0.  0.  0.  0.3]\n",
      " [0.1 0.6 0.  0.  0.3]]\n",
      "[[5.2918389e-02 9.2733544e-01 3.0210258e-03 1.8357152e-05 1.6706713e-02]\n",
      " [6.1440605e-01 7.2666824e-02 1.6325118e-01 1.1985912e-04 1.4955607e-01]\n",
      " [1.5935019e-01 4.7047704e-02 2.4946311e-02 7.9605769e-04 7.6785976e-01]\n",
      " ...\n",
      " [1.1475366e-01 7.7727372e-03 9.2759272e-03 6.1754734e-05 8.6813593e-01]\n",
      " [2.7694082e-02 2.3306408e-03 9.3538798e-03 6.9113834e-05 9.6055233e-01]\n",
      " [2.1015362e-01 6.6948646e-01 1.4592224e-02 2.6967813e-04 1.0549800e-01]]\n"
     ]
    }
   ],
   "source": [
    "#pred1=knn.predict_proba(X_t)\n",
    "pred2=classifier.predict_proba(X_t)\n",
    "pred3=xgbcl.predict_proba(X_t)\n",
    "\n",
    "#print(pred1)\n",
    "print(pred2)\n",
    "print(pred3)\n",
    "\n",
    "finalpred=(pred2+pred3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "asian-sterling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 14,742\n",
      "Trainable params: 14,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(32,input_dim = X.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(64,activation='relu'))\n",
    "NN_model.add(Dense(16,activation='relu'))\n",
    "NN_model.add(Dense(64 ,activation='relu'))\n",
    "NN_model.add(Dense(32,kernel_regularizer=keras.regularizers.l2() ,activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "# Compile the network :\n",
    "sgd = keras.optimizers.SGD(lr=0.01)\n",
    "NN_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "distinguished-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_THRESHOLD = 0.99\n",
    "class myCallback(tf.keras.callbacks.Callback): \n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "      if(logs.get('accuracy') > ACCURACY_THRESHOLD):     \n",
    "        print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))   \n",
    "        self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "concrete-shoot",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.2355 - accuracy: 0.8666 - val_loss: 0.2022 - val_accuracy: 0.9076\n",
      "Epoch 2/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2416 - accuracy: 0.8655 - val_loss: 0.2076 - val_accuracy: 0.8999\n",
      "Epoch 3/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2337 - accuracy: 0.8687 - val_loss: 0.2388 - val_accuracy: 0.8712\n",
      "Epoch 4/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2367 - accuracy: 0.8677 - val_loss: 0.2086 - val_accuracy: 0.8953\n",
      "Epoch 5/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2358 - accuracy: 0.8704 - val_loss: 0.2258 - val_accuracy: 0.8868\n",
      "Epoch 6/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2340 - accuracy: 0.8743 - val_loss: 0.2257 - val_accuracy: 0.8742\n",
      "Epoch 7/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2349 - accuracy: 0.8728 - val_loss: 0.2773 - val_accuracy: 0.8388\n",
      "Epoch 8/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2297 - accuracy: 0.8692 - val_loss: 0.2295 - val_accuracy: 0.8481\n",
      "Epoch 9/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2341 - accuracy: 0.8682 - val_loss: 0.2068 - val_accuracy: 0.8895\n",
      "Epoch 10/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2239 - accuracy: 0.8815 - val_loss: 0.2173 - val_accuracy: 0.8797\n",
      "Epoch 11/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.2309 - accuracy: 0.8775 - val_loss: 0.2247 - val_accuracy: 0.8322\n",
      "Epoch 12/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.2387 - accuracy: 0.8738 - val_loss: 0.2475 - val_accuracy: 0.8558\n",
      "Epoch 13/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.2179 - accuracy: 0.8820 - val_loss: 0.2837 - val_accuracy: 0.8196\n",
      "Epoch 14/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2354 - accuracy: 0.8666 - val_loss: 0.2505 - val_accuracy: 0.8555\n",
      "Epoch 15/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2232 - accuracy: 0.8835 - val_loss: 0.2381 - val_accuracy: 0.8651\n",
      "Epoch 16/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2253 - accuracy: 0.8793 - val_loss: 0.2357 - val_accuracy: 0.8692\n",
      "Epoch 17/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2338 - accuracy: 0.8731 - val_loss: 0.1971 - val_accuracy: 0.9073\n",
      "Epoch 18/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.2140 - accuracy: 0.8886 - val_loss: 0.1970 - val_accuracy: 0.9038\n",
      "Epoch 19/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2201 - accuracy: 0.8792 - val_loss: 0.2068 - val_accuracy: 0.8865\n",
      "Epoch 20/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2207 - accuracy: 0.8812 - val_loss: 0.1935 - val_accuracy: 0.9084\n",
      "Epoch 21/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.2220 - accuracy: 0.8807 - val_loss: 0.2059 - val_accuracy: 0.8890\n",
      "Epoch 22/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.2152 - accuracy: 0.8835 - val_loss: 0.3918 - val_accuracy: 0.8322\n",
      "Epoch 23/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2234 - accuracy: 0.8822 - val_loss: 0.1911 - val_accuracy: 0.9043\n",
      "Epoch 24/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2145 - accuracy: 0.8850 - val_loss: 0.2057 - val_accuracy: 0.8725\n",
      "Epoch 25/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2195 - accuracy: 0.8842 - val_loss: 0.1853 - val_accuracy: 0.9202\n",
      "Epoch 26/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2112 - accuracy: 0.8879 - val_loss: 0.2198 - val_accuracy: 0.8755\n",
      "Epoch 27/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.2098 - accuracy: 0.8864 - val_loss: 0.2223 - val_accuracy: 0.8558\n",
      "Epoch 28/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.2136 - accuracy: 0.8852 - val_loss: 0.2272 - val_accuracy: 0.8698\n",
      "Epoch 29/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2143 - accuracy: 0.8861 - val_loss: 0.2027 - val_accuracy: 0.8923\n",
      "Epoch 30/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.2165 - accuracy: 0.8849 - val_loss: 0.1882 - val_accuracy: 0.9065\n",
      "Epoch 31/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.2210 - accuracy: 0.8862 - val_loss: 0.1912 - val_accuracy: 0.9073\n",
      "Epoch 32/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2088 - accuracy: 0.8903 - val_loss: 0.2006 - val_accuracy: 0.8997\n",
      "Epoch 33/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2093 - accuracy: 0.8892 - val_loss: 0.1995 - val_accuracy: 0.8893\n",
      "Epoch 34/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2089 - accuracy: 0.8914 - val_loss: 0.1936 - val_accuracy: 0.8865\n",
      "Epoch 35/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2046 - accuracy: 0.8909 - val_loss: 0.1960 - val_accuracy: 0.8857\n",
      "Epoch 36/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2227 - accuracy: 0.8807 - val_loss: 0.1761 - val_accuracy: 0.9243\n",
      "Epoch 37/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2090 - accuracy: 0.8901 - val_loss: 0.1859 - val_accuracy: 0.9087\n",
      "Epoch 38/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2060 - accuracy: 0.8859 - val_loss: 0.1844 - val_accuracy: 0.9087\n",
      "Epoch 39/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2023 - accuracy: 0.8950 - val_loss: 0.1980 - val_accuracy: 0.8914\n",
      "Epoch 40/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.2047 - accuracy: 0.8923 - val_loss: 0.1819 - val_accuracy: 0.9115\n",
      "Epoch 41/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2087 - accuracy: 0.8830 - val_loss: 0.2051 - val_accuracy: 0.8969\n",
      "Epoch 42/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2121 - accuracy: 0.8908 - val_loss: 0.2086 - val_accuracy: 0.8813\n",
      "Epoch 43/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.2061 - accuracy: 0.8840 - val_loss: 0.1926 - val_accuracy: 0.8961\n",
      "Epoch 44/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1971 - accuracy: 0.9010 - val_loss: 0.1964 - val_accuracy: 0.8964\n",
      "Epoch 45/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2016 - accuracy: 0.8908 - val_loss: 0.2146 - val_accuracy: 0.8766\n",
      "Epoch 46/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2064 - accuracy: 0.8903 - val_loss: 0.1833 - val_accuracy: 0.9115\n",
      "Epoch 47/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2022 - accuracy: 0.8971 - val_loss: 0.2112 - val_accuracy: 0.8649\n",
      "Epoch 48/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2113 - accuracy: 0.8913 - val_loss: 0.1851 - val_accuracy: 0.9120\n",
      "Epoch 49/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2017 - accuracy: 0.8918 - val_loss: 0.2067 - val_accuracy: 0.8654\n",
      "Epoch 50/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2002 - accuracy: 0.8985 - val_loss: 0.1735 - val_accuracy: 0.9158\n",
      "Epoch 51/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2033 - accuracy: 0.8923 - val_loss: 0.3929 - val_accuracy: 0.6828\n",
      "Epoch 52/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1986 - accuracy: 0.8983 - val_loss: 0.2398 - val_accuracy: 0.8369\n",
      "Epoch 53/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2018 - accuracy: 0.8946 - val_loss: 0.1861 - val_accuracy: 0.9035\n",
      "Epoch 54/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2042 - accuracy: 0.8939 - val_loss: 0.1917 - val_accuracy: 0.8958\n",
      "Epoch 55/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2068 - accuracy: 0.8889 - val_loss: 0.1766 - val_accuracy: 0.9147\n",
      "Epoch 56/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2174 - accuracy: 0.8943 - val_loss: 0.1805 - val_accuracy: 0.9095\n",
      "Epoch 57/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1833 - accuracy: 0.9096 - val_loss: 0.1852 - val_accuracy: 0.9043\n",
      "Epoch 58/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2085 - accuracy: 0.8995 - val_loss: 0.1969 - val_accuracy: 0.8953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2004 - accuracy: 0.8919 - val_loss: 0.1875 - val_accuracy: 0.9008\n",
      "Epoch 60/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1948 - accuracy: 0.8988 - val_loss: 0.1867 - val_accuracy: 0.9041\n",
      "Epoch 61/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1940 - accuracy: 0.8990 - val_loss: 0.3358 - val_accuracy: 0.8440\n",
      "Epoch 62/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1976 - accuracy: 0.8906 - val_loss: 0.2449 - val_accuracy: 0.8654\n",
      "Epoch 63/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2046 - accuracy: 0.8968 - val_loss: 0.1814 - val_accuracy: 0.9073\n",
      "Epoch 64/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1964 - accuracy: 0.8990 - val_loss: 0.2080 - val_accuracy: 0.8835\n",
      "Epoch 65/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2023 - accuracy: 0.8970 - val_loss: 0.1889 - val_accuracy: 0.9030\n",
      "Epoch 66/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1975 - accuracy: 0.8929 - val_loss: 0.1882 - val_accuracy: 0.9030\n",
      "Epoch 67/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2021 - accuracy: 0.8956 - val_loss: 0.1705 - val_accuracy: 0.9227\n",
      "Epoch 68/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2014 - accuracy: 0.8976 - val_loss: 0.1860 - val_accuracy: 0.9032\n",
      "Epoch 69/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1950 - accuracy: 0.8992 - val_loss: 0.2353 - val_accuracy: 0.8750\n",
      "Epoch 70/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1905 - accuracy: 0.9029 - val_loss: 0.1976 - val_accuracy: 0.8898\n",
      "Epoch 71/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1939 - accuracy: 0.9018 - val_loss: 0.1810 - val_accuracy: 0.9131\n",
      "Epoch 72/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1973 - accuracy: 0.8968 - val_loss: 0.1846 - val_accuracy: 0.9041\n",
      "Epoch 73/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1912 - accuracy: 0.9022 - val_loss: 0.2559 - val_accuracy: 0.8218\n",
      "Epoch 74/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1957 - accuracy: 0.8993 - val_loss: 0.1917 - val_accuracy: 0.8958\n",
      "Epoch 75/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2030 - accuracy: 0.8909 - val_loss: 0.1766 - val_accuracy: 0.9115\n",
      "Epoch 76/500\n",
      "93/93 [==============================] - 1s 11ms/step - loss: 0.1919 - accuracy: 0.9000 - val_loss: 0.1753 - val_accuracy: 0.9241\n",
      "Epoch 77/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1969 - accuracy: 0.8990 - val_loss: 0.2089 - val_accuracy: 0.8813\n",
      "Epoch 78/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1925 - accuracy: 0.8983 - val_loss: 0.1829 - val_accuracy: 0.9046\n",
      "Epoch 79/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1927 - accuracy: 0.8992 - val_loss: 0.1936 - val_accuracy: 0.9005\n",
      "Epoch 80/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1939 - accuracy: 0.8982 - val_loss: 0.1707 - val_accuracy: 0.9219\n",
      "Epoch 81/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1906 - accuracy: 0.9032 - val_loss: 0.1631 - val_accuracy: 0.9276\n",
      "Epoch 82/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1925 - accuracy: 0.9010 - val_loss: 0.1892 - val_accuracy: 0.8972\n",
      "Epoch 83/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1920 - accuracy: 0.9035 - val_loss: 0.1718 - val_accuracy: 0.9213\n",
      "Epoch 84/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1995 - accuracy: 0.9000 - val_loss: 0.1857 - val_accuracy: 0.9002\n",
      "Epoch 85/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1839 - accuracy: 0.9055 - val_loss: 0.1808 - val_accuracy: 0.9084\n",
      "Epoch 86/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2046 - accuracy: 0.8976 - val_loss: 0.2086 - val_accuracy: 0.8840\n",
      "Epoch 87/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1901 - accuracy: 0.9035 - val_loss: 0.1671 - val_accuracy: 0.9153\n",
      "Epoch 88/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1884 - accuracy: 0.9030 - val_loss: 0.1707 - val_accuracy: 0.9194\n",
      "Epoch 89/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1877 - accuracy: 0.9042 - val_loss: 0.1645 - val_accuracy: 0.9230\n",
      "Epoch 90/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1895 - accuracy: 0.9035 - val_loss: 0.3170 - val_accuracy: 0.8133\n",
      "Epoch 91/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1878 - accuracy: 0.9062 - val_loss: 0.1657 - val_accuracy: 0.9312\n",
      "Epoch 92/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1856 - accuracy: 0.9027 - val_loss: 0.1984 - val_accuracy: 0.8934\n",
      "Epoch 93/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1901 - accuracy: 0.8997 - val_loss: 0.1749 - val_accuracy: 0.9071\n",
      "Epoch 94/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1911 - accuracy: 0.9013 - val_loss: 0.1726 - val_accuracy: 0.9120\n",
      "Epoch 95/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1884 - accuracy: 0.9042 - val_loss: 0.1686 - val_accuracy: 0.9153\n",
      "Epoch 96/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1951 - accuracy: 0.8965 - val_loss: 0.2046 - val_accuracy: 0.8695\n",
      "Epoch 97/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1975 - accuracy: 0.9030 - val_loss: 0.1698 - val_accuracy: 0.9194\n",
      "Epoch 98/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1835 - accuracy: 0.9064 - val_loss: 0.1678 - val_accuracy: 0.9235\n",
      "Epoch 99/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1869 - accuracy: 0.9024 - val_loss: 0.2003 - val_accuracy: 0.8821\n",
      "Epoch 100/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1916 - accuracy: 0.9047 - val_loss: 0.1852 - val_accuracy: 0.9054\n",
      "Epoch 101/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1832 - accuracy: 0.9072 - val_loss: 0.1817 - val_accuracy: 0.9032\n",
      "Epoch 102/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1931 - accuracy: 0.9109 - val_loss: 0.2143 - val_accuracy: 0.8862\n",
      "Epoch 103/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1882 - accuracy: 0.9066 - val_loss: 0.1670 - val_accuracy: 0.9158\n",
      "Epoch 104/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1850 - accuracy: 0.9025 - val_loss: 0.1692 - val_accuracy: 0.9200\n",
      "Epoch 105/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1820 - accuracy: 0.9121 - val_loss: 0.1971 - val_accuracy: 0.9005\n",
      "Epoch 106/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1890 - accuracy: 0.9042 - val_loss: 0.1677 - val_accuracy: 0.9235\n",
      "Epoch 107/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1894 - accuracy: 0.9020 - val_loss: 0.1813 - val_accuracy: 0.9062\n",
      "Epoch 108/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.2078 - accuracy: 0.9062 - val_loss: 0.1735 - val_accuracy: 0.9158\n",
      "Epoch 109/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1816 - accuracy: 0.9091 - val_loss: 0.1710 - val_accuracy: 0.9153\n",
      "Epoch 110/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1896 - accuracy: 0.9037 - val_loss: 0.1841 - val_accuracy: 0.9062\n",
      "Epoch 111/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1862 - accuracy: 0.9071 - val_loss: 0.1656 - val_accuracy: 0.9202\n",
      "Epoch 112/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1808 - accuracy: 0.9092 - val_loss: 0.2854 - val_accuracy: 0.8243\n",
      "Epoch 113/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1819 - accuracy: 0.9082 - val_loss: 0.2294 - val_accuracy: 0.8613\n",
      "Epoch 114/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1901 - accuracy: 0.9062 - val_loss: 0.1628 - val_accuracy: 0.9186\n",
      "Epoch 115/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1757 - accuracy: 0.9145 - val_loss: 0.1603 - val_accuracy: 0.9232\n",
      "Epoch 116/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1828 - accuracy: 0.9092 - val_loss: 0.1758 - val_accuracy: 0.9109\n",
      "Epoch 117/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1908 - accuracy: 0.9069 - val_loss: 0.1904 - val_accuracy: 0.9008\n",
      "Epoch 118/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1770 - accuracy: 0.9113 - val_loss: 0.1832 - val_accuracy: 0.9046\n",
      "Epoch 119/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1812 - accuracy: 0.9062 - val_loss: 0.1628 - val_accuracy: 0.9183\n",
      "Epoch 120/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1875 - accuracy: 0.9087 - val_loss: 0.1729 - val_accuracy: 0.9164\n",
      "Epoch 121/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1840 - accuracy: 0.9134 - val_loss: 0.1810 - val_accuracy: 0.9041\n",
      "Epoch 122/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1843 - accuracy: 0.9145 - val_loss: 0.1676 - val_accuracy: 0.9147\n",
      "Epoch 123/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1848 - accuracy: 0.9059 - val_loss: 0.1841 - val_accuracy: 0.9062\n",
      "Epoch 124/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1739 - accuracy: 0.9166 - val_loss: 0.1590 - val_accuracy: 0.9213\n",
      "Epoch 125/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1784 - accuracy: 0.9176 - val_loss: 0.1761 - val_accuracy: 0.9131\n",
      "Epoch 126/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1833 - accuracy: 0.9104 - val_loss: 0.1674 - val_accuracy: 0.9235\n",
      "Epoch 127/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1720 - accuracy: 0.9148 - val_loss: 0.1805 - val_accuracy: 0.9057\n",
      "Epoch 128/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1756 - accuracy: 0.9131 - val_loss: 0.2064 - val_accuracy: 0.8838\n",
      "Epoch 129/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1802 - accuracy: 0.9155 - val_loss: 0.1666 - val_accuracy: 0.9178\n",
      "Epoch 130/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1773 - accuracy: 0.9128 - val_loss: 0.2005 - val_accuracy: 0.8986\n",
      "Epoch 131/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1768 - accuracy: 0.9150 - val_loss: 0.1751 - val_accuracy: 0.9082\n",
      "Epoch 132/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1771 - accuracy: 0.9158 - val_loss: 0.1649 - val_accuracy: 0.9150\n",
      "Epoch 133/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1781 - accuracy: 0.9121 - val_loss: 0.1727 - val_accuracy: 0.9082\n",
      "Epoch 134/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1788 - accuracy: 0.9084 - val_loss: 0.1824 - val_accuracy: 0.9041\n",
      "Epoch 135/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1724 - accuracy: 0.9188 - val_loss: 0.1649 - val_accuracy: 0.9257\n",
      "Epoch 136/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1816 - accuracy: 0.9143 - val_loss: 0.1708 - val_accuracy: 0.9213\n",
      "Epoch 137/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1815 - accuracy: 0.9187 - val_loss: 0.1536 - val_accuracy: 0.9290\n",
      "Epoch 138/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1812 - accuracy: 0.9101 - val_loss: 0.2254 - val_accuracy: 0.8577\n",
      "Epoch 139/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1696 - accuracy: 0.9182 - val_loss: 0.1619 - val_accuracy: 0.9211\n",
      "Epoch 140/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1759 - accuracy: 0.9139 - val_loss: 0.1795 - val_accuracy: 0.9073\n",
      "Epoch 141/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1845 - accuracy: 0.9185 - val_loss: 0.1537 - val_accuracy: 0.9271\n",
      "Epoch 142/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1759 - accuracy: 0.9183 - val_loss: 0.1659 - val_accuracy: 0.9150\n",
      "Epoch 143/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1771 - accuracy: 0.9171 - val_loss: 0.1650 - val_accuracy: 0.9230\n",
      "Epoch 144/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1978 - accuracy: 0.9069 - val_loss: 0.1808 - val_accuracy: 0.9076\n",
      "Epoch 145/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1762 - accuracy: 0.9170 - val_loss: 0.3515 - val_accuracy: 0.8125\n",
      "Epoch 146/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1790 - accuracy: 0.9145 - val_loss: 0.1877 - val_accuracy: 0.9057\n",
      "Epoch 147/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1808 - accuracy: 0.9176 - val_loss: 0.1758 - val_accuracy: 0.9128\n",
      "Epoch 148/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1762 - accuracy: 0.9128 - val_loss: 0.1526 - val_accuracy: 0.9279\n",
      "Epoch 149/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1736 - accuracy: 0.9139 - val_loss: 0.2055 - val_accuracy: 0.8739\n",
      "Epoch 150/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1807 - accuracy: 0.9158 - val_loss: 0.3213 - val_accuracy: 0.8235\n",
      "Epoch 151/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1828 - accuracy: 0.9111 - val_loss: 0.1435 - val_accuracy: 0.9375\n",
      "Epoch 152/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1685 - accuracy: 0.9158 - val_loss: 0.1752 - val_accuracy: 0.9186\n",
      "Epoch 153/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1743 - accuracy: 0.9155 - val_loss: 0.1640 - val_accuracy: 0.9227\n",
      "Epoch 154/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1736 - accuracy: 0.9155 - val_loss: 0.1640 - val_accuracy: 0.9158\n",
      "Epoch 155/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1769 - accuracy: 0.9116 - val_loss: 0.1510 - val_accuracy: 0.9296\n",
      "Epoch 156/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1849 - accuracy: 0.9124 - val_loss: 0.1743 - val_accuracy: 0.9054\n",
      "Epoch 157/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1715 - accuracy: 0.9168 - val_loss: 0.1525 - val_accuracy: 0.9304\n",
      "Epoch 158/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1683 - accuracy: 0.9202 - val_loss: 0.1589 - val_accuracy: 0.9202\n",
      "Epoch 159/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1766 - accuracy: 0.9133 - val_loss: 0.1675 - val_accuracy: 0.9156\n",
      "Epoch 160/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1697 - accuracy: 0.9195 - val_loss: 0.1424 - val_accuracy: 0.9391\n",
      "Epoch 161/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1719 - accuracy: 0.9151 - val_loss: 0.1535 - val_accuracy: 0.9290\n",
      "Epoch 162/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1694 - accuracy: 0.9171 - val_loss: 0.1524 - val_accuracy: 0.9348\n",
      "Epoch 163/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1687 - accuracy: 0.9200 - val_loss: 0.1597 - val_accuracy: 0.9197\n",
      "Epoch 164/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1850 - accuracy: 0.9202 - val_loss: 0.1693 - val_accuracy: 0.9216\n",
      "Epoch 165/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1679 - accuracy: 0.9195 - val_loss: 0.1993 - val_accuracy: 0.9021\n",
      "Epoch 166/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1706 - accuracy: 0.9171 - val_loss: 0.1402 - val_accuracy: 0.9350\n",
      "Epoch 167/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1747 - accuracy: 0.9178 - val_loss: 0.1911 - val_accuracy: 0.9027\n",
      "Epoch 168/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1834 - accuracy: 0.9104 - val_loss: 0.1737 - val_accuracy: 0.9172\n",
      "Epoch 169/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1638 - accuracy: 0.9217 - val_loss: 0.1490 - val_accuracy: 0.9285\n",
      "Epoch 170/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1659 - accuracy: 0.9205 - val_loss: 0.1643 - val_accuracy: 0.9186\n",
      "Epoch 171/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1729 - accuracy: 0.9188 - val_loss: 0.1495 - val_accuracy: 0.9274\n",
      "Epoch 172/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1683 - accuracy: 0.9200 - val_loss: 0.3625 - val_accuracy: 0.7048\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1785 - accuracy: 0.9170 - val_loss: 0.1627 - val_accuracy: 0.9191\n",
      "Epoch 174/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1734 - accuracy: 0.9220 - val_loss: 0.5527 - val_accuracy: 0.8257\n",
      "Epoch 175/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1715 - accuracy: 0.9232 - val_loss: 0.1799 - val_accuracy: 0.9115\n",
      "Epoch 176/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1869 - accuracy: 0.9124 - val_loss: 0.1422 - val_accuracy: 0.9312\n",
      "Epoch 177/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1744 - accuracy: 0.9176 - val_loss: 0.1494 - val_accuracy: 0.9296\n",
      "Epoch 178/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1768 - accuracy: 0.9155 - val_loss: 0.1424 - val_accuracy: 0.9331\n",
      "Epoch 179/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1690 - accuracy: 0.9205 - val_loss: 0.1463 - val_accuracy: 0.9331\n",
      "Epoch 180/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1732 - accuracy: 0.9207 - val_loss: 0.1658 - val_accuracy: 0.9216\n",
      "Epoch 181/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1706 - accuracy: 0.9165 - val_loss: 0.1444 - val_accuracy: 0.9337\n",
      "Epoch 182/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1681 - accuracy: 0.9215 - val_loss: 0.1471 - val_accuracy: 0.9306\n",
      "Epoch 183/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1632 - accuracy: 0.9239 - val_loss: 0.1742 - val_accuracy: 0.9090\n",
      "Epoch 184/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1764 - accuracy: 0.9192 - val_loss: 0.1745 - val_accuracy: 0.9131\n",
      "Epoch 185/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1581 - accuracy: 0.9239 - val_loss: 0.1683 - val_accuracy: 0.9194\n",
      "Epoch 186/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1717 - accuracy: 0.9185 - val_loss: 0.1697 - val_accuracy: 0.9156\n",
      "Epoch 187/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1624 - accuracy: 0.9212 - val_loss: 0.2343 - val_accuracy: 0.8723\n",
      "Epoch 188/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1694 - accuracy: 0.9208 - val_loss: 0.1365 - val_accuracy: 0.9380\n",
      "Epoch 189/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1644 - accuracy: 0.9229 - val_loss: 0.1728 - val_accuracy: 0.9134\n",
      "Epoch 190/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1657 - accuracy: 0.9217 - val_loss: 0.1504 - val_accuracy: 0.9268\n",
      "Epoch 191/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1732 - accuracy: 0.9185 - val_loss: 0.1848 - val_accuracy: 0.9101\n",
      "Epoch 192/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1650 - accuracy: 0.9212 - val_loss: 0.1417 - val_accuracy: 0.9339\n",
      "Epoch 193/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1842 - accuracy: 0.9176 - val_loss: 0.1601 - val_accuracy: 0.9227\n",
      "Epoch 194/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1565 - accuracy: 0.9267 - val_loss: 0.1529 - val_accuracy: 0.9260\n",
      "Epoch 195/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1677 - accuracy: 0.9210 - val_loss: 0.1497 - val_accuracy: 0.9279\n",
      "Epoch 196/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1597 - accuracy: 0.9264 - val_loss: 0.1697 - val_accuracy: 0.9164\n",
      "Epoch 197/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1612 - accuracy: 0.9297 - val_loss: 0.1461 - val_accuracy: 0.9353\n",
      "Epoch 198/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1652 - accuracy: 0.9240 - val_loss: 0.1581 - val_accuracy: 0.9205\n",
      "Epoch 199/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1611 - accuracy: 0.9252 - val_loss: 0.1455 - val_accuracy: 0.9337\n",
      "Epoch 200/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1531 - accuracy: 0.9294 - val_loss: 0.1398 - val_accuracy: 0.9356\n",
      "Epoch 201/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1579 - accuracy: 0.9282 - val_loss: 0.2035 - val_accuracy: 0.9093\n",
      "Epoch 202/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1580 - accuracy: 0.9271 - val_loss: 0.1582 - val_accuracy: 0.9246\n",
      "Epoch 203/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1624 - accuracy: 0.9227 - val_loss: 0.1670 - val_accuracy: 0.9230\n",
      "Epoch 204/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1732 - accuracy: 0.9267 - val_loss: 0.1718 - val_accuracy: 0.9153\n",
      "Epoch 205/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1635 - accuracy: 0.9291 - val_loss: 0.1678 - val_accuracy: 0.9194\n",
      "Epoch 206/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1598 - accuracy: 0.9259 - val_loss: 0.1463 - val_accuracy: 0.9282\n",
      "Epoch 207/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1611 - accuracy: 0.9254 - val_loss: 0.2604 - val_accuracy: 0.8440\n",
      "Epoch 208/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1666 - accuracy: 0.9200 - val_loss: 0.2517 - val_accuracy: 0.8854\n",
      "Epoch 209/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1687 - accuracy: 0.9224 - val_loss: 0.1458 - val_accuracy: 0.9326\n",
      "Epoch 210/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1655 - accuracy: 0.9247 - val_loss: 0.4238 - val_accuracy: 0.8169\n",
      "Epoch 211/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1614 - accuracy: 0.9232 - val_loss: 0.1433 - val_accuracy: 0.9361\n",
      "Epoch 212/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1577 - accuracy: 0.9271 - val_loss: 0.1994 - val_accuracy: 0.8964\n",
      "Epoch 213/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1642 - accuracy: 0.9261 - val_loss: 0.1514 - val_accuracy: 0.9334\n",
      "Epoch 214/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1765 - accuracy: 0.9202 - val_loss: 0.1799 - val_accuracy: 0.9131\n",
      "Epoch 215/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1553 - accuracy: 0.9249 - val_loss: 0.1620 - val_accuracy: 0.9221\n",
      "Epoch 216/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1504 - accuracy: 0.9296 - val_loss: 0.1608 - val_accuracy: 0.9263\n",
      "Epoch 217/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1681 - accuracy: 0.9234 - val_loss: 0.1327 - val_accuracy: 0.9391\n",
      "Epoch 218/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1582 - accuracy: 0.9309 - val_loss: 0.1483 - val_accuracy: 0.9317\n",
      "Epoch 219/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1634 - accuracy: 0.9301 - val_loss: 0.2740 - val_accuracy: 0.8649\n",
      "Epoch 220/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1533 - accuracy: 0.9313 - val_loss: 0.1557 - val_accuracy: 0.9279\n",
      "Epoch 221/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1615 - accuracy: 0.9274 - val_loss: 0.1757 - val_accuracy: 0.9178\n",
      "Epoch 222/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1641 - accuracy: 0.9224 - val_loss: 0.5914 - val_accuracy: 0.8056\n",
      "Epoch 223/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1622 - accuracy: 0.9297 - val_loss: 0.1851 - val_accuracy: 0.9052\n",
      "Epoch 224/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1693 - accuracy: 0.9297 - val_loss: 0.1557 - val_accuracy: 0.9304\n",
      "Epoch 225/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1534 - accuracy: 0.9314 - val_loss: 0.1981 - val_accuracy: 0.8958\n",
      "Epoch 226/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1549 - accuracy: 0.9237 - val_loss: 0.1666 - val_accuracy: 0.9183\n",
      "Epoch 227/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1584 - accuracy: 0.9276 - val_loss: 0.1493 - val_accuracy: 0.9265\n",
      "Epoch 228/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1529 - accuracy: 0.9353 - val_loss: 0.1541 - val_accuracy: 0.9326\n",
      "Epoch 229/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1728 - accuracy: 0.9195 - val_loss: 0.1450 - val_accuracy: 0.9287\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1584 - accuracy: 0.9299 - val_loss: 0.1441 - val_accuracy: 0.9312\n",
      "Epoch 231/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1689 - accuracy: 0.9262 - val_loss: 0.1663 - val_accuracy: 0.9224\n",
      "Epoch 232/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1588 - accuracy: 0.9291 - val_loss: 0.1841 - val_accuracy: 0.9175\n",
      "Epoch 233/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1658 - accuracy: 0.9259 - val_loss: 0.1640 - val_accuracy: 0.9249\n",
      "Epoch 234/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1572 - accuracy: 0.9311 - val_loss: 0.3336 - val_accuracy: 0.8621\n",
      "Epoch 235/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1576 - accuracy: 0.9309 - val_loss: 0.1892 - val_accuracy: 0.9172\n",
      "Epoch 236/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1560 - accuracy: 0.9296 - val_loss: 0.3480 - val_accuracy: 0.8306\n",
      "Epoch 237/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1569 - accuracy: 0.9292 - val_loss: 0.1471 - val_accuracy: 0.9309\n",
      "Epoch 238/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1574 - accuracy: 0.9271 - val_loss: 0.1532 - val_accuracy: 0.9293\n",
      "Epoch 239/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1613 - accuracy: 0.9287 - val_loss: 0.1556 - val_accuracy: 0.9312\n",
      "Epoch 240/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1733 - accuracy: 0.9252 - val_loss: 0.1803 - val_accuracy: 0.9082\n",
      "Epoch 241/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1660 - accuracy: 0.9200 - val_loss: 0.1426 - val_accuracy: 0.9339\n",
      "Epoch 242/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1640 - accuracy: 0.9274 - val_loss: 0.1634 - val_accuracy: 0.9180\n",
      "Epoch 243/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1543 - accuracy: 0.9324 - val_loss: 0.1454 - val_accuracy: 0.9317\n",
      "Epoch 244/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1659 - accuracy: 0.9296 - val_loss: 0.1340 - val_accuracy: 0.9411\n",
      "Epoch 245/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1593 - accuracy: 0.9271 - val_loss: 0.1630 - val_accuracy: 0.9197\n",
      "Epoch 246/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1569 - accuracy: 0.9309 - val_loss: 0.1400 - val_accuracy: 0.9304\n",
      "Epoch 247/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1596 - accuracy: 0.9294 - val_loss: 0.1389 - val_accuracy: 0.9342\n",
      "Epoch 248/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1540 - accuracy: 0.9355 - val_loss: 0.2365 - val_accuracy: 0.8810\n",
      "Epoch 249/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1599 - accuracy: 0.9284 - val_loss: 0.1895 - val_accuracy: 0.9005\n",
      "Epoch 250/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1671 - accuracy: 0.9294 - val_loss: 0.1392 - val_accuracy: 0.9348\n",
      "Epoch 251/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1556 - accuracy: 0.9296 - val_loss: 0.1378 - val_accuracy: 0.9372\n",
      "Epoch 252/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1577 - accuracy: 0.9249 - val_loss: 0.1784 - val_accuracy: 0.9117\n",
      "Epoch 253/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1691 - accuracy: 0.9237 - val_loss: 0.1544 - val_accuracy: 0.9276\n",
      "Epoch 254/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1508 - accuracy: 0.9323 - val_loss: 0.2229 - val_accuracy: 0.8934\n",
      "Epoch 255/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1535 - accuracy: 0.9341 - val_loss: 0.1270 - val_accuracy: 0.9465\n",
      "Epoch 256/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1588 - accuracy: 0.9274 - val_loss: 0.2448 - val_accuracy: 0.8928\n",
      "Epoch 257/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1629 - accuracy: 0.9316 - val_loss: 0.2746 - val_accuracy: 0.8627\n",
      "Epoch 258/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1588 - accuracy: 0.9277 - val_loss: 0.1436 - val_accuracy: 0.9309\n",
      "Epoch 259/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1732 - accuracy: 0.9308 - val_loss: 0.1580 - val_accuracy: 0.9282\n",
      "Epoch 260/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1532 - accuracy: 0.9356 - val_loss: 0.1763 - val_accuracy: 0.9178\n",
      "Epoch 261/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1481 - accuracy: 0.9336 - val_loss: 0.1597 - val_accuracy: 0.9219\n",
      "Epoch 262/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1885 - accuracy: 0.9324 - val_loss: 0.1472 - val_accuracy: 0.9342\n",
      "Epoch 263/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1457 - accuracy: 0.9387 - val_loss: 0.1377 - val_accuracy: 0.9350\n",
      "Epoch 264/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1512 - accuracy: 0.9339 - val_loss: 0.2191 - val_accuracy: 0.8986\n",
      "Epoch 265/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1518 - accuracy: 0.9326 - val_loss: 0.5381 - val_accuracy: 0.8396\n",
      "Epoch 266/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1813 - accuracy: 0.9277 - val_loss: 0.1495 - val_accuracy: 0.9326\n",
      "Epoch 267/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1445 - accuracy: 0.9333 - val_loss: 0.1716 - val_accuracy: 0.9175\n",
      "Epoch 268/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1680 - accuracy: 0.9303 - val_loss: 0.1811 - val_accuracy: 0.9112\n",
      "Epoch 269/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1557 - accuracy: 0.9306 - val_loss: 0.1423 - val_accuracy: 0.9367\n",
      "Epoch 270/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1487 - accuracy: 0.9306 - val_loss: 0.1437 - val_accuracy: 0.9337\n",
      "Epoch 271/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1620 - accuracy: 0.9313 - val_loss: 0.1506 - val_accuracy: 0.9304\n",
      "Epoch 272/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1500 - accuracy: 0.9355 - val_loss: 0.1534 - val_accuracy: 0.9304\n",
      "Epoch 273/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1445 - accuracy: 0.9380 - val_loss: 0.1615 - val_accuracy: 0.9254\n",
      "Epoch 274/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1699 - accuracy: 0.9313 - val_loss: 0.1615 - val_accuracy: 0.9243\n",
      "Epoch 275/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1527 - accuracy: 0.9360 - val_loss: 0.1295 - val_accuracy: 0.9449\n",
      "Epoch 276/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1537 - accuracy: 0.9336 - val_loss: 0.1274 - val_accuracy: 0.9452\n",
      "Epoch 277/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1470 - accuracy: 0.9333 - val_loss: 0.1470 - val_accuracy: 0.9323\n",
      "Epoch 278/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1614 - accuracy: 0.9368 - val_loss: 0.1628 - val_accuracy: 0.9232\n",
      "Epoch 279/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1488 - accuracy: 0.9358 - val_loss: 0.1615 - val_accuracy: 0.9238\n",
      "Epoch 280/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1451 - accuracy: 0.9366 - val_loss: 0.1588 - val_accuracy: 0.9312\n",
      "Epoch 281/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1528 - accuracy: 0.9345 - val_loss: 0.1420 - val_accuracy: 0.9306\n",
      "Epoch 282/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1797 - accuracy: 0.9316 - val_loss: 0.1438 - val_accuracy: 0.9350\n",
      "Epoch 283/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1675 - accuracy: 0.9324 - val_loss: 0.1348 - val_accuracy: 0.9438\n",
      "Epoch 284/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1519 - accuracy: 0.9324 - val_loss: 0.1728 - val_accuracy: 0.9216\n",
      "Epoch 285/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1468 - accuracy: 0.9338 - val_loss: 0.1432 - val_accuracy: 0.9364\n",
      "Epoch 286/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1655 - accuracy: 0.9336 - val_loss: 0.1466 - val_accuracy: 0.9320\n",
      "Epoch 287/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1454 - accuracy: 0.9363 - val_loss: 0.1538 - val_accuracy: 0.9290\n",
      "Epoch 288/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1564 - accuracy: 0.9339 - val_loss: 0.1356 - val_accuracy: 0.9389\n",
      "Epoch 289/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1480 - accuracy: 0.9412 - val_loss: 0.1950 - val_accuracy: 0.8983\n",
      "Epoch 290/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1539 - accuracy: 0.9334 - val_loss: 0.1603 - val_accuracy: 0.9216\n",
      "Epoch 291/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1455 - accuracy: 0.9358 - val_loss: 0.2057 - val_accuracy: 0.8898\n",
      "Epoch 292/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1438 - accuracy: 0.9378 - val_loss: 0.1342 - val_accuracy: 0.9389\n",
      "Epoch 293/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1487 - accuracy: 0.9363 - val_loss: 0.2608 - val_accuracy: 0.8591\n",
      "Epoch 294/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1450 - accuracy: 0.9400 - val_loss: 0.1982 - val_accuracy: 0.9194\n",
      "Epoch 295/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1399 - accuracy: 0.9393 - val_loss: 0.1358 - val_accuracy: 0.9435\n",
      "Epoch 296/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1560 - accuracy: 0.9355 - val_loss: 0.2146 - val_accuracy: 0.8980\n",
      "Epoch 297/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1439 - accuracy: 0.9350 - val_loss: 0.1965 - val_accuracy: 0.9060\n",
      "Epoch 298/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1475 - accuracy: 0.9356 - val_loss: 0.1468 - val_accuracy: 0.9306\n",
      "Epoch 299/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1510 - accuracy: 0.9363 - val_loss: 0.1541 - val_accuracy: 0.9268\n",
      "Epoch 300/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1491 - accuracy: 0.9341 - val_loss: 0.1649 - val_accuracy: 0.9224\n",
      "Epoch 301/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1399 - accuracy: 0.9387 - val_loss: 0.1584 - val_accuracy: 0.9274\n",
      "Epoch 302/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1667 - accuracy: 0.9378 - val_loss: 0.1470 - val_accuracy: 0.9386\n",
      "Epoch 303/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1436 - accuracy: 0.9393 - val_loss: 0.1557 - val_accuracy: 0.9309\n",
      "Epoch 304/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1592 - accuracy: 0.9343 - val_loss: 0.1462 - val_accuracy: 0.9337\n",
      "Epoch 305/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1384 - accuracy: 0.9390 - val_loss: 0.1824 - val_accuracy: 0.9243\n",
      "Epoch 306/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1787 - accuracy: 0.9345 - val_loss: 0.1409 - val_accuracy: 0.9375\n",
      "Epoch 307/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1362 - accuracy: 0.9440 - val_loss: 0.1688 - val_accuracy: 0.9254\n",
      "Epoch 308/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1645 - accuracy: 0.9360 - val_loss: 0.1859 - val_accuracy: 0.9054\n",
      "Epoch 309/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1368 - accuracy: 0.9393 - val_loss: 0.1350 - val_accuracy: 0.9424\n",
      "Epoch 310/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.2002 - accuracy: 0.9324 - val_loss: 0.2408 - val_accuracy: 0.8638\n",
      "Epoch 311/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1478 - accuracy: 0.9353 - val_loss: 0.2036 - val_accuracy: 0.8980\n",
      "Epoch 312/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1427 - accuracy: 0.9390 - val_loss: 0.1480 - val_accuracy: 0.9306\n",
      "Epoch 313/500\n",
      "93/93 [==============================] - 1s 7ms/step - loss: 0.1482 - accuracy: 0.9413 - val_loss: 0.2333 - val_accuracy: 0.8816\n",
      "Epoch 314/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1525 - accuracy: 0.9358 - val_loss: 0.1488 - val_accuracy: 0.9334\n",
      "Epoch 315/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1559 - accuracy: 0.9358 - val_loss: 0.2659 - val_accuracy: 0.8766\n",
      "Epoch 316/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1547 - accuracy: 0.9360 - val_loss: 0.2619 - val_accuracy: 0.8413\n",
      "Epoch 317/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1443 - accuracy: 0.9366 - val_loss: 0.1433 - val_accuracy: 0.9370\n",
      "Epoch 318/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1503 - accuracy: 0.9400 - val_loss: 0.1243 - val_accuracy: 0.9457\n",
      "Epoch 319/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1599 - accuracy: 0.9318 - val_loss: 0.1451 - val_accuracy: 0.9391\n",
      "Epoch 320/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1440 - accuracy: 0.9420 - val_loss: 0.1693 - val_accuracy: 0.9274\n",
      "Epoch 321/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1446 - accuracy: 0.9402 - val_loss: 0.1383 - val_accuracy: 0.9372\n",
      "Epoch 322/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1690 - accuracy: 0.9368 - val_loss: 0.1509 - val_accuracy: 0.9271\n",
      "Epoch 323/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1418 - accuracy: 0.9371 - val_loss: 0.1288 - val_accuracy: 0.9449\n",
      "Epoch 324/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1617 - accuracy: 0.9356 - val_loss: 0.1726 - val_accuracy: 0.9208\n",
      "Epoch 325/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1533 - accuracy: 0.9388 - val_loss: 0.1968 - val_accuracy: 0.8821\n",
      "Epoch 326/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1427 - accuracy: 0.9400 - val_loss: 0.1555 - val_accuracy: 0.9320\n",
      "Epoch 327/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1542 - accuracy: 0.9383 - val_loss: 0.1554 - val_accuracy: 0.9293\n",
      "Epoch 328/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1372 - accuracy: 0.9407 - val_loss: 0.1370 - val_accuracy: 0.9427\n",
      "Epoch 329/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1492 - accuracy: 0.9371 - val_loss: 0.2124 - val_accuracy: 0.8914\n",
      "Epoch 330/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1384 - accuracy: 0.9383 - val_loss: 0.1296 - val_accuracy: 0.9457\n",
      "Epoch 331/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1401 - accuracy: 0.9442 - val_loss: 0.1441 - val_accuracy: 0.9326\n",
      "Epoch 332/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1394 - accuracy: 0.9442 - val_loss: 0.1391 - val_accuracy: 0.9386\n",
      "Epoch 333/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1418 - accuracy: 0.9403 - val_loss: 0.1306 - val_accuracy: 0.9463\n",
      "Epoch 334/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1392 - accuracy: 0.9398 - val_loss: 0.1310 - val_accuracy: 0.9389\n",
      "Epoch 335/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1395 - accuracy: 0.9417 - val_loss: 0.1348 - val_accuracy: 0.9482\n",
      "Epoch 336/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1522 - accuracy: 0.9378 - val_loss: 0.1730 - val_accuracy: 0.9194\n",
      "Epoch 337/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1350 - accuracy: 0.9408 - val_loss: 0.1557 - val_accuracy: 0.9293\n",
      "Epoch 338/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1354 - accuracy: 0.9410 - val_loss: 0.1698 - val_accuracy: 0.9183\n",
      "Epoch 339/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1368 - accuracy: 0.9424 - val_loss: 0.1397 - val_accuracy: 0.9375\n",
      "Epoch 340/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1418 - accuracy: 0.9434 - val_loss: 0.1473 - val_accuracy: 0.9339\n",
      "Epoch 341/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1575 - accuracy: 0.9378 - val_loss: 0.1493 - val_accuracy: 0.9331\n",
      "Epoch 342/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1413 - accuracy: 0.9413 - val_loss: 0.2207 - val_accuracy: 0.8887\n",
      "Epoch 343/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1431 - accuracy: 0.9378 - val_loss: 0.1364 - val_accuracy: 0.9430\n",
      "Epoch 344/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1423 - accuracy: 0.9371 - val_loss: 0.1864 - val_accuracy: 0.9076\n",
      "Epoch 345/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1368 - accuracy: 0.9471 - val_loss: 0.1492 - val_accuracy: 0.9328\n",
      "Epoch 346/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1517 - accuracy: 0.9383 - val_loss: 0.1469 - val_accuracy: 0.9375\n",
      "Epoch 347/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1340 - accuracy: 0.9461 - val_loss: 0.1676 - val_accuracy: 0.9139\n",
      "Epoch 348/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1390 - accuracy: 0.9375 - val_loss: 0.2202 - val_accuracy: 0.8994\n",
      "Epoch 349/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1331 - accuracy: 0.9474 - val_loss: 0.1562 - val_accuracy: 0.9263\n",
      "Epoch 350/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1428 - accuracy: 0.9410 - val_loss: 0.2148 - val_accuracy: 0.9060\n",
      "Epoch 351/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1363 - accuracy: 0.9445 - val_loss: 0.1353 - val_accuracy: 0.9389\n",
      "Epoch 352/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1602 - accuracy: 0.9408 - val_loss: 0.1505 - val_accuracy: 0.9315\n",
      "Epoch 353/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1353 - accuracy: 0.9432 - val_loss: 0.1338 - val_accuracy: 0.9465\n",
      "Epoch 354/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1408 - accuracy: 0.9429 - val_loss: 0.1259 - val_accuracy: 0.9416\n",
      "Epoch 355/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1382 - accuracy: 0.9403 - val_loss: 0.1465 - val_accuracy: 0.9372\n",
      "Epoch 356/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1330 - accuracy: 0.9471 - val_loss: 0.1612 - val_accuracy: 0.9221\n",
      "Epoch 357/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1319 - accuracy: 0.9454 - val_loss: 0.1556 - val_accuracy: 0.9323\n",
      "Epoch 358/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1345 - accuracy: 0.9434 - val_loss: 0.1505 - val_accuracy: 0.9356\n",
      "Epoch 359/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1580 - accuracy: 0.9429 - val_loss: 0.1416 - val_accuracy: 0.9380\n",
      "Epoch 360/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1433 - accuracy: 0.9437 - val_loss: 0.1555 - val_accuracy: 0.9320\n",
      "Epoch 361/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1256 - accuracy: 0.9454 - val_loss: 0.1632 - val_accuracy: 0.9246\n",
      "Epoch 362/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1424 - accuracy: 0.9405 - val_loss: 0.1463 - val_accuracy: 0.9323\n",
      "Epoch 363/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1474 - accuracy: 0.9449 - val_loss: 0.1415 - val_accuracy: 0.9375\n",
      "Epoch 364/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1307 - accuracy: 0.9444 - val_loss: 0.1523 - val_accuracy: 0.9334\n",
      "Epoch 365/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1281 - accuracy: 0.9440 - val_loss: 0.1680 - val_accuracy: 0.9282\n",
      "Epoch 366/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1380 - accuracy: 0.9442 - val_loss: 0.1246 - val_accuracy: 0.9490\n",
      "Epoch 367/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1380 - accuracy: 0.9445 - val_loss: 0.1815 - val_accuracy: 0.9202\n",
      "Epoch 368/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1256 - accuracy: 0.9471 - val_loss: 0.2123 - val_accuracy: 0.8840\n",
      "Epoch 369/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1368 - accuracy: 0.9434 - val_loss: 0.1682 - val_accuracy: 0.9238\n",
      "Epoch 370/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1355 - accuracy: 0.9425 - val_loss: 0.1417 - val_accuracy: 0.9367\n",
      "Epoch 371/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1294 - accuracy: 0.9434 - val_loss: 0.2274 - val_accuracy: 0.8884\n",
      "Epoch 372/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1507 - accuracy: 0.9418 - val_loss: 0.3008 - val_accuracy: 0.8569\n",
      "Epoch 373/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1367 - accuracy: 0.9420 - val_loss: 0.1422 - val_accuracy: 0.9361\n",
      "Epoch 374/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1318 - accuracy: 0.9472 - val_loss: 0.1944 - val_accuracy: 0.9101\n",
      "Epoch 375/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1428 - accuracy: 0.9387 - val_loss: 0.1702 - val_accuracy: 0.9191\n",
      "Epoch 376/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1346 - accuracy: 0.9445 - val_loss: 0.1589 - val_accuracy: 0.9342\n",
      "Epoch 377/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1358 - accuracy: 0.9439 - val_loss: 0.2596 - val_accuracy: 0.8772\n",
      "Epoch 378/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1332 - accuracy: 0.9477 - val_loss: 0.1765 - val_accuracy: 0.9101\n",
      "Epoch 379/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1433 - accuracy: 0.9454 - val_loss: 0.1488 - val_accuracy: 0.9328\n",
      "Epoch 380/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1290 - accuracy: 0.9452 - val_loss: 0.2093 - val_accuracy: 0.8999\n",
      "Epoch 381/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1443 - accuracy: 0.9481 - val_loss: 0.1339 - val_accuracy: 0.9435\n",
      "Epoch 382/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1379 - accuracy: 0.9412 - val_loss: 0.1471 - val_accuracy: 0.9337\n",
      "Epoch 383/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1391 - accuracy: 0.9449 - val_loss: 0.1424 - val_accuracy: 0.9433\n",
      "Epoch 384/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1361 - accuracy: 0.9482 - val_loss: 0.1436 - val_accuracy: 0.9356\n",
      "Epoch 385/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1293 - accuracy: 0.9457 - val_loss: 0.1408 - val_accuracy: 0.9359\n",
      "Epoch 386/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1323 - accuracy: 0.9472 - val_loss: 0.1305 - val_accuracy: 0.9441\n",
      "Epoch 387/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1372 - accuracy: 0.9420 - val_loss: 0.1358 - val_accuracy: 0.9405\n",
      "Epoch 388/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1300 - accuracy: 0.9459 - val_loss: 0.1360 - val_accuracy: 0.9419\n",
      "Epoch 389/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1348 - accuracy: 0.9457 - val_loss: 0.1796 - val_accuracy: 0.9200\n",
      "Epoch 390/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1396 - accuracy: 0.9442 - val_loss: 0.1774 - val_accuracy: 0.9232\n",
      "Epoch 391/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1269 - accuracy: 0.9440 - val_loss: 0.1296 - val_accuracy: 0.9444\n",
      "Epoch 392/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1417 - accuracy: 0.9434 - val_loss: 0.2972 - val_accuracy: 0.8435\n",
      "Epoch 393/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1396 - accuracy: 0.9469 - val_loss: 0.1431 - val_accuracy: 0.9372\n",
      "Epoch 394/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1286 - accuracy: 0.9437 - val_loss: 0.1274 - val_accuracy: 0.9438\n",
      "Epoch 395/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1294 - accuracy: 0.9479 - val_loss: 0.1298 - val_accuracy: 0.9430\n",
      "Epoch 396/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1304 - accuracy: 0.9496 - val_loss: 0.1157 - val_accuracy: 0.9501\n",
      "Epoch 397/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1372 - accuracy: 0.9427 - val_loss: 0.1678 - val_accuracy: 0.9276\n",
      "Epoch 398/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1327 - accuracy: 0.9464 - val_loss: 0.1378 - val_accuracy: 0.9413\n",
      "Epoch 399/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1475 - accuracy: 0.9390 - val_loss: 0.2387 - val_accuracy: 0.9101\n",
      "Epoch 400/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1245 - accuracy: 0.9497 - val_loss: 0.1916 - val_accuracy: 0.9186\n",
      "Epoch 401/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1401 - accuracy: 0.9474 - val_loss: 0.1565 - val_accuracy: 0.9328\n",
      "Epoch 402/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1301 - accuracy: 0.9418 - val_loss: 0.1416 - val_accuracy: 0.9334\n",
      "Epoch 403/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1298 - accuracy: 0.9455 - val_loss: 0.1645 - val_accuracy: 0.9293\n",
      "Epoch 404/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1406 - accuracy: 0.9449 - val_loss: 0.1409 - val_accuracy: 0.9419\n",
      "Epoch 405/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1416 - accuracy: 0.9425 - val_loss: 0.1518 - val_accuracy: 0.9296\n",
      "Epoch 406/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1411 - accuracy: 0.9430 - val_loss: 0.1335 - val_accuracy: 0.9402\n",
      "Epoch 407/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1532 - accuracy: 0.9442 - val_loss: 0.1214 - val_accuracy: 0.9498\n",
      "Epoch 408/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1399 - accuracy: 0.9444 - val_loss: 0.1754 - val_accuracy: 0.9227\n",
      "Epoch 409/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1250 - accuracy: 0.9482 - val_loss: 0.1528 - val_accuracy: 0.9345\n",
      "Epoch 410/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1364 - accuracy: 0.9457 - val_loss: 0.1291 - val_accuracy: 0.9452\n",
      "Epoch 411/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1353 - accuracy: 0.9439 - val_loss: 0.2207 - val_accuracy: 0.9013\n",
      "Epoch 412/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1236 - accuracy: 0.9528 - val_loss: 0.1359 - val_accuracy: 0.9416\n",
      "Epoch 413/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1404 - accuracy: 0.9461 - val_loss: 0.1814 - val_accuracy: 0.9208\n",
      "Epoch 414/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1247 - accuracy: 0.9482 - val_loss: 0.1647 - val_accuracy: 0.9238\n",
      "Epoch 415/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1279 - accuracy: 0.9462 - val_loss: 0.2161 - val_accuracy: 0.9071\n",
      "Epoch 416/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1518 - accuracy: 0.9474 - val_loss: 0.1229 - val_accuracy: 0.9479\n",
      "Epoch 417/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1255 - accuracy: 0.9459 - val_loss: 0.1569 - val_accuracy: 0.9301\n",
      "Epoch 418/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1321 - accuracy: 0.9472 - val_loss: 0.1642 - val_accuracy: 0.9306\n",
      "Epoch 419/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1343 - accuracy: 0.9486 - val_loss: 0.1255 - val_accuracy: 0.9476\n",
      "Epoch 420/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1329 - accuracy: 0.9412 - val_loss: 0.1257 - val_accuracy: 0.9446\n",
      "Epoch 421/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1678 - accuracy: 0.9439 - val_loss: 0.1476 - val_accuracy: 0.9391\n",
      "Epoch 422/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1340 - accuracy: 0.9445 - val_loss: 0.1407 - val_accuracy: 0.9394\n",
      "Epoch 423/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1343 - accuracy: 0.9513 - val_loss: 0.1396 - val_accuracy: 0.9411\n",
      "Epoch 424/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1355 - accuracy: 0.9440 - val_loss: 0.1733 - val_accuracy: 0.9298\n",
      "Epoch 425/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1287 - accuracy: 0.9509 - val_loss: 0.1357 - val_accuracy: 0.9383\n",
      "Epoch 426/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1295 - accuracy: 0.9464 - val_loss: 0.1753 - val_accuracy: 0.9191\n",
      "Epoch 427/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1417 - accuracy: 0.9472 - val_loss: 0.1932 - val_accuracy: 0.9126\n",
      "Epoch 428/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1322 - accuracy: 0.9514 - val_loss: 0.1280 - val_accuracy: 0.9485\n",
      "Epoch 429/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1224 - accuracy: 0.9518 - val_loss: 0.1291 - val_accuracy: 0.9452\n",
      "Epoch 430/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1436 - accuracy: 0.9466 - val_loss: 0.1676 - val_accuracy: 0.9282\n",
      "Epoch 431/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1195 - accuracy: 0.9504 - val_loss: 0.1686 - val_accuracy: 0.9221\n",
      "Epoch 432/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1331 - accuracy: 0.9514 - val_loss: 0.1395 - val_accuracy: 0.9419\n",
      "Epoch 433/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1350 - accuracy: 0.9452 - val_loss: 0.1350 - val_accuracy: 0.9394\n",
      "Epoch 434/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1319 - accuracy: 0.9462 - val_loss: 0.1523 - val_accuracy: 0.9334\n",
      "Epoch 435/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1229 - accuracy: 0.9482 - val_loss: 0.1427 - val_accuracy: 0.9372\n",
      "Epoch 436/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1266 - accuracy: 0.9459 - val_loss: 0.1894 - val_accuracy: 0.9161\n",
      "Epoch 437/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1341 - accuracy: 0.9482 - val_loss: 0.1292 - val_accuracy: 0.9411\n",
      "Epoch 438/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1270 - accuracy: 0.9513 - val_loss: 0.1594 - val_accuracy: 0.9378\n",
      "Epoch 439/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1231 - accuracy: 0.9482 - val_loss: 0.1273 - val_accuracy: 0.9454\n",
      "Epoch 440/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1534 - accuracy: 0.9435 - val_loss: 0.1911 - val_accuracy: 0.9216\n",
      "Epoch 441/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1360 - accuracy: 0.9486 - val_loss: 0.3206 - val_accuracy: 0.8607\n",
      "Epoch 442/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1281 - accuracy: 0.9487 - val_loss: 0.1709 - val_accuracy: 0.9254\n",
      "Epoch 443/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1163 - accuracy: 0.9528 - val_loss: 0.1904 - val_accuracy: 0.9104\n",
      "Epoch 444/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1277 - accuracy: 0.9518 - val_loss: 0.1242 - val_accuracy: 0.9490\n",
      "Epoch 445/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1269 - accuracy: 0.9444 - val_loss: 0.1299 - val_accuracy: 0.9452\n",
      "Epoch 446/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1293 - accuracy: 0.9523 - val_loss: 0.1486 - val_accuracy: 0.9317\n",
      "Epoch 447/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1213 - accuracy: 0.9508 - val_loss: 0.1285 - val_accuracy: 0.9474\n",
      "Epoch 448/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1375 - accuracy: 0.9491 - val_loss: 0.2002 - val_accuracy: 0.9005\n",
      "Epoch 449/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1446 - accuracy: 0.9464 - val_loss: 0.1466 - val_accuracy: 0.9364\n",
      "Epoch 450/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1223 - accuracy: 0.9479 - val_loss: 0.1358 - val_accuracy: 0.9400\n",
      "Epoch 451/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1313 - accuracy: 0.9476 - val_loss: 0.1339 - val_accuracy: 0.9449\n",
      "Epoch 452/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1318 - accuracy: 0.9482 - val_loss: 0.1301 - val_accuracy: 0.9485\n",
      "Epoch 453/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1168 - accuracy: 0.9543 - val_loss: 0.1689 - val_accuracy: 0.9178\n",
      "Epoch 454/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1300 - accuracy: 0.9513 - val_loss: 0.1561 - val_accuracy: 0.9309\n",
      "Epoch 455/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1241 - accuracy: 0.9487 - val_loss: 0.1502 - val_accuracy: 0.9370\n",
      "Epoch 456/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1755 - accuracy: 0.9466 - val_loss: 0.1774 - val_accuracy: 0.9249\n",
      "Epoch 457/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1287 - accuracy: 0.9526 - val_loss: 0.3130 - val_accuracy: 0.8698\n",
      "Epoch 458/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1219 - accuracy: 0.9529 - val_loss: 0.1347 - val_accuracy: 0.9465\n",
      "Epoch 459/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1222 - accuracy: 0.9509 - val_loss: 0.1888 - val_accuracy: 0.9161\n",
      "Epoch 460/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1324 - accuracy: 0.9511 - val_loss: 0.1399 - val_accuracy: 0.9441\n",
      "Epoch 461/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1260 - accuracy: 0.9491 - val_loss: 0.1399 - val_accuracy: 0.9397\n",
      "Epoch 462/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1382 - accuracy: 0.9511 - val_loss: 0.1710 - val_accuracy: 0.9320\n",
      "Epoch 463/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1456 - accuracy: 0.9481 - val_loss: 0.1402 - val_accuracy: 0.9419\n",
      "Epoch 464/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1219 - accuracy: 0.9528 - val_loss: 0.1104 - val_accuracy: 0.9542\n",
      "Epoch 465/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1172 - accuracy: 0.9519 - val_loss: 0.1644 - val_accuracy: 0.9230\n",
      "Epoch 466/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1333 - accuracy: 0.9503 - val_loss: 0.1413 - val_accuracy: 0.9397\n",
      "Epoch 467/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1202 - accuracy: 0.9486 - val_loss: 0.1248 - val_accuracy: 0.9471\n",
      "Epoch 468/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1295 - accuracy: 0.9494 - val_loss: 0.1433 - val_accuracy: 0.9378\n",
      "Epoch 469/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1880 - accuracy: 0.9459 - val_loss: 0.1616 - val_accuracy: 0.9254\n",
      "Epoch 470/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1529 - accuracy: 0.9457 - val_loss: 0.1395 - val_accuracy: 0.9389\n",
      "Epoch 471/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1270 - accuracy: 0.9484 - val_loss: 0.1505 - val_accuracy: 0.9348\n",
      "Epoch 472/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1297 - accuracy: 0.9548 - val_loss: 0.1840 - val_accuracy: 0.9200\n",
      "Epoch 473/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1188 - accuracy: 0.9541 - val_loss: 0.1472 - val_accuracy: 0.9386\n",
      "Epoch 474/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1236 - accuracy: 0.9516 - val_loss: 0.1274 - val_accuracy: 0.9460\n",
      "Epoch 475/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1273 - accuracy: 0.9524 - val_loss: 0.3595 - val_accuracy: 0.8950\n",
      "Epoch 476/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1317 - accuracy: 0.9501 - val_loss: 0.1225 - val_accuracy: 0.9490\n",
      "Epoch 477/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1303 - accuracy: 0.9499 - val_loss: 0.1326 - val_accuracy: 0.9457\n",
      "Epoch 478/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1944 - accuracy: 0.9499 - val_loss: 0.1386 - val_accuracy: 0.9471\n",
      "Epoch 479/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1184 - accuracy: 0.9583 - val_loss: 0.2127 - val_accuracy: 0.9139\n",
      "Epoch 480/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1188 - accuracy: 0.9519 - val_loss: 0.1579 - val_accuracy: 0.9320\n",
      "Epoch 481/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1670 - accuracy: 0.9528 - val_loss: 0.1221 - val_accuracy: 0.9498\n",
      "Epoch 482/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1190 - accuracy: 0.9526 - val_loss: 0.1178 - val_accuracy: 0.9526\n",
      "Epoch 483/500\n",
      "93/93 [==============================] - 1s 7ms/step - loss: 0.1292 - accuracy: 0.9538 - val_loss: 0.1608 - val_accuracy: 0.9348\n",
      "Epoch 484/500\n",
      "93/93 [==============================] - 1s 7ms/step - loss: 0.1277 - accuracy: 0.9541 - val_loss: 0.1305 - val_accuracy: 0.9433\n",
      "Epoch 485/500\n",
      "93/93 [==============================] - 1s 7ms/step - loss: 0.1295 - accuracy: 0.9466 - val_loss: 0.4886 - val_accuracy: 0.8448\n",
      "Epoch 486/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1231 - accuracy: 0.9523 - val_loss: 0.2344 - val_accuracy: 0.9052\n",
      "Epoch 487/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1210 - accuracy: 0.9518 - val_loss: 0.1421 - val_accuracy: 0.9416\n",
      "Epoch 488/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1393 - accuracy: 0.9504 - val_loss: 0.1398 - val_accuracy: 0.9386\n",
      "Epoch 489/500\n",
      "93/93 [==============================] - 1s 8ms/step - loss: 0.1337 - accuracy: 0.9489 - val_loss: 0.1524 - val_accuracy: 0.9359\n",
      "Epoch 490/500\n",
      "93/93 [==============================] - 1s 7ms/step - loss: 0.1126 - accuracy: 0.9536 - val_loss: 0.1732 - val_accuracy: 0.9276\n",
      "Epoch 491/500\n",
      "93/93 [==============================] - 1s 6ms/step - loss: 0.1296 - accuracy: 0.9514 - val_loss: 0.1210 - val_accuracy: 0.9498\n",
      "Epoch 492/500\n",
      "93/93 [==============================] - 1s 6ms/step - loss: 0.1245 - accuracy: 0.9526 - val_loss: 0.1572 - val_accuracy: 0.9386\n",
      "Epoch 493/500\n",
      "93/93 [==============================] - 1s 7ms/step - loss: 0.1354 - accuracy: 0.9536 - val_loss: 0.1529 - val_accuracy: 0.9309\n",
      "Epoch 494/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1167 - accuracy: 0.9528 - val_loss: 0.1722 - val_accuracy: 0.9438\n",
      "Epoch 495/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1300 - accuracy: 0.9534 - val_loss: 0.1437 - val_accuracy: 0.9482\n",
      "Epoch 496/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1203 - accuracy: 0.9496 - val_loss: 0.1303 - val_accuracy: 0.9435\n",
      "Epoch 497/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.1428 - accuracy: 0.9526 - val_loss: 0.1429 - val_accuracy: 0.9380\n",
      "Epoch 498/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.1250 - accuracy: 0.9518 - val_loss: 0.1272 - val_accuracy: 0.9553\n",
      "Epoch 499/500\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.2092 - accuracy: 0.9503 - val_loss: 0.1452 - val_accuracy: 0.9380\n",
      "Epoch 500/500\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.3554 - accuracy: 0.9444 - val_loss: 0.1626 - val_accuracy: 0.9287\n"
     ]
    }
   ],
   "source": [
    "history1 = NN_model.fit(X, Y, epochs=500, batch_size=64, validation_split = 0.38, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "industrial-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.save('dataset2_model_011.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "wrapped-surveillance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABDV0lEQVR4nO3dd3hUVfrA8e+bHiCQEIr00HuTLnZQKYLdRUVFXbGuuquusuuqa1l13XUta/1Zd+1dVhEVxd5ARQQBjQgSivRe0s7vj3Nv5s7kTmaSzDAk836eJ09m7tyZe+6U855+xRiDUkqp5JWS6AQopZRKLA0ESimV5DQQKKVUktNAoJRSSU4DgVJKJTkNBEopleQ0EKikIiKPi8hNUe67TERGxztNSiWaBgKllEpyGgiUqoNEJC3RaVD1hwYCtc9xmmSuFJH5IrJDRB4RkZYi8qaIbBORWSKS59l/oogsFJHNIvK+iPT0PDZQRL52nvcckBVyrKNFZJ7z3E9FpF+UaRwvIt+IyFYRWSEi14c8fqDzepudx6c427NF5J8islxEtojIx862Q0WkyOd9GO3cvl5EXhSRJ0VkKzBFRIaKyGfOMVaLyL9FJMPz/N4i8o6IbBSRX0XkTyKyn4jsFJF8z377i8g6EUmP5txV/aOBQO2rTgCOALoBE4A3gT8BzbHf20sARKQb8AxwmfPYDOB/IpLhZIqvAv8FmgIvOK+L89yBwKPAeUA+8CAwXUQyo0jfDuAMIBcYD1wgIsc6r9vBSe89TpoGAPOc5/0DGAQc4KTpj0B5lO/JMcCLzjGfAsqA3wPNgBHAKOBCJw05wCxgJtAa6AK8a4xZA7wPnOx53dOBZ40xJVGmQ9UzGgjUvuoeY8yvxpiVwEfAF8aYb4wxu4FXgIHOfr8B3jDGvONkZP8AsrEZ7XAgHbjTGFNijHkRmOM5xlTgQWPMF8aYMmPME8Ae53lVMsa8b4z5zhhTboyZjw1GhzgPnwrMMsY84xx3gzFmnoikAGcDlxpjVjrH/NQYsyfK9+QzY8yrzjF3GWO+MsZ8bowpNcYswwYyNw1HA2uMMf80xuw2xmwzxnzhPPYEMBlARFKBU7DBUiUpDQRqX/Wr5/Yun/uNnNutgeXuA8aYcmAF0MZ5bKUJXllxued2B+Byp2lls4hsBto5z6uSiAwTkdlOk8oW4HxsyRznNX7yeVozbNOU32PRWBGShm4i8rqIrHGai/4WRRoAXgN6iUhHbK1rizHmyxqmSdUDGghUXbcKm6EDICKCzQRXAquBNs42V3vP7RXAzcaYXM9fA2PMM1Ec92lgOtDOGNMEeABwj7MC6OzznPXA7jCP7QAaeM4jFdus5BW6VPD9wGKgqzGmMbbpzJuGTn4Jd2pVz2NrBaejtYGkp4FA1XXPA+NFZJTT2Xk5tnnnU+AzoBS4RETSReR4YKjnuf8HnO+U7kVEGjqdwDlRHDcH2GiM2S0iQ7HNQa6ngNEicrKIpIlIvogMcGorjwJ3iEhrEUkVkRFOn8QPQJZz/HTgGiBSX0UOsBXYLiI9gAs8j70OtBKRy0QkU0RyRGSY5/H/AFOAiWggSHoaCFSdZoxZgi3Z3oMtcU8AJhhjio0xxcDx2AxvI7Y/4WXPc+cC5wL/BjYBhc6+0bgQuEFEtgHXYgOS+7q/AOOwQWkjtqO4v/PwFcB32L6KjcBtQIoxZovzmg9jazM7gKBRRD6uwAagbdig9pwnDduwzT4TgDXAj8Bhnsc/wXZSf22M8TaXqSQkemEapZKTiLwHPG2MeTjRaVGJpYFAqSQkIkOAd7B9HNsSnR6VWNo0pFSSEZEnsHMMLtMgoEBrBEoplfS0RqCUUkmuzi1c1axZM1NQUJDoZCilVJ3y1VdfrTfGhM5NAepgICgoKGDu3LmJToZSStUpIhJ2mHDcmoZE5FERWSsiC8I8LiJyt4gUil1lcv94pUUppVR48ewjeBwYU8XjY4Guzt9U7HR5pZRSe1ncAoEx5kPszMlwjgH+Y6zPgVwRaRWv9CillPKXyD6CNgSvpljkbFsduqOITMXWGmjfvn3ow5SUlFBUVMTu3bvjk9J9RFZWFm3btiU9Xa8fopSKnTrRWWyMeQh4CGDw4MGVJj4UFRWRk5NDQUEBwQtN1h/GGDZs2EBRUREdO3ZMdHKUUvVIIucRrMQuF+xq62yrtt27d5Ofn19vgwCAiJCfn1/vaz1Kqb0vkYFgOnCGM3poOPbiGJWahaJVn4OAKxnOUSm198WtaUhEngEOBZo5F+W+DnvZQIwxD2CvLTsOu/TvTuCseKVFKaX2FeXlhpQUYXdJGXOWbSQ9NYWmDTPo1jKay2DER9wCgTHmlAiPG+CieB1/b9q8eTNPP/00F154YbWeN27cOJ5++mlyc3PjkzClktAnhevZVVzG6F4tY/q6yzfs4L3FazlrZOU+uvJyw86SMnYVl1FSVk5ZuaFdU3vBudVbdnHJM99w83F9adYok/1vfIfbTujLys27ufvdHyte4/oJvVi2YSe5DdLJzU7nkO4tWLJmG9+s2MTgDk257/1C/jqxN/3a5sb0vKAOLjo3ePBgEzqzeNGiRfTs2TNBKYJly5Zx9NFHs2BB8Ny50tJS0tJiG2sTfa5K7Q0lZeWkpQjvLV7L01/8wkNnDGbjjmK+/Hkjb3y3ivtOG8SaLbv5avkm+rZpQn6jDBpm2t9awdVvAHDZ6K4MLWjKNa8uYOn6HVx8WBd6tmpM3zZNEIHWudm8t3gtQwryKCkzbN1dQm52Ohc+9TX92+VyyaiubN5ZzK9b93DSA59S7skqLzm8C3e/V8gZIzrwn8/8J+w+ec4w7pz1A3OXb4rZ+3LPKQOZ0D/iJbV9ichXxpjBvo9pIKi9SZMm8dprr9G9e3fS09PJysoiLy+PxYsX88MPP3DssceyYsUKdu/ezaWXXsrUqVOBwHIZ27dvZ+zYsRx44IF8+umntGnThtdee43s7OxKx0r0uar6bcXGnaSkCG1yg797u4rLKC0vR0RolBlcuNm4o5jCtdtpnZtF6ybZ/LB2GyWlhp6tckhLTeH66QvJSEthUIc83pi/mvMO6cT0eat48MOljO/bitlL1jL94pEUrt3B5p3FrNpiS8rHDmjNq/NWAVTKcPMapLNpZ0ml9I/sks8nhRvi8M4k3oT+rbnrNwNISalZX2FSBYK//m8h36/aGtNj9mrdmOsm9A77uLdG8P777zN+/HgWLFhQMcxz48aNNG3alF27djFkyBA++OAD8vPzgwJBly5dmDt3LgMGDODkk09m4sSJTJ48udKxNBCo6tq+p5THP/mZncVljOmzHze9sYhHpwwhIzWFTwrXk9cwg2e//IUrjurO4JtmAbDs1vG8t/hX7pv9k2+JtmFGKmP6tGJ3SRlvfBd+jMfoni2ZtejXuJ1bPLVqksXqLeFH6XVq3pCl63YAUJDfgGUbdgIwvl8r3phv35NmjTJYv7046Hm9WjXm7yf246vlm1i3bQ//nl0Y9Hhug3SePGcYvVo1ZkdxKU98uozFa7Zx2wn9Kmo9NVFVIKgT8wjqmqFDhwaN9b/77rt55ZVXAFixYgU//vgj+fn5Qc/p2LEjAwYMAGDQoEEsW7ZsbyVX7WWzvrcZ46AOeeQ1zIjqOTuLS7ntzcW0ycvm7JEd+eHX7dz/wU/sKi7l5/U76NGqMa0aZ3HmAQW0a9qAwrXbufTZb9iwvZg1WwOZ2X3v/wRAn+veqnSMZ+cE5ne6zSvh7Cgu46WvI11SmbgEgU7NG3J0v9YV7eu9WjXmkO7Nud85N69uLRtRVm44ul9rphxQwJF3fkiP/XIY1CGPO2f9yPH7t+Hlr+2o9b8c3YuJ/VvzyjdF9GndhBGd85m5YA3PzlnBXZMG8Pr81YjAiYPa8knheg7r3gIRYeOOYnKy0li+YQcLV23lmAFt+OvEPaSnptAkO50fft1GQX5DPvhhHV1bNKKgWUMA+rRpgjGGHq1yOLBLM0rKDGXlhv2aZFWkPycrnYsP7xrz9zBUvQsEVZXc95aGDRtW3H7//feZNWsWn332GQ0aNODQQw/1nQuQmZlZcTs1NZVdu3btlbSq2FuwcgvLNuxgdM+WfL96KwPb5VYM/S1cu53f/idQo1184xiy0lMB2yzz+vzVHN6jBTe8vpBhHfPZXVLGjO9WV5Q2Af42Y3GlY/7klEwf/vjneJ5aRJlpKewpLa+4f/2EXnz043oKmjXkkZC0PTd1OMM65TN32Ub++/lyBrbLZb8m2fy0bjvZ6akMKWhKs5wMMlJTWLV5Nx2aNeCntdtp2TiL1rnZXHJ4Fx7/dBmnDmtPg4w0rhrTgz2lZXS/ZiYXHtqZK47sDhDUlDLnz6Mrbl82uhsAd5w8IChdUw/uXHF7bN9WjO1rV76ZPLxDxfbDewQ6ops6wbxLixy6tLAjf5o1Cvye3dFAR/h0XosIR/erWZt/LNW7QJAIOTk5bNvmf8W/LVu2kJeXR4MGDVi8eDGff/75Xk6dqo3ycsPmXSW8+NUKThrUjpQUoUl2Olt2lbCntIy3F/7KgpVb2LCjmPlFm7n48K785dXKC+42a5TJ+u17Km3v8ZeZlbbdNtNm9LVt6z5h/7aVSu2PThnMTa8vYun6HRXb5vx5NM1zMvmuaAt//d9Crp3Qi5ysdC548isWrwl8r88Y0YFz2/xCm/6jWLRuN2u37uFPr3zHo1OGkNcgg/cWr+XUYe3ZsrOEz3/ewFG99wNgijPKZtrYHmzbXcqUx74kt0EGwzrZWvHggqYMLmhqD1JeDp+/CIPOhMzAcMp8J2Md2D6vYltaagq/PahT0PllLp3Fj+N+JPXAcTVuS98nbVoGpXugefe4vLwGghjIz89n5MiR9OnTh+zsbFq2DET+MWPG8MADD9CzZ0+6d+/O8OHDE5hSVZW3F65h1eZdjOvXiqz0VBpnpfPIxz9z84xFgC2Jt2/agA+uPJTTH/mC+UVbKr2GXxAAfINAtLq1bMSxA9uwq7iMe96z7ck99svhvEM6sWTNdi48rDN7SspZvmEHbfKyWbZ+J933yyGvQTrXTexF0cZdlJSV0zAzjS4tGnFItxb8/rl5TP92FW9ddjDNc2wm27dtE148u589aGZDZl52MPfOLuT2t5Yw79ojyN3wLTxyCmy8hN5H3kjv1vDZtFEV6Tx1WHv46A6atB3CUb0PqnQeaakp5DXM4LWLDwx/sj+9C2//Gdb/ABPvrv6b9fTJdrLSwZcFtpWXwa5N0LAZlBZDWnTNcQBsXwsNm0PoZM7ycti5Hhq1gBVzYNdG6HZU1a9Vsgtu3g8KDoKDr4BOh0aXhrISuKu/vX195e9cLNS7zuL6LpnO1c+y9Tt4d/FaThvWvqJJJZySsnKe+nw5peWGxz9dxgOTB9G7dWNmLlhDnzZNaNe0ATO+W83NbyxiR3Epm31GodRGlxaNOGlQW25505bwJw1px7BOTRnZpRlH3/0xa7ft4d5T9+eip79mfN9W3HZiP7LTU/m4cD1tdyyk42vHMWvsbI4cNgCwo3POf/Irrp/Qm16tGwcfbMNPMO9pOPyaypkW2Mzkkzth+IWQmgEf3wlDzoH0BrBuMbS2x+CveZCWDVcWwncvwP5nBF5v8Rvw7KnQbSyc+mzw66+eD3Mehq+fsPcv/wFyfMbxbymyGWvRXFi/BAafHfz4kjfhmUnQuC38YWHwY2sXQ0oqNPNpM9+0zD7nRqfv7dqNdl+AGX+ELx+E4x6CV6bCRV8GSta7NtuSdnoWvHsj9DkB2g2DlBTYvALu7AMNW8BJj0N+F/j4X9C4FXz+AGxbBVcuhdudWskFn0HzHvD3jpCWBWe8Bh/fAQf+Ae4bVjnNh1wN3Y6E/frZYJWeZd/H9OzAOf70HsycZj8jqFUg0M5iVed8vnQDbXKzKybluI659xO27Crh7zMX89iUIYzonM910xdyWPcWHNajBbfMWMSDHy7l9OEd+O/nweO7j7/vU/44pjs3vbGoYlsOO/ku67dcXPw7XmdElWnKbZAeFCxG92xBZloqX/y8geP3b8tDHy7lhfNHMLhDXkWfgDGGxtnpHNS1GW3zGsCm5bBrGV/+eTTGGESEIR1H0bxRJrL8E3j1Ag654DOY/jhQzpENCoEBsG4JTXNa8fx5ThrX/QBN2kCG0x/12DjYvsZmIiMvhdR0MMaTib8O790E236F9sNh9k22RLtrE8x/Dq4ohEbNwZRDyQ544EDY+BP87xKYthIyG9nXA1gzH25tD0ffCX2Ot9uemAC7NwferDkPw4BTYcd62LoSuoyC9Ibwr96QmgllTg2p+3j4ZzeY/BJ0PAT2bLfbtxbZTHfY+facIJCZDjsfjvpbIKPfscGWmIddEDj+lhWQ28EGli8ftNu+eizwXhTNsQHt9uCmJeb8n/3/u69hxZfO66+Fx8f5fyl+9dQA7x8Bf1gUeB9ePBvWLrTvr58PbrV/rpGXwid32ds5rWH09TZweRW+a9/LGNMaQR1Tn8916+4SXpu3ipLScm54/XsAbjuhL1e99F3Y53iH6kUrj61sIoc0yvg683way06+L+/AuOJbKu2bmiKUlRvO6FbKXzOfosd3p7CHDMb3a8U9kwZWbode+TX832Fw6beQV1D54Nc3sf/PeQca5EOTtvDhP2yb+H+OhQ0/wnkfwvTfwepv7b6X/2Azy74nwQkP29L9jc3sY9OK4PvX4DXPJP3xd9gA8PrvbSa+eTkUzoJ3roUOB0L7YfDRP6Hfb2yJc8c6OPttyG0Hd/h8t373NeR3hvkvwMu/DWxvf4Atubq1gJrY/0z7/J4Tbea96pvK+0x+2WZ+7nsHMPZ2GDYVln8Gj4W5/tWEu+HdG2zAA1taX1e5o71WjrzZNmX5yc6zgTaWJt5ja2k1oDUClVjG2FJSdl7Q5vJyw9L125n28nfMWWZ/MKelzuLm9Ef5O4+xm0yueuk7WrCJTimr+by8V6WX9gsCy7JO5f2y/sxsewl9Mtcws2QgH/+0GYBesowZmX+q9ByhnE7NGvD7I7pzUIPlfPfu0/Q9/XZyG2bBm1fBFw8AsCTrLa5tdT83tFsML91jmzh+8x9oPdC+0Lyn7P8f3oJh58G3z8Kyj2HoVFj5VeCAjxxh/4+5FT78uy3dbnZqMF89HggCAMs/sf+/ewHaDIbexwYee/EcKC8NeVP+ELi9+HV45TzPa31s/yC4pProkZXekwqlu6F4R3AQAPjlU/tXG24QKSvxDwIA3zwJM64M3jbreluDeWFK+NdeND0QBCD2QQBgTfhCSsyDAAR1oMeS1gjqmH3lXMud+fbeEnF5uWHOso3s1ySLDvm2yeKN+av5/rlruTL9eS5o9TwNm+7HpaO6cu5/5gaNSAHoKKt5O+OPpEsZg3bfzwZsCfDdjMvpnLKaN8uG8PfSSeSxjW4pRfSXn2gr62jXuRdZRR+z9cTnyVs3h+bv/j44sW0Gw5nTYUMhPHiw/wk172k7E5d/CrntYdPPMP6fMOdRW72vSs8J8Jsn7e03r4Yv7rclxQMuDi7FxsqAyTDPOZ6kQseDYels/31b9IK139fueBP/DdMvjm7fhi1sU0qyaNbNdmyHk9EIirdH/3opacGBvc2g4ALE5Jegy+jKz4tCUs0sru8Sda7Fz5xBauFb3Dbofa44sjs/3DSEWeWDGXbm3+jSohEzF67hpte/Z09pOZkpZSw+rxlzTQ/OeeAd3su8gmaylbF7bmGRsVeYy6AUwXBy6vu8VTaEUlL5Ouv8iuMtL2/BGSVXM66D4ao1l8fmJBo0Cy4hRiRAFL+P7uNsIHjhTFj0v8D2nhOC7yeD/C424O5tf14DXz0BM6+q/NhJj1ddc4ik+3hY4kywO/FR2/YPtpnNrV2F0/lw2/wW6rA/w+ybA/cLDoJlH9lAcOAfbC0R4PRX4ecPbH8JwJQZUDCyRqehTUPKKt4BKenhh8/t2W5HO6Tar8XW3SXklGxgzgevM3TJawA89OFSnv5wAQuyCumTUsi4h/vxvSkIepkrUp5BHpvBdXv+xvysQDPMm5nTKJEM0o2dcj/H9GCILObG9McrJaVDylo+OC038KOLhWoFAYgqCAAsmQFv/6Vypl/Xg0CXI6Dwncj7HXNvoI8iLSv8fmlZtpkpHtKzYfj5MPRcuKFpYPugKbbTuDrcTNk14qJAIMjxTP7qeHDkQPDTe3ak1o51sPR9+99N70FXwEf/sLW6gy4PHLPMsyRFWiYc+PtAIEiJT5adyAvT1BubN2/mvvvuq9Fz77zzTnbu3Bl5x0hmToMXgi/pULZ6AeVrPM0Cf2sNj4xmT2kZ016ez+pFn4ExfPTjOt5d9Cvc0obZN41lwj0fU3D1G/S7/m3m3D6RoXMDJfJUypiVGWivnZH5JzqLnaLflK2MSFlIb1kGQK5UrhK7QQBgiERos90d2zWjKhz/f7V7fovKfRV8fm/1XqPPibVLQ20dE/p99RlyOuz8yttC/fZd6OApoaZWcT3ths2rfq3fL7TDI0dda+8f8LvIxwfo6unfSHEyVZek2sw0kuOckUWDzgoMpXV5n5+eDcc/DAdfGShQSSpcE6Y5LK8AxtxiO/kvnhN4r9KzYf/T7e2MhoERUABt9g/czsq1o628x48DDQQxUO1AUFpsh/EZE7tA8Pl9sPBlKHLaE42h+MHDSXlgBN8uXxfYb/W3XHbtX7ll/kG0em4M5t+DufXR5zjnCdvcdpj5ku9WBsYqtyS4w+u4ttvZT4K3vesEhvcyL+eZjJsZ2tCuL/PUuQfU7pzcTtJY63gwXPxV5P3COeWZwO0WvaD/qdV/jeERrl3RMUxfhleGT8dha28mEqZ/Ir8LDDzNjtRx+WUwue2rPv5v34W2g22Ju2Vfuy21isla3gw1s3Hlx7Ny7f+BZ9hS+fCL7BDKUM08s2tb9oFTQoZnjrrWDi+t2Kc3HPtAVWcC/SfZIDThTntcL29wS8+GfifZ+RruuTZsFj7YnPdh4HZ2nt0X7GeXkh54TXEDgUCvY+CSeTD1A2jZy9bQp8yw78V+fas+jxrSQBADV199NT/99BMDBgzgyiuv5Pbbb2fIkCH069eP6667DoAdO3Ywfvx4+vfvT5++fXjuyce4+85/sWrVKg477DAOO+yw6A62a5Mdbugwd/Sm/DnPcLKHD4eZf4I188nGjtV+/qG/ccULgVEo92fcVXFbNhTyRuafOSTFM0oFaMguukgRHVKCSzp/7+rfcXpU6hxyxS5bkLbbLo0gX0T48UXy3Qu1e344aZnQ0LPoX8tq/Liy84IzsczGMPyC8PuH4x39cfS/Kj+e0Sj4/sFXVt7nqmXB99OyA6VpgKad8eU2T7T0rMvl16STlgE9jrYl8+MfhiNuCJ7Q1NZpbk5JsaVeiBAIPMdo2Qemvh/8uDsnolFzmPK6nbjlbYpxnfYCnPxfuG4zXPCJPX4oN5N1m/d6jA+frlCNW9magSs1A7o5Q1S9Gb57ru55/e5rOxnNddl3lYNxsVPoy2oCOa1g4Olw2ouVm3yadgyumRSMtE1Ecbpcbf3rI3jz6qqHdNXEfn1h7K1hH7711ltZsGAB8+bN4+233+bFF1/kyy+/xBjDxIkT+fDDD1m3bh2tW7fmjTfegF8XsmXTBpp0GcYdd93N7NmzadbMKSmU7oGdG+yXxO9D37MN3rsRGreBficjW4uQrSGrQH5+b1BTxc3pj3LlvDTnQqH+nsi4reJ2B1nDB5l/8N0v5bN/+25/MN0nM1v8evgDJlJadvAP+oDfVZ64UxVvIEhND19yHnBaYDhpqCzvazgZSpvBsNIZCOEGgpR0uNbp29izzfbjuCOGUkN+vtesCb7vZlA5rWCbZ5itm6kddIUtic6+yZZKQ9c5TEmDST7pb9rZTjbzKi8JPhewGWOTdnCT0yTkfc9NeWDfzCZwydf+3/f0kAA1+nr7fudFaPd3g4M7GKaqvgs/R94YmICWmgEnPGLnYnjnhoQGgvzOwc1fft+LEicQZDSwaTzG+T0Vza28716kNYIYe/vtt3n77bcZOHAg+++/P4sXL+bHhfPo26cP77zzDldddRUffTaXJo1zCGqXNcb+0DcUwvZfbUAwxs7KLNltawJbVwX2f/V8Fr4cPjiFuj39oaj3DRcEYkpSg+97mzQi8bYJu7qHmfl52ouVt6Vl2kyn7RAYcxs0dWaXRlvtTk2zTReu7Fw483924pOr93GV5k3Qc0LgtrdG4JYGvU0Qbum4Vb/AtrG3+dcewnGDzZBzAtsmv2QnJbnn0fcEe9vvPU0JU3I4/yO7tIJXhwNh4GSYEKhtkt85eGBCUGZsAhlpanqgySRUakiTy8jLoiwVu/uYwDG8+p8Kf1pFWKGfT2aj4PkbEEh/aLCqSrGz2J+33R9AEpsV178aQRUl973BGMO0adM477zz7Poha+bbB3Kb8fXXXzNjxgyuue1ORh04hGtvuTPwxJ0b7MxKV1mx/cJvX2v/fPRe8Pf4nUg0Rl5m16+JpNF+dvkDr4MutxN+RlxkZ9FWR5tB8OPbwdvG3GJH74TqegRcNAfuHRLY5mYkv50V2HbxV7Y07p18BeFnh/Y+1o7yMM6Syx0PDq6JnvQ4vH1N8HO8bfrpwUtnAMGl6fRsOGN65eBUnQXTDv6jLZUOu8AuLwG2JuktmTftZEvuDZoGSsCucCNUMhoGApU3XcdE6DAPqhGYqpuRXN4RNFDzppHQ521bXfkcwgmXTje4pFWjA7ffybB6XuVZ53EaDRQtrRFUx55t8OvCwHoophxMedAy1EcddRSPPvoo27dvh53rWbl6LWvXb2TVsp9okJXB5MmTufKCM/n6u8WwcwM5jRqybekcindsDjpUScluivfEaahdLHQ5Ag68LLp9h59f+ccy4FS46AtPqTnMUM12Pqu1+mWiuR1g9F/hlGcrP9a8W+Q0Nuvi/2Ns3MZ/f7d06wYCqFx6DS3lZXra/d2MafDZgeaLnP1syRrspKJOh9gM2s/A0/23B6W9FYy7PeS4Pgv15XcOE5hinDl5awTepqGq7PFf3j0i9/31zpPqeDAc/hd7O5oRUa6wgcBtGopiVJJr+IVwzbrgPioIHjWUAPWvRlAbe7bbD9db6tq+zi6AtV8/u0xCWbFdlCuzEaxZAKaM/NwOjNy/F3369GbsmLGcevLxjBgxAsqKaZSdyZP33EThssVceeJppKRlkZ5Szv23/Am2/8q5px7HmFMuoHXL5sx+MdB8I1tXky6BTMa7fthelZphS845reAfnlUfJ/s0uYTjl8G6X/xIJaGCA2FFyDUcQptcspvaN8cNTO2GwYov/F9v3D/8t4N/9bzScEjnQ3AztfKyyPu6QkfmuB2v37jt8GJrGss/rrxshN/zIvHLwMJlOO6+GTlQ7GS+sS6lhvYRRJP59T4Wfv4Q5vsE+Kh4AsGZzryOg68IbGvSPjiYe7Xqb5f6CDf6yk1/dYZ0ivjX6twAnZAfeZwDgYiMAe4CUoGHjTG3hjzeAXgUaA5sBCYbYyJf/y5eNvxoM4NWztrfaxdDqdODVlYCpU6H2NZVdiq9cTKBzct5+t6/2RLdzo1QVsylU8+AbYHmkM4F7TjqUDuc0iCI8wW9ZMoJXDLlhEpJSZPgL2eCvh/Q6TD7frhtmzXhl6G4X3y3HdoY266+8JXg/Zq0sRmfu1TDubODm2oO/4vPIlw+b9Ypz9p234Iq1sL3y5gqZaYhnY/eTCS0ZBgaWPxK3d7XkJTAe1VVIIiW35j+cF8kEdu+3+FA+Pcguy1cH0FNBb2eCZxr41bhn5PREI5/sAaBwKdG4Of3VQwsmfyy04QVJpt0J8f5dURHU2PzSnCNIG5NQyKSCtwLjAV6AaeISOhMnH8A/zHG9ANuACov/7i3uT9KYwJBwN1uPD/OPT6TnbatCbRpVjEZSqKdsRoqmqq0X9W/NtzSd7hMLBopaVRq+nEzAe8P4KTH4aQIK1m22T84sDTvYS8OEkn3sVUHAW+agraFyQzT/ZqGQj6f0B932JKj894EBYKyMPtWg9/3parvx6AptonMFesagTcIGWObvY59AE6NwzDhWJScGjazQ1nDKXECgd/n6tf5XpX6GgiAoUChMWapMaYYeBY4JmSfXoC7EMdsn8fja+fG8CXd0Orinq3BpYuNIaMmQrnDxGIpo1Fg0k043h9vddpBw3GHwNXmh+X3JXe3VZRaPZlhEJ/jel/Pt7ZR0w5Fn3SGGxrq20cQmvGGNg2FqxG4506MawTVaBry4zc+v1a8gcB53wacUnWNoNbHiuNaam6LQIZP3091v4OxLsBVUzwDQRvAMwyGImeb17eAc2ULjgNyRCSkFwVEZKqIzBWRuevWrQt9GLCjdapt8/LAyoHeEljpnsqBYNvqyNXMGDGhbeDudiR4/Lkfb3PAIT4LcIXjDqF0nfSEXVrZO/TQ5Q4/jJbflzxcH0E0GZVECAR+wSMafsfucIC9YEroa7ud31UFgpz9gu+HqxG4E7sKDo5tIPA7n0RmOEGZY5x/Sw2cbCRcZ38s9D3ZLi9++DWR942k4rNKTBtwokcNXQEcIiLfAIcAK4FKdWJjzEPGmMHGmMHNm1euqmVlZbFhw4aaBQOXd4LMliL/DqTS0Bk38SE+JUdjDBu27yErK8KYZe+Pv6p1X0JNftleyANsh2HvY+3Mz9DMDGznbHX4vZehfQQV26P4SgbVCPwyu5rWCHyOLQLthlbe7rYbe79zoYFg0Fl2IpI7BDRc017bwfbiM/1/EzifstheNrNCNO9vi96R96mteJepeoy3K4X6zciOlfQsOyorO7fyY9XNixI8fDSeR18JtPPcb+tsq2CMWYVTIxCRRsAJxpjN1T1Q27ZtKSoqIlxtwVfp7sD4/C2LYPMvgcdSNoH5KaY1gDU0o2XmHiSa4XDZJXbYXHq2Z/icIStzK237RliC1puxVqezL7OxXYMGqshInWWZq9ue6ZepVdQIQmaAVjsQxPArHO68/L4Hbjq9QS60KSUlBfqeaK8PDFUHZvf6vu4+segj8BPNZ3f2TDupsaZG/9V/7oU3EA6cXPPXj4aIvf5wXZHgpqF4BoI5QFcR6YgNAJOAoNW5RKQZsNEYUw5Mw44gqrb09HQ6duxYvSd5LxgyZQY8d3JNDh21ntdvsUsVf3p35J3H/QOGnWtve9M56jpIPxTO+8hebCR08hMEZ4zejMe7VLAfkciZakZDe5GN6s6CDJ0UFJTOkKAT+tp+QUki1Ahq3DRUjZ+DXyAIx21LjiYw5ztDdDsdGn1aIDA0MpJoPrusxpGbIKsSbn7J6Ott38qRN1Zv7H0yqK+dxcaYUuBi4C1gEfC8MWahiNwgIu6yh4cCS0TkB6AlcLPvi8Wbd+mGeIo2o/H7UnQbGyhFtepnV0v04838q1NyjiYQuI97MxN3gk5V/AJBpRKQWyMIycRbDfBJR4Tz6nxo5DT5CVsq86kRuG3Pfn0oodymtGjGm7foYZuJhp4beV+vSIMIXInMcBo0hXF/1yDgpz5PKDPGzABmhGy71nP7RaAaM5PipKQWY+Sr0qJ38GUOow0EfhnSqVGOow77hYpUSpbIHVYVIyI8geDgK2DHent5xnCqahoKzfjd125/gF1l0jsrtmKfCDWCAy8PLKlQHd7XymzsP0TYTW92bviJXd41hcC2VS+aHmh6i8RtJoqHBK9pU2OtBsDaRYlORfzU46ahuqPYf6jnt+Wd6J+ylAWpPelTVoMv4SF/tGORf3WCwaAzA5egq0ptSgfhgk2kUlhQjSBc34hPIIDIzSPlPoEgNAC4h/T+ILxB4ILPAtfCjVQjqOmwR/fYjVra68K6K4dWp6/ous2Vzy2npS3hb4njXMloO8gTkeGc/krt+9vO+yA2aYm7Wg5USNDM0TpaPIixt6b5bt5mbFW+z6jTqvd63vXtCw6EYU5bfpO2dn33SGrzYw3XDt3j6MorHgYdMyW65iN3X69IgSArt4qMIGS8d7gfRMtegXZzbzpjmbFVBJBa/Bir+iHHtTQeZZoT0QTR+XDoMmrvH7cucb/T7qoGe/vwCTnqvqCqTBHYY9Joked0mOV3tWvLR6tpgf1fnejuvVqU98fabnjwksd+zn7L81yfzDynlR3uOKqq9nzx9C9UNWqIyplJVYFg5KVVT2xz15U/4JIq0haaDM/XNprO2qhf1z2v0KAVo9Fj8QwE+3KNQEWWlgFnzYRTn0/M4RNy1H1BelaVfQPp2Tl0a9EIthL+Rzb6eph1feXbNakGj/+nbUeG4AzjnLf89/fK8czMDG0WuejLwMUyqsqIJCVyabGitF6NGsH+Z1a9imVWkzBt7VVkbN50hhtvf+XS6lezvUHU+xnGKgOPaybsOdepH4Sf2V5X+wiSQYcRCTt08n4rwszcfNnYS0amZDQMrCcTLoP0Ljbldw3WSNX1NoM9u3qOUd0fa1D6Qo7ZvHtgKeMqA0EUo4Zq0jQUj4zH+1759T+AXeY33BLO4YT7nEdeGrlWFo29VSNoPcDOiPYT82UjlL+9swpBrCTtt8KU+QeCjWVO5p7REMbfYa8T2ukw/1J+uFmlLfvY/zkR1lDx/nhr8wP1K2m6afCqssTvCQRh45cbCEKbhkImP3kvphKPGZPe84hlm6r3AuIHX2FHqvQYb2stE6OY/xHx9RO1hCz2Yj5KhZFcTUMrv4bln0DTzkiYZqHDe7WBH7CBoFEL2+QTljcQeNf4+aPtHGs3pPJTgjIDz+3alBaDMnhjR9g09rnod8SmIXfZhHD7hKsReJ7QeiD85kn4V2+ftEUjipKUN7hUZy34iK/rOa/8zsEjVWLRrBPXjtoIQeb0V+I7aknVackVCP7vsIi7dGrZJBAIvNxmoo4Hw/pC2LYqOAP03k5J9V+fJpQ3KNQmowl9bsvQ1b4dVTbhSBQZVZiVFUNfN9yFPKojEaNvqvoMYnHMRHYWZzWJzeeiqpbIWl8tJG3TUFju8MvQQDDqWhhxMZz2kr2AChBUet21McoDhKkR1GrugHcUTRUl6irXr4niC+x+ySONGsrMgcwaZjrRdLTHq2RdVTPWvh4IErRqpQqR64yCq+7CjAmmgSCUO8IldAJWdh4cdbMd5uV3PdSdG6p/rJaeVR5rk0lISNNQOFVlspISWC9/8Fnhdgrs6+X90rd3OikrZsfGIYOK1+ibSE1n9kZ8Xr+26mhJtN45/C8w6RnoGIPBBXtRcjUNRcMtFYZeiNyXJ2MdcBp89M/IT+k/yQ4TPfYB6HM8zH3Ebq/VJDLPx9i2iiap0E5dLxEb/K5ZF36VTAnTWTzqWmja0TabucsonPYCLHzVc9GRGI6iiFtbe4RAWVs6hr/+S8uAHuMSnYpq00AQym0a8rvAtMuvRpDfObrX7zHef9x8rZqGPM8dc2v4/SL1EUB05x2aKWY0CMyeduUVhF+FsrbivXa774qnMShxa9OQ2kdpIAgVVY0gDpfBq01G42Yw2XlVZ+S1noUbJhBUx3EPQdmeWiajjpastWlI7aM0EIRym0+qHLXiUyNIpNR0WxPoMrrq/Wp7sZNwncXV0fWICBO9ouksdjLUaJdejlajlnZNpqqWu6jVtZu1S07tm5IqEJiUdMRvJmrTTrYpo2UfaNbNbguzIikQmDwWixLeBZ95rkJWC8MviLxPVX0EUYlF80i0a+JE2O/Y+6H98NqnxyslFSY9FdvX9ONefCaWtEagaiGpAsGeBq3I2v4L80fcRb+R42D1t/DBbTDpaTt5DKC83E4i6xfmwi9glxzYtdleuHrjUsh1rsjZ9ajqj9UON+Y/HmrbNBSLmlCk4OnOxq6q0xtgwKlVP76vmvIGNOsehxfWQKBqLqkCQXl5KS+VHUTv/sdDo8a2maLrEcE7paTYZSWqkpkD4/9hb3uXHjgtVisHxqnJqTwkEDTaD7avqcYLxKBvJFIgaN4dLvw8UDPbV7jrSoVbwydaBQfWPi1KxVhSNVqa8nLKjZDXoIoO1frMWyMYdBZc+Fn1nl9xnd44BgKAFj0Tfum+SrIa22a84x5MdEr8adOQqoUkCwRllJFCboMoLiKeCAddYf9Hu0BYzwmVL4tYFbePoMtomHBn9VfnlL1QI9iXtewV27WNYkoDgaq5pGoawpSTkpJCZto+Vtp0jfpLhIvHhPjNk9V7fbdZY+Rl1XtehViMltIMKy60RqBqoQ4Xz2rAlO97TQ57U+fDYVpRzae/H3GD7QzPbV/950Y1U1vVnAYCVXNJVSMQU153JyPFSmZOzZ/bfQxc/UvNnnvO27Dof4FVXFVsaY1A1UJcawQiMkZElohIoYhc7fN4exGZLSLfiMh8EYnrIh1COaKTehKjZS849KpEp0Ip5SNuuaKIpAL3AmOBXsApIhI6aP4a4HljzEBgEnBfvNIDbo1AA4Gqj7RGoGounrniUKDQGLPUGFMMPAscE7KPAdyL/TYBVsUxPTYQJHMfgaq/tGlI1UI8A0EbYIXnfpGzzet6YLKIFAEzgN/5vZCITBWRuSIyd926dTVOkFCOJHsfgaqnNBComkt0O8kpwOPGmLbAOOC/IpXbbowxDxljBhtjBjdv3rzGBxOjfQSqntIagaqFeOaKK4F2nvttnW1e5wDPAxhjPgOygGbxSlCK1giUUqqSeAaCOUBXEekoIhnYzuDpIfv8AowCEJGe2EBQ87afCFIw2keg6imtEaiai1sgMMaUAhcDbwGLsKODForIDSIy0dntcuBcEfkWeAaYYkycFvk3hhTszGKl6h1tGlK1ENcJZcaYGdhOYO+2az23vwdGxjMNngPb/1ojUPWSBgJVc8lTPHYWXEtJTZ5TVklEawSqFpInV3SXYJakWlVDJQ0NBKrmkicQlEdxLWKllEpCyRMInBqBSaJTVklECziqFpInV3QvyqJrDSmlVJDkyRWdGkG5TihT9ZHWCFQtJE8gcC/crjUCVS9pIFA1lzy5ottHoIFAKaWCJE+uqH0Eqj7TpiFVC8mTK2qNQNVrGghUzSVPrliuNQJVj2mNQNVC8uSKFTUCHTWk6iMNBKrmkigQ2BqB0R+Mqo+0RqBqIYkCgbP6qNYIlFIqSBIFAncegZacVH2k32tVc8mzFGdFjSB5Yl9UTv4v7NyQ6FSo2tICjqqFqAKBiLwMPAK8aYxbtK5rbCDQn0uIXhMj76PqAP1mq5qLtnh8H3Aq8KOI3Coi3eOYpvioqBHoD0bVQ/q9VrUQVSAwxswyxpwG7A8sA2aJyKcicpaIpMczgbEm+oNR9ZJ+r1XNRd1gLiL5wBTgt8A3wF3YwPBOXFIWcybRCVBKqX1StH0ErwDdgf8CE4wxq52HnhORufFKXEw5TUOSop3Fqh7Smq6qhWhHDd1tjJnt94AxZnAM07MX6A9G1Uf6vVY1F23xuJeI5Lp3RCRPRC6M9CQRGSMiS0SkUESu9nn8XyIyz/n7QUQ2R53yatOmIVWPaY1A1UK0geBcY8xm944xZhNwblVPEJFU4F5gLNALOEVEenn3Mcb83hgzwBgzALgHeDn6pFeT2zSkPxhVL+n3WtVctIEgVTw5qJPJZ0R4zlCg0Biz1BhTDDwLHFPF/qcAz0SZnprTQKCUUkGiDQQzsR3Do0RkFDbDnhnhOW2AFZ77Rc62SkSkA9AReC/K9NSArRGkaMlJ1UdawFG1EG1n8VXAecAFzv13gIdjmI5JwIvGuJcRCyYiU4GpAO3bt6/ZESomlNXs6Urt0zQQqFqIKhA4y0rc7/xFayXQznO/rbPNzyTgoiqO/xDwEMDgwYNr2eurPxillPKKdh5BV+AWbKdvlrvdGNOpiqfNAbqKSEdsAJiEXaYi9LV7AHnAZ9Enu/qMKUfQzmKllAoVbR/BY9jaQClwGPAf4MmqnmCMKQUuBt4CFgHPG2MWisgNIuJd6WwS8KwxJq7jOyteXQOBUkoFibaPINsY866IiDFmOXC9iHwFXFvVk4wxM4AZIduuDbl/fTXSW2PG7SzWOKCUUkGiDQR7RCQFu/roxdimnkbxS1bslRuDvTaZRgKllPKKtmnoUqABcAkwCJgMnBmvRMVFxSrUutaQUkp5RawROJPHfmOMuQLYDpwV91TFQXl5Hb2ejlJKxVnE4rEztv/AvZCWONN5BEop5SfaPoJvRGQ68AKww91ojInf2kAxZrRpSCmlfEUbCLKADcDhnm2GeC4SF2PupZa1QqCUUsGinVlcJ/sFvIyuPqqUUr6inVn8GD4L+htjzo55iuKkIvEaCJRSKki0TUOve25nAccBq2KfnPgx5XphGqWU8hNt09BL3vsi8gzwcVxSFCcVfQTaWayUUkFqmit2BVrEMiHxpmsNKaWUv2j7CLYR3EewBnuNgjpE1xpSSik/0TYN5cQ7IfFWHphIkNiEKKXUPiaqpiEROU5Emnju54rIsXFLVRzo8FGllPIXbR/BdcaYLe4dY8xm4Lq4pCheAp0ECU2GUkrta6INBH77RTv0dJ8Q6CvWQKCUUl7RBoK5InKHiHR2/u4AvopnwmLNnUegcUAppYJFGwh+BxQDzwHPArup4mLz+6LAWkMaCVQ90v+URKdA1QPRjhraAVwd57TEVaBpSCeUqXrkuAfsn1K1EO2ooXdEJNdzP09E3opbquLAuBem0QqBUkoFibZ43MwZKQSAMWYTdWxmsUs7i5VSKli0gaBcRNq7d0SkAJ/VSPdl5Tp8VCmlfEU7BPTPwMci8gE2Jz0ImBq3VMWD0VFDSinlJ6oagTFmJjAYWAI8A1wO7Ir0PBEZIyJLRKRQRHw7m0XkZBH5XkQWisjT1Uh7tRjcQKCdxUop5RXtonO/BS4F2gLzgOHAZwRfujL0OanAvcARQBEwR0SmG2O+9+zTFZgGjDTGbBKRuPU76BITSinlL9ri8aXAEGC5MeYwYCCwOcJzhgKFxpilxphi7PyDY0L2ORe41+l8xhizNtqEV5epWHQuXkdQSqm6KdpAsNsYsxtARDKNMYuB7hGe0wZY4blf5Gzz6gZ0E5FPRORzERnj90IiMlVE5orI3HXr1kWZ5GCBOKCRQCmlvKLtLC5y5hG8CrwjIpuA5TE6flfgUGyz04ci0tc7VBXAGPMQ8BDA4MGDazhaSTuLlVLKT7Qzi49zbl4vIrOBJsDMCE9bCbTz3G/rbPMqAr4wxpQAP4vID9jAMCeadFVHYK0h7SxWSimvaueKxpgPjDHTnXb/qswBuopIRxHJACYB00P2eRVbG0BEmmGbipZWN03RMOg8AqWU8hO34rExphS4GHgLWAQ8b4xZKCI3iMhEZ7e3gA0i8j0wG7jSGLMhTukBtGlIKaVCxfWaAsaYGcCMkG3Xem4b4A/OX1wFmoY0EiillFfSNZhrH4FSSgVLmlyxvOJ6BEoppbySJhAEJhJoKFBKKa+kCQTumKGUFA0ESinllTyBwNSpVbOVUmqvSbpAoJ3FSikVLHlyRe0jUEopX0kTCIyuNaSUUr6SJxA4NYIUjQRKKRUkiQJBolOglFL7pqQJBGhnsVJK+UqaXNFoZ7FSSvlKmkDgTinTPgKllAqWNIFAL16vlFL+kigQuLc0ECillFfyBAKdR6CUUr6SJxC4y1BrJFBKqSBJEwjc5UclJXlOWSmlopE0uWJFZ3GC06GUUvuapAkEOqFMKaX8JU2uGOgs1jqBUkp5JU8g0MWGlFLKV1wDgYiMEZElIlIoIlf7PD5FRNaJyDzn77dxS4yuPqqUUr7S4vXCIpIK3AscARQBc0RkujHm+5BdnzPGXByvdLgq6gN6zWKllAoSzxrBUKDQGLPUGFMMPAscE8fjVc2tEei4IaWUChLPQNAGWOG5X+RsC3WCiMwXkRdFpJ3fC4nIVBGZKyJz161bV6PElLudxVojUEqpIInuLP4fUGCM6Qe8Azzht5Mx5iFjzGBjzODmzZvX7EjaWayUUr7iGQhWAt4SfltnWwVjzAZjzB7n7sPAoLilpuJyBImOfUoptW+JZ644B+gqIh1FJAOYBEz37iAirTx3JwKL4pUYXXROKaX8xW3UkDGmVEQuBt4CUoFHjTELReQGYK4xZjpwiYhMBEqBjcCUeKVHh48qpZS/uAUCAGPMDGBGyLZrPbenAdPimQbPsZxbGgiUUsoraRrM3TiQoquPKqVUkKTJFQPXI0hwQpRSah+TNIEgMLdYI4FSSnklTyDQC9MopZSvpMkV9cI0SinlL2kCgVsl0OGjSikVLGkCQXnFsCENBEop5ZU0gaCijyCxqVBKqX1O0gSCwBITSXPKSikVlSTKFXWtIaWU8pM8gaBiraHkOWWllIpG0uSKgaWGtEqglFJeSRQIdPioUkr5SZpAgLPWkNYIlFIqWPIEAodoIFBKqSBJEwh0PplSSvlLmkAA7jLUGgmUUsoraQJBVnoqoBPKlFIqVNLkikML8gDIdgKCUkopK2kCQUUngTYNKaVUkOQJBBU0ECillFcSBQITeRellEpCcQ0EIjJGRJaISKGIXF3FfieIiBGRwfFMj3OwuB9CKaXqkrgFAhFJBe4FxgK9gFNEpJfPfjnApcAX8UoL4FlsSCmllFc8awRDgUJjzFJjTDHwLHCMz343ArcBu+OYFg+tESillFc8A0EbYIXnfpGzrYKI7A+0M8a8UdULichUEZkrInPXrVtXu1Rp05BSSgVJWGex2JlddwCXR9rXGPOQMWawMWZw8+bNa3ZAbRpSSilf8QwEK4F2nvttnW2uHKAP8L6ILAOGA9Pj12FccUGC+Ly8UkrVUfEMBHOAriLSUUQygEnAdPdBY8wWY0wzY0yBMaYA+ByYaIyZG8c0adOQUkqFiFsgMMaUAhcDbwGLgOeNMQtF5AYRmRiv41aRoL1+SKWUqgvS4vnixpgZwIyQbdeG2ffQeKZFJ5QppZS/JJpZ7NCmIaWUCpI8gUCbhpRSylfyBAIdNaSUUr6SKBA4tGlIKaWCJE8g0KYhpZTylTyBoILWCJRSyiuJAoHWCJRSyk/yBAK9VKVSSvlKnkBQQQOBUkp5JVEg0KYhpZTykzyBIL8r9DoWUuK6qoZSStU5yZMr9hhn/5RSSgVJnhqBUkopXxoIlFIqyWkgUEqpJKeBQCmlkpwGAqWUSnIaCJRSKslpIFBKqSSngUAppZKcmDq2Tr+IrAOW1/DpzYD1MUxOXaDnnBz0nJNDbc65gzGmud8DdS4Q1IaIzDXGDE50OvYmPefkoOecHOJ1zto0pJRSSU4DgVJKJblkCwQPJToBCaDnnBz0nJNDXM45qfoIlFJKVZZsNQKllFIhNBAopVSSS5pAICJjRGSJiBSKyNWJTk+siEg7EZktIt+LyEIRudTZ3lRE3hGRH53/ec52EZG7nfdhvojsn9gzqBkRSRWRb0Tkded+RxH5wjmv50Qkw9me6dwvdB4vSGjCa0hEckXkRRFZLCKLRGREEnzGv3e+0wtE5BkRyaqPn7OIPCoia0VkgWdbtT9bETnT2f9HETmzOmlIikAgIqnAvcBYoBdwioj0SmyqYqYUuNwY0wsYDlzknNvVwLvGmK7Au859sO9BV+dvKnD/3k9yTFwKLPLcvw34lzGmC7AJOMfZfg6wydn+L2e/uuguYKYxpgfQH3vu9fYzFpE2wCXAYGNMHyAVmET9/JwfB8aEbKvWZysiTYHrgGHAUOA6N3hExRhT7/+AEcBbnvvTgGmJTleczvU14AhgCdDK2dYKWOLcfhA4xbN/xX515Q9o6/w4DgdeBwQ72zIt9PMG3gJGOLfTnP0k0edQzfNtAvwcmu56/hm3AVYATZ3P7XXgqPr6OQMFwIKafrbAKcCDnu1B+0X6S4oaAYEvlavI2VavONXhgcAXQEtjzGrnoTVAS+d2fXgv7gT+CJQ79/OBzcaYUue+95wqztd5fIuzf13SEVgHPOY0hz0sIg2px5+xMWYl8A/gF2A19nP7ivr9OXtV97Ot1WeeLIGg3hORRsBLwGXGmK3ex4wtItSLccIicjSw1hjzVaLTshelAfsD9xtjBgI7CDQVAPXrMwZwmjWOwQbB1kBDKjefJIW98dkmSyBYCbTz3G/rbKsXRCQdGwSeMsa87Gz+VURaOY+3AtY62+v6ezESmCgiy4Bnsc1DdwG5IpLm7OM9p4rzdR5vAmzYmwmOgSKgyBjzhXP/RWxgqK+fMcBo4GdjzDpjTAnwMvazr8+fs1d1P9tafebJEgjmAF2dEQcZ2E6n6QlOU0yIiACPAIuMMXd4HpoOuCMHzsT2Hbjbz3BGHwwHtniqoPs8Y8w0Y0xbY0wB9nN8zxhzGjAbONHZLfR83ffhRGf/OlVyNsasAVaISHdn0yjge+rpZ+z4BRguIg2c77h7zvX2cw5R3c/2LeBIEclzalNHOtuik+hOkr3YGTMO+AH4CfhzotMTw/M6EFttnA/Mc/7GYdtH3wV+BGYBTZ39BTuC6ifgO+yojISfRw3P/VDgded2J+BLoBB4Ach0tmc59wudxzslOt01PNcBwFznc34VyKvvnzHwV2AxsAD4L5BZHz9n4BlsP0gJtvZ3Tk0+W+Bs5/wLgbOqkwZdYkIppZJcsjQNKaWUCkMDgVJKJTkNBEopleQ0ECilVJLTQKCUUklOA4FSe5GIHOqumKrUvkIDgVJKJTkNBEr5EJHJIvKliMwTkQed6x9sF5F/OWvkvysizZ19B4jI58768K941o7vIiKzRORbEflaRDo7L99IAtcWeMqZOatUwmggUCqEiPQEfgOMNMYMAMqA07ALn801xvQGPsCu/w7wH+AqY0w/7GxPd/tTwL3GmP7AAdjZo2BXiL0Me22MTtg1dJRKmLTIuyiVdEYBg4A5TmE9G7voVznwnLPPk8DLItIEyDXGfOBsfwJ4QURygDbGmFcAjDG7AZzX+9IYU+Tcn4ddi/7juJ+VUmFoIFCqMgGeMMZMC9oo8peQ/Wq6Pssez+0y9HeoEkybhpSq7F3gRBFpARXXj+2A/b24K1+eCnxsjNkCbBKRg5ztpwMfGGO2AUUicqzzGpki0mBvnoRS0dKSiFIhjDHfi8g1wNsikoJdFfIi7AVhhjqPrcX2I4BdJvgBJ6NfCpzlbD8deFBEbnBe46S9eBpKRU1XH1UqSiKy3RjTKNHpUCrWtGlIKaWSnNYIlFIqyWmNQCmlkpwGAqWUSnIaCJRSKslpIFBKqSSngUAppZLc/wPfbxQw4qg5DwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history1.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "honey-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.621372938156128,\n",
       "  1.1458256244659424,\n",
       "  0.7838808298110962,\n",
       "  0.6040056943893433,\n",
       "  0.4816644787788391,\n",
       "  0.4405907690525055,\n",
       "  0.4274061322212219,\n",
       "  0.38939741253852844,\n",
       "  0.3718048632144928,\n",
       "  0.35667702555656433,\n",
       "  0.3269493877887726,\n",
       "  0.33074742555618286,\n",
       "  0.3108063340187073,\n",
       "  0.31061071157455444,\n",
       "  0.29421374201774597,\n",
       "  0.3017352521419525,\n",
       "  0.286801815032959,\n",
       "  0.26742371916770935,\n",
       "  0.28820642828941345,\n",
       "  0.256865918636322,\n",
       "  0.27299764752388,\n",
       "  0.2675818204879761,\n",
       "  0.25190478563308716,\n",
       "  0.2605530023574829,\n",
       "  0.2539798617362976,\n",
       "  0.24876295030117035,\n",
       "  0.24865499138832092,\n",
       "  0.25539469718933105,\n",
       "  0.24183157086372375,\n",
       "  0.24466921389102936,\n",
       "  0.24921825528144836,\n",
       "  0.2476324886083603,\n",
       "  0.23838376998901367,\n",
       "  0.24041683971881866,\n",
       "  0.23448748886585236,\n",
       "  0.23676437139511108,\n",
       "  0.23321036994457245,\n",
       "  0.23329782485961914,\n",
       "  0.23611228168010712,\n",
       "  0.23831787705421448,\n",
       "  0.23508571088314056,\n",
       "  0.22333475947380066,\n",
       "  0.22874325513839722,\n",
       "  0.2258015125989914,\n",
       "  0.232664093375206,\n",
       "  0.21865974366664886,\n",
       "  0.2241836041212082,\n",
       "  0.21938876807689667,\n",
       "  0.23655177652835846,\n",
       "  0.21958892047405243,\n",
       "  0.22112901508808136,\n",
       "  0.22580774128437042,\n",
       "  0.21741892397403717,\n",
       "  0.21378175914287567,\n",
       "  0.21598733961582184,\n",
       "  0.22360508143901825,\n",
       "  0.21527795493602753,\n",
       "  0.22092477977275848,\n",
       "  0.20692183077335358,\n",
       "  0.2286597639322281,\n",
       "  0.2096651792526245,\n",
       "  0.21737903356552124,\n",
       "  0.21478316187858582,\n",
       "  0.20950964093208313,\n",
       "  0.2078467160463333,\n",
       "  0.2087130844593048,\n",
       "  0.21728281676769257,\n",
       "  0.2161635309457779,\n",
       "  0.21034912765026093,\n",
       "  0.21174703538417816,\n",
       "  0.2094372659921646,\n",
       "  0.20075060427188873,\n",
       "  0.2085181623697281,\n",
       "  0.20368826389312744,\n",
       "  0.20785872638225555,\n",
       "  0.21217404305934906,\n",
       "  0.21235060691833496,\n",
       "  0.20626240968704224,\n",
       "  0.2087075263261795,\n",
       "  0.20343008637428284,\n",
       "  0.20321330428123474,\n",
       "  0.2194567769765854,\n",
       "  0.2058880627155304,\n",
       "  0.20332971215248108,\n",
       "  0.2162332385778427,\n",
       "  0.20039355754852295,\n",
       "  0.21156808733940125,\n",
       "  0.20744241774082184,\n",
       "  0.19988597929477692,\n",
       "  0.20214956998825073,\n",
       "  0.20744971930980682,\n",
       "  0.20551282167434692,\n",
       "  0.19065165519714355,\n",
       "  0.20668253302574158,\n",
       "  0.20100761950016022,\n",
       "  0.20399866998195648,\n",
       "  0.20559538900852203,\n",
       "  0.19663982093334198,\n",
       "  0.20373141765594482,\n",
       "  0.19663268327713013,\n",
       "  0.21049657464027405,\n",
       "  0.20472083985805511,\n",
       "  0.20314690470695496,\n",
       "  0.19347859919071198,\n",
       "  0.20305782556533813,\n",
       "  0.20973172783851624,\n",
       "  0.19972684979438782,\n",
       "  0.19951465725898743,\n",
       "  0.20596224069595337,\n",
       "  0.20184123516082764,\n",
       "  0.20384839177131653,\n",
       "  0.22359530627727509,\n",
       "  0.20093579590320587,\n",
       "  0.1934310793876648,\n",
       "  0.201543927192688,\n",
       "  0.19542880356311798,\n",
       "  0.19607873260974884,\n",
       "  0.20022276043891907,\n",
       "  0.19920708239078522,\n",
       "  0.19988791644573212,\n",
       "  0.1992415487766266,\n",
       "  0.20538684725761414,\n",
       "  0.19476693868637085,\n",
       "  0.20617973804473877,\n",
       "  0.19625316560268402,\n",
       "  0.20111475884914398,\n",
       "  0.18699520826339722,\n",
       "  0.19604003429412842,\n",
       "  0.1952100247144699,\n",
       "  0.193948432803154,\n",
       "  0.198674276471138,\n",
       "  0.19787582755088806,\n",
       "  0.2032736986875534,\n",
       "  0.19297786056995392,\n",
       "  0.19893193244934082,\n",
       "  0.1902375966310501,\n",
       "  0.19624529778957367,\n",
       "  0.18784083425998688,\n",
       "  0.18950963020324707,\n",
       "  0.19924980401992798,\n",
       "  0.2025468349456787,\n",
       "  0.21286796033382416,\n",
       "  0.1946377456188202,\n",
       "  0.19159233570098877,\n",
       "  0.19330056011676788,\n",
       "  0.185867041349411,\n",
       "  0.18945714831352234,\n",
       "  0.19259761273860931,\n",
       "  0.20121833682060242,\n",
       "  0.18892629444599152,\n",
       "  0.19282133877277374,\n",
       "  0.1932399868965149,\n",
       "  0.19140653312206268,\n",
       "  0.19994759559631348,\n",
       "  0.20150132477283478,\n",
       "  0.18719491362571716,\n",
       "  0.1925608366727829,\n",
       "  0.19979731738567352,\n",
       "  0.18734927475452423,\n",
       "  0.19244857132434845,\n",
       "  0.18803781270980835,\n",
       "  0.19226974248886108,\n",
       "  0.19531352818012238,\n",
       "  0.19387373328208923,\n",
       "  0.1819852888584137,\n",
       "  0.19603568315505981,\n",
       "  0.19863823056221008,\n",
       "  0.19155694544315338,\n",
       "  0.19557107985019684,\n",
       "  0.18267497420310974,\n",
       "  0.18961140513420105,\n",
       "  0.19983921945095062,\n",
       "  0.1869499683380127,\n",
       "  0.2102997601032257,\n",
       "  0.17895594239234924,\n",
       "  0.20091791450977325,\n",
       "  0.19598419964313507,\n",
       "  0.187698632478714,\n",
       "  0.18725672364234924,\n",
       "  0.1922406703233719,\n",
       "  0.1885637789964676,\n",
       "  0.18521708250045776,\n",
       "  0.19569939374923706,\n",
       "  0.1818346083164215,\n",
       "  0.1847497820854187,\n",
       "  0.18504029512405396,\n",
       "  0.17694619297981262,\n",
       "  0.1791619211435318,\n",
       "  0.1741291731595993,\n",
       "  0.17920488119125366,\n",
       "  0.1809660792350769,\n",
       "  0.1809173971414566,\n",
       "  0.19262996315956116,\n",
       "  0.1880715936422348,\n",
       "  0.1946888566017151,\n",
       "  0.1764918863773346,\n",
       "  0.17166702449321747,\n",
       "  0.1960296779870987,\n",
       "  0.18649692833423615,\n",
       "  0.1844787895679474,\n",
       "  0.17445267736911774,\n",
       "  0.18620915710926056,\n",
       "  0.1813725233078003,\n",
       "  0.17050819098949432,\n",
       "  0.17742951214313507,\n",
       "  0.1873467117547989,\n",
       "  0.16963036358356476,\n",
       "  0.19516469538211823,\n",
       "  0.17298497259616852,\n",
       "  0.21216744184494019,\n",
       "  0.18985460698604584,\n",
       "  0.1700088083744049,\n",
       "  0.18557102978229523,\n",
       "  0.17739048600196838,\n",
       "  0.173308864235878,\n",
       "  0.1792629063129425,\n",
       "  0.1765284240245819,\n",
       "  0.21018517017364502,\n",
       "  0.17605161666870117,\n",
       "  0.18450602889060974,\n",
       "  0.1776379495859146,\n",
       "  0.171014666557312,\n",
       "  0.17257769405841827,\n",
       "  0.16906589269638062,\n",
       "  0.17300693690776825,\n",
       "  0.17989283800125122,\n",
       "  0.20092903077602386,\n",
       "  0.17730671167373657,\n",
       "  0.177787184715271,\n",
       "  0.17345881462097168,\n",
       "  0.18028101325035095,\n",
       "  0.16440485417842865,\n",
       "  0.17705948650836945,\n",
       "  0.16253653168678284,\n",
       "  0.17029345035552979,\n",
       "  0.20805908739566803,\n",
       "  0.1614857316017151,\n",
       "  0.17130428552627563,\n",
       "  0.18085557222366333,\n",
       "  0.1737975925207138,\n",
       "  0.18693245947360992,\n",
       "  0.1760827600955963,\n",
       "  0.1642172932624817,\n",
       "  0.1774531751871109,\n",
       "  0.16930674016475677,\n",
       "  0.1842927485704422,\n",
       "  0.17157959938049316,\n",
       "  0.15992289781570435,\n",
       "  0.21147972345352173,\n",
       "  0.17313799262046814,\n",
       "  0.1713358759880066,\n",
       "  0.16927285492420197,\n",
       "  0.17150983214378357,\n",
       "  0.19376368820667267,\n",
       "  0.17063218355178833,\n",
       "  0.1699708104133606,\n",
       "  0.17520679533481598,\n",
       "  0.1693677306175232,\n",
       "  0.1935010552406311,\n",
       "  0.16695065796375275,\n",
       "  0.19831380248069763,\n",
       "  0.16897113621234894,\n",
       "  0.15233181416988373,\n",
       "  0.1770579218864441,\n",
       "  0.16903190314769745,\n",
       "  0.169295072555542,\n",
       "  0.16423454880714417,\n",
       "  0.17425081133842468,\n",
       "  0.17463071644306183,\n",
       "  0.15754005312919617,\n",
       "  0.18845604360103607,\n",
       "  0.16731776297092438,\n",
       "  0.1624632328748703,\n",
       "  0.17495881021022797,\n",
       "  0.18046124279499054,\n",
       "  0.16330185532569885,\n",
       "  0.16750016808509827,\n",
       "  0.14682739973068237,\n",
       "  0.17734524607658386,\n",
       "  0.16552019119262695,\n",
       "  0.17578980326652527,\n",
       "  0.1824302077293396,\n",
       "  0.17660242319107056,\n",
       "  0.16123434901237488,\n",
       "  0.1628943532705307,\n",
       "  0.15071755647659302,\n",
       "  0.18232105672359467,\n",
       "  0.1570092737674713,\n",
       "  0.19710877537727356,\n",
       "  0.16283272206783295,\n",
       "  0.15682922303676605,\n",
       "  0.16246753931045532,\n",
       "  0.15771038830280304,\n",
       "  0.17873156070709229,\n",
       "  0.1610785573720932,\n",
       "  0.16099031269550323,\n",
       "  0.16227199137210846,\n",
       "  0.16369588673114777,\n",
       "  0.15935774147510529,\n",
       "  0.17514650523662567,\n",
       "  0.1552339792251587,\n",
       "  0.15892308950424194,\n",
       "  0.16364659368991852,\n",
       "  0.17398200929164886,\n",
       "  0.16463413834571838,\n",
       "  0.16533270478248596,\n",
       "  0.19814489781856537,\n",
       "  0.16281986236572266,\n",
       "  0.1583058089017868,\n",
       "  0.1650739163160324,\n",
       "  0.13817350566387177,\n",
       "  0.1577467918395996,\n",
       "  0.15518489480018616,\n",
       "  0.16539517045021057,\n",
       "  0.19299255311489105,\n",
       "  0.13865797221660614,\n",
       "  0.15961328148841858,\n",
       "  0.15671229362487793,\n",
       "  0.1459195464849472,\n",
       "  0.15388193726539612,\n",
       "  0.15831945836544037,\n",
       "  0.1403331160545349,\n",
       "  0.16769398748874664,\n",
       "  0.1495404988527298,\n",
       "  0.1511145681142807,\n",
       "  0.15352801978588104,\n",
       "  0.16701485216617584,\n",
       "  0.1387329399585724,\n",
       "  0.16356076300144196,\n",
       "  0.19672857224941254,\n",
       "  0.15042313933372498,\n",
       "  0.20815955102443695,\n",
       "  0.14498916268348694,\n",
       "  0.18072807788848877,\n",
       "  0.17188571393489838,\n",
       "  0.15329024195671082,\n",
       "  0.14351119101047516,\n",
       "  0.15997155010700226,\n",
       "  0.18519046902656555,\n",
       "  0.14900948107242584,\n",
       "  0.16380761563777924,\n",
       "  0.14816266298294067,\n",
       "  0.17495931684970856,\n",
       "  0.15572680532932281,\n",
       "  0.1747526377439499,\n",
       "  0.14114028215408325,\n",
       "  0.15644018352031708,\n",
       "  0.17106404900550842,\n",
       "  0.1631368100643158,\n",
       "  0.15661203861236572,\n",
       "  0.1422528177499771,\n",
       "  0.18226513266563416,\n",
       "  0.16902324557304382,\n",
       "  0.13940417766571045,\n",
       "  0.15884697437286377,\n",
       "  0.15101593732833862,\n",
       "  0.1498776525259018,\n",
       "  0.15124592185020447,\n",
       "  0.1392170935869217,\n",
       "  0.17109151184558868,\n",
       "  0.15577542781829834,\n",
       "  0.14263884723186493,\n",
       "  0.1561582386493683,\n",
       "  0.15023034811019897,\n",
       "  0.14695560932159424,\n",
       "  0.13848240673542023,\n",
       "  0.16316361725330353,\n",
       "  0.14967386424541473,\n",
       "  0.1574704498052597,\n",
       "  0.14125314354896545,\n",
       "  0.163054421544075,\n",
       "  0.1441131979227066,\n",
       "  0.16376833617687225,\n",
       "  0.14448150992393494,\n",
       "  0.1581132709980011,\n",
       "  0.14137551188468933,\n",
       "  0.19056671857833862,\n",
       "  0.1465655416250229,\n",
       "  0.15158219635486603,\n",
       "  0.159376859664917,\n",
       "  0.1540440022945404,\n",
       "  0.1369692087173462,\n",
       "  0.14846642315387726,\n",
       "  0.14798450469970703,\n",
       "  0.14059871435165405,\n",
       "  0.1523483395576477,\n",
       "  0.16661621630191803,\n",
       "  0.14981456100940704,\n",
       "  0.1479986160993576,\n",
       "  0.13676254451274872,\n",
       "  0.156963512301445,\n",
       "  0.13490767776966095,\n",
       "  0.16622816026210785,\n",
       "  0.1374213695526123,\n",
       "  0.13789808750152588,\n",
       "  0.16036824882030487,\n",
       "  0.13636940717697144,\n",
       "  0.17350035905838013,\n",
       "  0.12965767085552216,\n",
       "  0.14883093535900116,\n",
       "  0.16741427779197693,\n",
       "  0.1435120403766632,\n",
       "  0.13700667023658752,\n",
       "  0.16151732206344604,\n",
       "  0.14403656125068665,\n",
       "  0.17637211084365845,\n",
       "  0.13425172865390778,\n",
       "  0.1582861989736557,\n",
       "  0.13837751746177673,\n",
       "  0.1431235820055008,\n",
       "  0.14508838951587677,\n",
       "  0.1385626643896103,\n",
       "  0.15230846405029297,\n",
       "  0.1330200433731079,\n",
       "  0.14331744611263275,\n",
       "  0.15374580025672913,\n",
       "  0.13715924322605133,\n",
       "  0.14537745714187622,\n",
       "  0.13796870410442352,\n",
       "  0.14250998198986053,\n",
       "  0.19232068955898285,\n",
       "  0.14128759503364563,\n",
       "  0.13923171162605286,\n",
       "  0.14729051291942596,\n",
       "  0.16291308403015137,\n",
       "  0.14349445700645447,\n",
       "  0.15330977737903595,\n",
       "  0.1281314492225647,\n",
       "  0.15542742609977722,\n",
       "  0.15986543893814087,\n",
       "  0.13631874322891235,\n",
       "  0.18688246607780457,\n",
       "  0.15829063951969147,\n",
       "  0.13049693405628204,\n",
       "  0.12802754342556,\n",
       "  0.12367948889732361,\n",
       "  0.14777345955371857,\n",
       "  0.14194048941135406,\n",
       "  0.13494588434696198,\n",
       "  0.13341239094734192,\n",
       "  0.1570262312889099,\n",
       "  0.14175711572170258,\n",
       "  0.1864786148071289,\n",
       "  0.1582239419221878,\n",
       "  0.170297309756279,\n",
       "  0.18075767159461975,\n",
       "  0.1282000094652176,\n",
       "  0.13998772203922272,\n",
       "  0.13744542002677917,\n",
       "  0.1413663923740387,\n",
       "  0.1578904688358307,\n",
       "  0.12757568061351776,\n",
       "  0.1437806636095047,\n",
       "  0.1640792191028595,\n",
       "  0.14928612112998962,\n",
       "  0.14975079894065857,\n",
       "  0.14436757564544678,\n",
       "  0.13943400979042053,\n",
       "  0.13400396704673767,\n",
       "  0.16643604636192322,\n",
       "  0.16609284281730652,\n",
       "  0.12666113674640656,\n",
       "  0.14469338953495026,\n",
       "  0.13804416358470917,\n",
       "  0.14831842482089996,\n",
       "  0.15920139849185944,\n",
       "  0.13521388173103333,\n",
       "  0.20444512367248535,\n",
       "  0.1325700432062149,\n",
       "  0.15335054695606232,\n",
       "  0.14785803854465485,\n",
       "  0.1491980254650116,\n",
       "  0.13870790600776672,\n",
       "  0.14405861496925354,\n",
       "  0.14954893290996552,\n",
       "  0.13004189729690552,\n",
       "  0.15861943364143372,\n",
       "  0.13882605731487274,\n",
       "  0.16725610196590424,\n",
       "  0.13790763914585114,\n",
       "  0.13914212584495544,\n",
       "  0.13578447699546814,\n",
       "  0.15010668337345123,\n",
       "  0.14489395916461945,\n",
       "  0.1431502252817154,\n",
       "  0.12159384787082672,\n",
       "  0.12878377735614777,\n",
       "  0.15802013874053955,\n",
       "  0.1662147343158722,\n",
       "  0.1270992010831833,\n",
       "  0.13191086053848267,\n",
       "  0.14340850710868835,\n",
       "  0.13392052054405212,\n",
       "  0.1445307582616806,\n",
       "  0.14006845653057098,\n",
       "  0.14682745933532715,\n",
       "  0.12551067769527435,\n",
       "  0.15659968554973602,\n",
       "  0.14045795798301697,\n",
       "  0.13652101159095764,\n",
       "  0.1510632038116455,\n",
       "  0.13835665583610535,\n",
       "  0.2223127782344818,\n",
       "  0.13869298994541168,\n",
       "  0.20664680004119873,\n",
       "  0.12276466935873032,\n",
       "  0.14059709012508392,\n",
       "  0.14759773015975952,\n",
       "  0.1303715854883194,\n",
       "  0.183033749461174,\n",
       "  0.12411480396986008,\n",
       "  0.13780874013900757,\n",
       "  0.17231151461601257,\n",
       "  0.12941984832286835,\n",
       "  0.14115381240844727,\n",
       "  0.13227331638336182,\n",
       "  0.18779584765434265,\n",
       "  0.12876461446285248,\n",
       "  0.14988009631633759,\n",
       "  0.13309359550476074,\n",
       "  0.13143576681613922,\n",
       "  0.13122612237930298,\n",
       "  0.12655511498451233,\n",
       "  0.13199082016944885,\n",
       "  0.12715519964694977,\n",
       "  0.13557066023349762,\n",
       "  0.12789621949195862,\n",
       "  0.171053946018219,\n",
       "  0.12190333008766174,\n",
       "  0.14789527654647827,\n",
       "  0.11626778542995453,\n",
       "  0.1735784411430359,\n",
       "  0.13505204021930695,\n",
       "  0.1284097284078598,\n",
       "  0.1521276831626892,\n",
       "  0.12635451555252075,\n",
       "  0.1987067461013794,\n",
       "  0.11756357550621033,\n",
       "  0.15435798466205597,\n",
       "  0.11941078305244446,\n",
       "  0.24451009929180145,\n",
       "  0.14389197528362274,\n",
       "  0.15297414362430573,\n",
       "  0.15109440684318542,\n",
       "  0.15205268561840057,\n",
       "  0.16009967029094696,\n",
       "  0.18355616927146912,\n",
       "  0.15055547654628754,\n",
       "  0.10959667712450027,\n",
       "  0.18098372220993042,\n",
       "  0.11825446784496307,\n",
       "  0.15791109204292297,\n",
       "  0.11961883306503296,\n",
       "  0.12964743375778198,\n",
       "  0.1654302477836609,\n",
       "  0.16644544899463654,\n",
       "  0.13161686062812805,\n",
       "  0.26162391901016235,\n",
       "  0.14127112925052643,\n",
       "  0.1138133630156517,\n",
       "  0.20675334334373474,\n",
       "  0.11780495196580887,\n",
       "  0.14430683851242065,\n",
       "  0.12891586124897003,\n",
       "  0.18868553638458252,\n",
       "  0.1626378446817398,\n",
       "  0.11990512162446976,\n",
       "  0.14962339401245117,\n",
       "  0.12332095950841904,\n",
       "  0.12848377227783203,\n",
       "  0.16013340651988983,\n",
       "  0.13900889456272125,\n",
       "  0.11750387400388718,\n",
       "  0.13717710971832275,\n",
       "  0.13191519677639008,\n",
       "  0.13624681532382965,\n",
       "  0.11442606151103973,\n",
       "  0.12378852814435959,\n",
       "  0.1330237239599228,\n",
       "  0.2050468623638153,\n",
       "  0.157647967338562,\n",
       "  0.10708124935626984,\n",
       "  0.12897847592830658,\n",
       "  0.10443373024463654,\n",
       "  0.1322833001613617,\n",
       "  0.12864342331886292,\n",
       "  0.12800657749176025,\n",
       "  0.1554947942495346,\n",
       "  0.11418186873197556,\n",
       "  0.19364233314990997,\n",
       "  0.1553156077861786,\n",
       "  0.12065161764621735,\n",
       "  0.1211170181632042,\n",
       "  0.10635606944561005,\n",
       "  0.12452290207147598,\n",
       "  0.15047363936901093,\n",
       "  0.11322073638439178,\n",
       "  0.1414482295513153,\n",
       "  0.1994255781173706,\n",
       "  0.13759051263332367,\n",
       "  0.16077853739261627,\n",
       "  0.15518444776535034,\n",
       "  0.15733489394187927,\n",
       "  0.11472666263580322,\n",
       "  0.13135036826133728,\n",
       "  0.16210734844207764,\n",
       "  0.1199280172586441,\n",
       "  0.12396213412284851,\n",
       "  0.11501099914312363,\n",
       "  0.1524178832769394,\n",
       "  0.12272332608699799,\n",
       "  0.16749265789985657,\n",
       "  0.09352198243141174,\n",
       "  0.12564106285572052,\n",
       "  0.1250213235616684,\n",
       "  0.11210867762565613,\n",
       "  0.11646784842014313,\n",
       "  0.2617114782333374,\n",
       "  0.1197085902094841,\n",
       "  0.12366972863674164,\n",
       "  0.16631610691547394,\n",
       "  0.13330547511577606,\n",
       "  0.16316862404346466,\n",
       "  0.10518918931484222,\n",
       "  0.11528807878494263,\n",
       "  0.12384980916976929,\n",
       "  0.11779949069023132,\n",
       "  0.10905880481004715,\n",
       "  0.1212233304977417,\n",
       "  0.19525669515132904,\n",
       "  0.11450687795877457,\n",
       "  0.2699882984161377,\n",
       "  0.19353146851062775,\n",
       "  0.090838722884655,\n",
       "  0.14353962242603302,\n",
       "  0.12200363725423813,\n",
       "  0.11168219894170761,\n",
       "  0.11635301262140274,\n",
       "  0.11520737409591675,\n",
       "  0.13488326966762543,\n",
       "  0.29027897119522095,\n",
       "  0.15140607953071594,\n",
       "  0.15138685703277588,\n",
       "  0.1140209212899208,\n",
       "  0.16821004450321198,\n",
       "  0.11384068429470062,\n",
       "  0.13528408110141754,\n",
       "  0.12003535032272339,\n",
       "  0.18212157487869263,\n",
       "  0.13753938674926758,\n",
       "  0.1942751258611679,\n",
       "  0.11637428402900696,\n",
       "  0.12091048061847687,\n",
       "  0.11749245971441269,\n",
       "  0.1828429102897644,\n",
       "  0.11717887222766876,\n",
       "  0.1170002892613411,\n",
       "  0.16146810352802277,\n",
       "  0.09800196439027786,\n",
       "  0.18434886634349823,\n",
       "  0.17193745076656342,\n",
       "  0.1052473857998848,\n",
       "  0.13937415182590485,\n",
       "  0.28397291898727417,\n",
       "  0.10149470716714859,\n",
       "  0.15991464257240295,\n",
       "  0.10532917082309723,\n",
       "  0.10950025171041489,\n",
       "  0.1477239727973938,\n",
       "  0.13090787827968597,\n",
       "  0.13490650057792664,\n",
       "  0.13614290952682495,\n",
       "  0.1482621729373932,\n",
       "  0.21964053809642792,\n",
       "  0.1612810492515564,\n",
       "  0.23897583782672882,\n",
       "  0.15218107402324677,\n",
       "  0.16456031799316406,\n",
       "  0.10125386714935303,\n",
       "  0.22344090044498444,\n",
       "  0.19874048233032227,\n",
       "  0.20022620260715485,\n",
       "  0.10725372284650803,\n",
       "  0.10612662881612778,\n",
       "  0.10402734577655792,\n",
       "  0.15375936031341553,\n",
       "  0.119261234998703,\n",
       "  0.11949994415044785,\n",
       "  0.2274111658334732,\n",
       "  0.11200190335512161,\n",
       "  0.1392698585987091,\n",
       "  0.1546606719493866,\n",
       "  0.17207121849060059,\n",
       "  0.1471109390258789,\n",
       "  0.19136781990528107,\n",
       "  0.10833301395177841,\n",
       "  0.21854738891124725,\n",
       "  0.18508701026439667,\n",
       "  0.13814035058021545,\n",
       "  0.17059849202632904,\n",
       "  0.1302867978811264,\n",
       "  0.11196411401033401,\n",
       "  0.12897683680057526,\n",
       "  0.11937379091978073,\n",
       "  0.14012134075164795,\n",
       "  0.10624043643474579,\n",
       "  0.32441338896751404,\n",
       "  0.25763770937919617,\n",
       "  0.14898964762687683,\n",
       "  0.21765609085559845,\n",
       "  0.16383032500743866,\n",
       "  0.17266322672367096,\n",
       "  0.13625571131706238,\n",
       "  0.10595084726810455,\n",
       "  0.106083944439888,\n",
       "  0.3084827959537506,\n",
       "  0.11559704691171646,\n",
       "  0.10198359936475754,\n",
       "  0.308756947517395,\n",
       "  0.13751374185085297,\n",
       "  0.11440397053956985,\n",
       "  0.13300475478172302,\n",
       "  0.16657571494579315,\n",
       "  0.12953943014144897,\n",
       "  0.151901513338089,\n",
       "  0.22985509037971497,\n",
       "  0.1269609034061432,\n",
       "  0.14738847315311432,\n",
       "  0.11611931025981903,\n",
       "  0.3539941906929016,\n",
       "  0.13979962468147278,\n",
       "  0.11939965188503265,\n",
       "  0.16817161440849304,\n",
       "  0.112503781914711,\n",
       "  0.14863963425159454,\n",
       "  0.15949943661689758,\n",
       "  0.10132072865962982,\n",
       "  0.3218516707420349,\n",
       "  0.18199490010738373,\n",
       "  0.1263323575258255,\n",
       "  0.15460611879825592,\n",
       "  0.20954157412052155,\n",
       "  0.1029345840215683,\n",
       "  0.13242486119270325,\n",
       "  0.10899942368268967,\n",
       "  0.20421671867370605,\n",
       "  0.1573912799358368,\n",
       "  0.13178938627243042,\n",
       "  0.13140729069709778,\n",
       "  0.18445023894309998],\n",
       " 'accuracy': [0.36521008610725403,\n",
       "  0.5515966415405273,\n",
       "  0.7151260375976562,\n",
       "  0.7640336155891418,\n",
       "  0.8078991770744324,\n",
       "  0.827899158000946,\n",
       "  0.8225210309028625,\n",
       "  0.8384873867034912,\n",
       "  0.8331092596054077,\n",
       "  0.8356302380561829,\n",
       "  0.8415126204490662,\n",
       "  0.841680645942688,\n",
       "  0.8403361439704895,\n",
       "  0.8473949432373047,\n",
       "  0.8509243726730347,\n",
       "  0.8500840067863464,\n",
       "  0.8500840067863464,\n",
       "  0.8621848821640015,\n",
       "  0.8458823561668396,\n",
       "  0.8623529672622681,\n",
       "  0.8591596484184265,\n",
       "  0.8559663891792297,\n",
       "  0.868571400642395,\n",
       "  0.8603361248970032,\n",
       "  0.8581512570381165,\n",
       "  0.86806720495224,\n",
       "  0.8633613586425781,\n",
       "  0.8586554527282715,\n",
       "  0.8746218681335449,\n",
       "  0.8702520728111267,\n",
       "  0.8689075708389282,\n",
       "  0.8690756559371948,\n",
       "  0.8584873676300049,\n",
       "  0.8687394857406616,\n",
       "  0.8709243535995483,\n",
       "  0.8684033751487732,\n",
       "  0.8719327449798584,\n",
       "  0.8702520728111267,\n",
       "  0.8732773065567017,\n",
       "  0.8647058606147766,\n",
       "  0.8647058606147766,\n",
       "  0.8794957995414734,\n",
       "  0.8774790167808533,\n",
       "  0.8699159622192383,\n",
       "  0.8705882430076599,\n",
       "  0.8831932544708252,\n",
       "  0.8793277144432068,\n",
       "  0.880168080329895,\n",
       "  0.8673949837684631,\n",
       "  0.8734453916549683,\n",
       "  0.8823529481887817,\n",
       "  0.8784874081611633,\n",
       "  0.8811764717102051,\n",
       "  0.8921008110046387,\n",
       "  0.8860504031181335,\n",
       "  0.8887394666671753,\n",
       "  0.8872268795967102,\n",
       "  0.8852100968360901,\n",
       "  0.8931092619895935,\n",
       "  0.8821848630905151,\n",
       "  0.8873949646949768,\n",
       "  0.8848739266395569,\n",
       "  0.8820167779922485,\n",
       "  0.8867226839065552,\n",
       "  0.8878991603851318,\n",
       "  0.8882352709770203,\n",
       "  0.87966388463974,\n",
       "  0.8922688961029053,\n",
       "  0.8910924196243286,\n",
       "  0.8894117474555969,\n",
       "  0.8929411768913269,\n",
       "  0.8927730917930603,\n",
       "  0.8890756368637085,\n",
       "  0.8969748020172119,\n",
       "  0.8968067169189453,\n",
       "  0.8875630497932434,\n",
       "  0.8872268795967102,\n",
       "  0.8936134576797485,\n",
       "  0.8884033560752869,\n",
       "  0.8941176533699036,\n",
       "  0.8946218490600586,\n",
       "  0.8942857384681702,\n",
       "  0.890420138835907,\n",
       "  0.8889075517654419,\n",
       "  0.8862184882164001,\n",
       "  0.8973109126091003,\n",
       "  0.8926050662994385,\n",
       "  0.8915966153144836,\n",
       "  0.8978151082992554,\n",
       "  0.9057142734527588,\n",
       "  0.894453763961792,\n",
       "  0.8978151082992554,\n",
       "  0.9033613204956055,\n",
       "  0.8875630497932434,\n",
       "  0.8919327855110168,\n",
       "  0.8978151082992554,\n",
       "  0.8996638655662537,\n",
       "  0.8998319506645203,\n",
       "  0.8934453725814819,\n",
       "  0.9013445377349854,\n",
       "  0.8994957804679871,\n",
       "  0.8973109126091003,\n",
       "  0.894453763961792,\n",
       "  0.9095798134803772,\n",
       "  0.9026890993118286,\n",
       "  0.8957983255386353,\n",
       "  0.898991584777832,\n",
       "  0.902016818523407,\n",
       "  0.8952941298484802,\n",
       "  0.8996638655662537,\n",
       "  0.8956302404403687,\n",
       "  0.8929411768913269,\n",
       "  0.9035294055938721,\n",
       "  0.9058823585510254,\n",
       "  0.9026890993118286,\n",
       "  0.9043697714805603,\n",
       "  0.9080672264099121,\n",
       "  0.9040336012840271,\n",
       "  0.9005042314529419,\n",
       "  0.8981512784957886,\n",
       "  0.8964706063270569,\n",
       "  0.8986554741859436,\n",
       "  0.9048739671707153,\n",
       "  0.8978151082992554,\n",
       "  0.9052100777626038,\n",
       "  0.9013445377349854,\n",
       "  0.9110924601554871,\n",
       "  0.9038655757904053,\n",
       "  0.9072269201278687,\n",
       "  0.9010084271430969,\n",
       "  0.9035294055938721,\n",
       "  0.9048739671707153,\n",
       "  0.902016818523407,\n",
       "  0.9058823585510254,\n",
       "  0.9063865542411804,\n",
       "  0.906554639339447,\n",
       "  0.9045377969741821,\n",
       "  0.9099159836769104,\n",
       "  0.9031932950019836,\n",
       "  0.9043697714805603,\n",
       "  0.907058835029602,\n",
       "  0.9050419926643372,\n",
       "  0.9068907499313354,\n",
       "  0.9085714221000671,\n",
       "  0.905546247959137,\n",
       "  0.9080672264099121,\n",
       "  0.9072269201278687,\n",
       "  0.9011764526367188,\n",
       "  0.9042016863822937,\n",
       "  0.9095798134803772,\n",
       "  0.9102520942687988,\n",
       "  0.9050419926643372,\n",
       "  0.9082353115081787,\n",
       "  0.9033613204956055,\n",
       "  0.9031932950019836,\n",
       "  0.9095798134803772,\n",
       "  0.905546247959137,\n",
       "  0.910588264465332,\n",
       "  0.9078991413116455,\n",
       "  0.9132773280143738,\n",
       "  0.910588264465332,\n",
       "  0.9073949456214905,\n",
       "  0.9097478985786438,\n",
       "  0.9109243750572205,\n",
       "  0.9121008515357971,\n",
       "  0.9057142734527588,\n",
       "  0.9100840091705322,\n",
       "  0.902016818523407,\n",
       "  0.9134453535079956,\n",
       "  0.9129411578178406,\n",
       "  0.9129411578178406,\n",
       "  0.9063865542411804,\n",
       "  0.9043697714805603,\n",
       "  0.907058835029602,\n",
       "  0.9139496088027954,\n",
       "  0.906554639339447,\n",
       "  0.9095798134803772,\n",
       "  0.9121008515357971,\n",
       "  0.9136134386062622,\n",
       "  0.9095798134803772,\n",
       "  0.906554639339447,\n",
       "  0.9099159836769104,\n",
       "  0.9122689366340637,\n",
       "  0.9104201793670654,\n",
       "  0.9084033370018005,\n",
       "  0.9092437028884888,\n",
       "  0.9115966558456421,\n",
       "  0.9127731323242188,\n",
       "  0.9164705872535706,\n",
       "  0.916302502155304,\n",
       "  0.9141176342964172,\n",
       "  0.9126050472259521,\n",
       "  0.9179831743240356,\n",
       "  0.9134453535079956,\n",
       "  0.9151260256767273,\n",
       "  0.9178151488304138,\n",
       "  0.921848714351654,\n",
       "  0.9198319315910339,\n",
       "  0.9084033370018005,\n",
       "  0.9147899150848389,\n",
       "  0.9191596508026123,\n",
       "  0.9087395071983337,\n",
       "  0.9164705872535706,\n",
       "  0.920336127281189,\n",
       "  0.9186554551124573,\n",
       "  0.9166386723518372,\n",
       "  0.9186554551124573,\n",
       "  0.9149580001831055,\n",
       "  0.9152941107749939,\n",
       "  0.9164705872535706,\n",
       "  0.9181512594223022,\n",
       "  0.916806697845459,\n",
       "  0.9132773280143738,\n",
       "  0.9196638464927673,\n",
       "  0.9201680421829224,\n",
       "  0.9233613610267639,\n",
       "  0.9164705872535706,\n",
       "  0.9154621958732605,\n",
       "  0.9173109531402588,\n",
       "  0.9141176342964172,\n",
       "  0.920336127281189,\n",
       "  0.921344518661499,\n",
       "  0.9252100586891174,\n",
       "  0.9223529696464539,\n",
       "  0.9188235402107239,\n",
       "  0.9147899150848389,\n",
       "  0.9236974716186523,\n",
       "  0.9149580001831055,\n",
       "  0.9257143139839172,\n",
       "  0.9174789786338806,\n",
       "  0.9178151488304138,\n",
       "  0.9270588159561157,\n",
       "  0.9181512594223022,\n",
       "  0.9250420331954956,\n",
       "  0.9201680421829224,\n",
       "  0.9173109531402588,\n",
       "  0.925378143787384,\n",
       "  0.9184873700141907,\n",
       "  0.9179831743240356,\n",
       "  0.920336127281189,\n",
       "  0.916806697845459,\n",
       "  0.9235293865203857,\n",
       "  0.9236974716186523,\n",
       "  0.916302502155304,\n",
       "  0.9252100586891174,\n",
       "  0.9257143139839172,\n",
       "  0.9240336418151855,\n",
       "  0.928403377532959,\n",
       "  0.9142857193946838,\n",
       "  0.9231932759284973,\n",
       "  0.921848714351654,\n",
       "  0.9221848845481873,\n",
       "  0.921848714351654,\n",
       "  0.9201680421829224,\n",
       "  0.9235293865203857,\n",
       "  0.9285714030265808,\n",
       "  0.9277310967445374,\n",
       "  0.9250420331954956,\n",
       "  0.9188235402107239,\n",
       "  0.9240336418151855,\n",
       "  0.916806697845459,\n",
       "  0.9310924410820007,\n",
       "  0.9304201602935791,\n",
       "  0.9267227053642273,\n",
       "  0.9321008324623108,\n",
       "  0.9270588159561157,\n",
       "  0.9295798540115356,\n",
       "  0.9250420331954956,\n",
       "  0.9230251908302307,\n",
       "  0.928907573223114,\n",
       "  0.9277310967445374,\n",
       "  0.927899181842804,\n",
       "  0.9290756583213806,\n",
       "  0.9262185096740723,\n",
       "  0.9297478795051575,\n",
       "  0.9265546202659607,\n",
       "  0.9285714030265808,\n",
       "  0.9373109340667725,\n",
       "  0.9262185096740723,\n",
       "  0.9270588159561157,\n",
       "  0.924873948097229,\n",
       "  0.9312605261802673,\n",
       "  0.9310924410820007,\n",
       "  0.9282352924346924,\n",
       "  0.9309243559837341,\n",
       "  0.9336134195327759,\n",
       "  0.9302520751953125,\n",
       "  0.9331092238426208,\n",
       "  0.9260504245758057,\n",
       "  0.9304201602935791,\n",
       "  0.9295798540115356,\n",
       "  0.9304201602935791,\n",
       "  0.9337815046310425,\n",
       "  0.9300840497016907,\n",
       "  0.9282352924346924,\n",
       "  0.9300840497016907,\n",
       "  0.9285714030265808,\n",
       "  0.9273949861526489,\n",
       "  0.9309243559837341,\n",
       "  0.9305882453918457,\n",
       "  0.9352940917015076,\n",
       "  0.9332773089408875,\n",
       "  0.9302520751953125,\n",
       "  0.9314285516738892,\n",
       "  0.932437002658844,\n",
       "  0.9273949861526489,\n",
       "  0.9252100586891174,\n",
       "  0.9319327473640442,\n",
       "  0.9310924410820007,\n",
       "  0.9280672073364258,\n",
       "  0.9416806697845459,\n",
       "  0.9341176748275757,\n",
       "  0.9332773089408875,\n",
       "  0.933445394039154,\n",
       "  0.932941198348999,\n",
       "  0.9389916062355042,\n",
       "  0.932437002658844,\n",
       "  0.9368067383766174,\n",
       "  0.9379832148551941,\n",
       "  0.9366386532783508,\n",
       "  0.9331092238426208,\n",
       "  0.9383193254470825,\n",
       "  0.9337815046310425,\n",
       "  0.9363025426864624,\n",
       "  0.9347898960113525,\n",
       "  0.9384874105453491,\n",
       "  0.928907573223114,\n",
       "  0.9415125846862793,\n",
       "  0.9352940917015076,\n",
       "  0.9349579811096191,\n",
       "  0.9379832148551941,\n",
       "  0.933445394039154,\n",
       "  0.9389916062355042,\n",
       "  0.9351260662078857,\n",
       "  0.9302520751953125,\n",
       "  0.9379832148551941,\n",
       "  0.9405041933059692,\n",
       "  0.9332773089408875,\n",
       "  0.9336134195327759,\n",
       "  0.9421848654747009,\n",
       "  0.9321008324623108,\n",
       "  0.9373109340667725,\n",
       "  0.9342857003211975,\n",
       "  0.9384874105453491,\n",
       "  0.9361344575881958,\n",
       "  0.942689061164856,\n",
       "  0.9361344575881958,\n",
       "  0.9391596913337708,\n",
       "  0.9378151297569275,\n",
       "  0.9361344575881958,\n",
       "  0.9401680827140808,\n",
       "  0.9351260662078857,\n",
       "  0.9352940917015076,\n",
       "  0.9405041933059692,\n",
       "  0.9332773089408875,\n",
       "  0.9408403635025024,\n",
       "  0.9369747638702393,\n",
       "  0.9378151297569275,\n",
       "  0.9401680827140808,\n",
       "  0.9352940917015076,\n",
       "  0.9383193254470825,\n",
       "  0.9405041933059692,\n",
       "  0.9384874105453491,\n",
       "  0.9391596913337708,\n",
       "  0.9374790191650391,\n",
       "  0.9410083889961243,\n",
       "  0.9366386532783508,\n",
       "  0.9418487548828125,\n",
       "  0.9415125846862793,\n",
       "  0.9369747638702393,\n",
       "  0.9352940917015076,\n",
       "  0.9361344575881958,\n",
       "  0.9371428489685059,\n",
       "  0.938655436038971,\n",
       "  0.9379832148551941,\n",
       "  0.9391596913337708,\n",
       "  0.9415125846862793,\n",
       "  0.9423529505729675,\n",
       "  0.9410083889961243,\n",
       "  0.9435294270515442,\n",
       "  0.9396638870239258,\n",
       "  0.9403361082077026,\n",
       "  0.9406722784042358,\n",
       "  0.9369747638702393,\n",
       "  0.9421848654747009,\n",
       "  0.9398319125175476,\n",
       "  0.9438655376434326,\n",
       "  0.9447059035301208,\n",
       "  0.9425210356712341,\n",
       "  0.9447059035301208,\n",
       "  0.9398319125175476,\n",
       "  0.9401680827140808,\n",
       "  0.9369747638702393,\n",
       "  0.9463865756988525,\n",
       "  0.9452100992202759,\n",
       "  0.9361344575881958,\n",
       "  0.9455462098121643,\n",
       "  0.938655436038971,\n",
       "  0.9455462098121643,\n",
       "  0.9421848654747009,\n",
       "  0.9406722784042358,\n",
       "  0.9430252313613892,\n",
       "  0.9418487548828125,\n",
       "  0.9394958019256592,\n",
       "  0.943193256855011,\n",
       "  0.9415125846862793,\n",
       "  0.9425210356712341,\n",
       "  0.9411764740943909,\n",
       "  0.9457142949104309,\n",
       "  0.9411764740943909,\n",
       "  0.942689061164856,\n",
       "  0.9423529505729675,\n",
       "  0.9399999976158142,\n",
       "  0.9460504055023193,\n",
       "  0.9413445591926575,\n",
       "  0.9393277168273926,\n",
       "  0.946722686290741,\n",
       "  0.9452100992202759,\n",
       "  0.9410083889961243,\n",
       "  0.9492437243461609,\n",
       "  0.9363025426864624,\n",
       "  0.9442017078399658,\n",
       "  0.943697452545166,\n",
       "  0.9428571462631226,\n",
       "  0.9388235211372375,\n",
       "  0.9455462098121643,\n",
       "  0.9435294270515442,\n",
       "  0.9455462098121643,\n",
       "  0.9428571462631226,\n",
       "  0.9369747638702393,\n",
       "  0.9458823800086975,\n",
       "  0.9416806697845459,\n",
       "  0.9416806697845459,\n",
       "  0.9457142949104309,\n",
       "  0.948235273361206,\n",
       "  0.9499159455299377,\n",
       "  0.9416806697845459,\n",
       "  0.9452100992202759,\n",
       "  0.9463865756988525,\n",
       "  0.942689061164856,\n",
       "  0.9435294270515442,\n",
       "  0.9378151297569275,\n",
       "  0.9389916062355042,\n",
       "  0.9448739290237427,\n",
       "  0.9394958019256592,\n",
       "  0.9393277168273926,\n",
       "  0.947226881980896,\n",
       "  0.9384874105453491,\n",
       "  0.9455462098121643,\n",
       "  0.9505882263183594,\n",
       "  0.9448739290237427,\n",
       "  0.947226881980896,\n",
       "  0.9480672478675842,\n",
       "  0.9490756392478943,\n",
       "  0.9497479200363159,\n",
       "  0.9480672478675842,\n",
       "  0.9416806697845459,\n",
       "  0.9487394690513611,\n",
       "  0.9480672478675842,\n",
       "  0.947226881980896,\n",
       "  0.9463865756988525,\n",
       "  0.9455462098121643,\n",
       "  0.9413445591926575,\n",
       "  0.9438655376434326,\n",
       "  0.9463865756988525,\n",
       "  0.9487394690513611,\n",
       "  0.9475630521774292,\n",
       "  0.9435294270515442,\n",
       "  0.9519327878952026,\n",
       "  0.9458823800086975,\n",
       "  0.9406722784042358,\n",
       "  0.9406722784042358,\n",
       "  0.9539495706558228,\n",
       "  0.9455462098121643,\n",
       "  0.9487394690513611,\n",
       "  0.9494117498397827,\n",
       "  0.948235273361206,\n",
       "  0.947226881980896,\n",
       "  0.9487394690513611,\n",
       "  0.9445378184318542,\n",
       "  0.9484033584594727,\n",
       "  0.9531092643737793,\n",
       "  0.947226881980896,\n",
       "  0.946722686290741,\n",
       "  0.9524369835853577,\n",
       "  0.9510924220085144,\n",
       "  0.9433613419532776,\n",
       "  0.9509243965148926,\n",
       "  0.9490756392478943,\n",
       "  0.9473949670791626,\n",
       "  0.9499159455299377,\n",
       "  0.9463865756988525,\n",
       "  0.9489075541496277,\n",
       "  0.943697452545166,\n",
       "  0.9475630521774292,\n",
       "  0.9453781247138977,\n",
       "  0.9515966176986694,\n",
       "  0.9494117498397827,\n",
       "  0.9460504055023193,\n",
       "  0.9445378184318542,\n",
       "  0.9448739290237427,\n",
       "  0.9470587968826294,\n",
       "  0.946722686290741,\n",
       "  0.9495798349380493,\n",
       "  0.9452100992202759,\n",
       "  0.9499159455299377,\n",
       "  0.9529411792755127,\n",
       "  0.9526050686836243,\n",
       "  0.9490756392478943,\n",
       "  0.9473949670791626,\n",
       "  0.9462184906005859,\n",
       "  0.9497479200363159,\n",
       "  0.9531092643737793,\n",
       "  0.9499159455299377,\n",
       "  0.9489075541496277,\n",
       "  0.9475630521774292,\n",
       "  0.9521008133888245,\n",
       "  0.954789936542511,\n",
       "  0.9405041933059692,\n",
       "  0.9489075541496277,\n",
       "  0.9519327878952026,\n",
       "  0.9473949670791626,\n",
       "  0.9492437243461609,\n",
       "  0.9514285922050476,\n",
       "  0.9537814855575562,\n",
       "  0.9536134600639343,\n",
       "  0.950252115726471,\n",
       "  0.951764702796936,\n",
       "  0.9541176557540894,\n",
       "  0.9413445591926575,\n",
       "  0.951764702796936,\n",
       "  0.9495798349380493,\n",
       "  0.948235273361206,\n",
       "  0.9541176557540894,\n",
       "  0.9522688984870911,\n",
       "  0.9499159455299377,\n",
       "  0.9452100992202759,\n",
       "  0.9537814855575562,\n",
       "  0.9468907713890076,\n",
       "  0.9521008133888245,\n",
       "  0.9504201412200928,\n",
       "  0.9549579620361328,\n",
       "  0.9497479200363159,\n",
       "  0.955294132232666,\n",
       "  0.9564706087112427,\n",
       "  0.9448739290237427,\n",
       "  0.950756311416626,\n",
       "  0.9489075541496277,\n",
       "  0.954285740852356,\n",
       "  0.9514285922050476,\n",
       "  0.9509243965148926,\n",
       "  0.9573109149932861,\n",
       "  0.950756311416626,\n",
       "  0.9522688984870911,\n",
       "  0.9563025236129761,\n",
       "  0.9450420141220093,\n",
       "  0.9534453749656677,\n",
       "  0.9445378184318542,\n",
       "  0.9514285922050476,\n",
       "  0.955294132232666,\n",
       "  0.951260507106781,\n",
       "  0.9510924220085144,\n",
       "  0.9559664130210876,\n",
       "  0.9478991627693176,\n",
       "  0.9522688984870911,\n",
       "  0.9448739290237427,\n",
       "  0.9569748044013977,\n",
       "  0.9460504055023193,\n",
       "  0.9569748044013977,\n",
       "  0.9495798349380493,\n",
       "  0.9536134600639343,\n",
       "  0.9544537663459778,\n",
       "  0.9500840306282043,\n",
       "  0.947731077671051,\n",
       "  0.9522688984870911,\n",
       "  0.959831953048706,\n",
       "  0.9541176557540894,\n",
       "  0.951764702796936,\n",
       "  0.948235273361206,\n",
       "  0.9579831957817078,\n",
       "  0.9561344385147095,\n",
       "  0.9584873914718628,\n",
       "  0.9537814855575562,\n",
       "  0.9583193063735962,\n",
       "  0.9492437243461609,\n",
       "  0.9521008133888245,\n",
       "  0.9526050686836243,\n",
       "  0.9510924220085144,\n",
       "  0.955294132232666,\n",
       "  0.9515966176986694,\n",
       "  0.950756311416626,\n",
       "  0.9566386342048645,\n",
       "  0.9532772898674011,\n",
       "  0.9581512808799744,\n",
       "  0.9569748044013977,\n",
       "  0.948235273361206,\n",
       "  0.9527730941772461,\n",
       "  0.9603361487388611,\n",
       "  0.951764702796936,\n",
       "  0.9544537663459778,\n",
       "  0.9504201412200928,\n",
       "  0.9509243965148926,\n",
       "  0.9541176557540894,\n",
       "  0.9531092643737793,\n",
       "  0.954789936542511,\n",
       "  0.9536134600639343,\n",
       "  0.9576470851898193,\n",
       "  0.9527730941772461,\n",
       "  0.9584873914718628,\n",
       "  0.9563025236129761,\n",
       "  0.9551260471343994,\n",
       "  0.9537814855575562,\n",
       "  0.9642016887664795,\n",
       "  0.9554621577262878,\n",
       "  0.9564706087112427,\n",
       "  0.955294132232666,\n",
       "  0.9510924220085144,\n",
       "  0.947731077671051,\n",
       "  0.9566386342048645,\n",
       "  0.9574790000915527,\n",
       "  0.954789936542511,\n",
       "  0.9522688984870911,\n",
       "  0.955294132232666,\n",
       "  0.9586554765701294,\n",
       "  0.9574790000915527,\n",
       "  0.9536134600639343,\n",
       "  0.9564706087112427,\n",
       "  0.9633613228797913,\n",
       "  0.9539495706558228,\n",
       "  0.9546218514442444,\n",
       "  0.9589915871620178,\n",
       "  0.9579831957817078,\n",
       "  0.9487394690513611,\n",
       "  0.970084011554718,\n",
       "  0.9537814855575562,\n",
       "  0.9574790000915527,\n",
       "  0.9605041742324829,\n",
       "  0.9561344385147095,\n",
       "  0.955798327922821,\n",
       "  0.9529411792755127,\n",
       "  0.9484033584594727,\n",
       "  0.9529411792755127,\n",
       "  0.9526050686836243,\n",
       "  0.9559664130210876,\n",
       "  0.9603361487388611,\n",
       "  0.9561344385147095,\n",
       "  0.9578151106834412,\n",
       "  0.955294132232666,\n",
       "  0.9539495706558228,\n",
       "  0.9579831957817078,\n",
       "  0.9599999785423279,\n",
       "  0.9574790000915527,\n",
       "  0.9586554765701294,\n",
       "  0.9571428298950195,\n",
       "  0.955294132232666,\n",
       "  0.954285740852356,\n",
       "  0.9584873914718628,\n",
       "  0.9484033584594727,\n",
       "  0.9605041742324829,\n",
       "  0.9551260471343994,\n",
       "  0.9505882263183594,\n",
       "  0.9616806507110596,\n",
       "  0.9556302428245544,\n",
       "  0.959327757358551,\n",
       "  0.9623529314994812,\n",
       "  0.9596638679504395,\n",
       "  0.9573109149932861,\n",
       "  0.9620168209075928,\n",
       "  0.954789936542511,\n",
       "  0.9539495706558228,\n",
       "  0.9594957828521729,\n",
       "  0.9536134600639343,\n",
       "  0.9561344385147095,\n",
       "  0.9574790000915527,\n",
       "  0.9556302428245544,\n",
       "  0.9537814855575562,\n",
       "  0.9556302428245544,\n",
       "  0.9554621577262878,\n",
       "  0.9591596722602844,\n",
       "  0.9561344385147095,\n",
       "  0.9635294079780579,\n",
       "  0.9505882263183594,\n",
       "  0.9591596722602844,\n",
       "  0.959831953048706,\n",
       "  0.9616806507110596,\n",
       "  0.9568067193031311,\n",
       "  0.9583193063735962,\n",
       "  0.9554621577262878,\n",
       "  0.9569748044013977,\n",
       "  0.9640336036682129,\n",
       "  0.9588235020637512,\n",
       "  0.9579831957817078,\n",
       "  0.9611764550209045,\n",
       "  0.9579831957817078,\n",
       "  0.954789936542511,\n",
       "  0.9601680636405945,\n",
       "  0.9569748044013977,\n",
       "  0.9568067193031311,\n",
       "  0.9615126252174377,\n",
       "  0.9576470851898193,\n",
       "  0.9616806507110596,\n",
       "  0.9581512808799744,\n",
       "  0.9589915871620178,\n",
       "  0.9586554765701294,\n",
       "  0.9599999785423279,\n",
       "  0.9625210165977478,\n",
       "  0.959327757358551,\n",
       "  0.9549579620361328,\n",
       "  0.9589915871620178,\n",
       "  0.959831953048706,\n",
       "  0.9621848464012146,\n",
       "  0.9636974930763245,\n",
       "  0.9621848464012146,\n",
       "  0.9573109149932861,\n",
       "  0.9621848464012146,\n",
       "  0.9453781247138977,\n",
       "  0.9568067193031311,\n",
       "  0.9591596722602844,\n",
       "  0.951764702796936,\n",
       "  0.9626891016960144,\n",
       "  0.9635294079780579,\n",
       "  0.9618487358093262,\n",
       "  0.9584873914718628,\n",
       "  0.9596638679504395,\n",
       "  0.9589915871620178,\n",
       "  0.9566386342048645,\n",
       "  0.9601680636405945,\n",
       "  0.959831953048706,\n",
       "  0.9571428298950195,\n",
       "  0.9478991627693176,\n",
       "  0.959831953048706,\n",
       "  0.9648739695549011,\n",
       "  0.9599999785423279,\n",
       "  0.9588235020637512,\n",
       "  0.9603361487388611,\n",
       "  0.9653781652450562,\n",
       "  0.9589915871620178,\n",
       "  0.9581512808799744,\n",
       "  0.9569748044013977,\n",
       "  0.9635294079780579,\n",
       "  0.9640336036682129,\n",
       "  0.9564706087112427,\n",
       "  0.9626891016960144,\n",
       "  0.9621848464012146,\n",
       "  0.9581512808799744,\n",
       "  0.9568067193031311,\n",
       "  0.9549579620361328,\n",
       "  0.9566386342048645,\n",
       "  0.9564706087112427,\n",
       "  0.9608403444290161],\n",
       " 'val_loss': [1.2754356861114502,\n",
       "  1.0332967042922974,\n",
       "  0.6667512059211731,\n",
       "  0.4482727646827698,\n",
       "  0.4554917812347412,\n",
       "  0.3604678213596344,\n",
       "  0.3306910991668701,\n",
       "  0.3109181523323059,\n",
       "  0.3275800049304962,\n",
       "  0.2872031629085541,\n",
       "  0.270362913608551,\n",
       "  0.269560843706131,\n",
       "  0.363793283700943,\n",
       "  0.2613968551158905,\n",
       "  0.9161880612373352,\n",
       "  0.253618448972702,\n",
       "  0.2516035735607147,\n",
       "  0.23609864711761475,\n",
       "  0.24251559376716614,\n",
       "  0.26034635305404663,\n",
       "  0.23714391887187958,\n",
       "  0.7885320782661438,\n",
       "  0.23475779592990875,\n",
       "  0.36955031752586365,\n",
       "  0.23966628313064575,\n",
       "  0.28073838353157043,\n",
       "  0.22906586527824402,\n",
       "  0.23008200526237488,\n",
       "  0.21548597514629364,\n",
       "  0.2510117292404175,\n",
       "  0.2561485171318054,\n",
       "  0.22445867955684662,\n",
       "  0.22336198389530182,\n",
       "  0.21123473346233368,\n",
       "  0.21426518261432648,\n",
       "  0.2188822627067566,\n",
       "  0.23256279528141022,\n",
       "  0.21711495518684387,\n",
       "  0.21250997483730316,\n",
       "  0.21277369558811188,\n",
       "  0.2077920138835907,\n",
       "  0.2150748372077942,\n",
       "  0.21106980741024017,\n",
       "  0.2287043333053589,\n",
       "  0.21119581162929535,\n",
       "  0.2072901576757431,\n",
       "  0.19948478043079376,\n",
       "  0.34613528847694397,\n",
       "  0.20316006243228912,\n",
       "  0.2036103755235672,\n",
       "  0.21902501583099365,\n",
       "  0.21005889773368835,\n",
       "  0.20819047093391418,\n",
       "  0.20674309134483337,\n",
       "  0.18768049776554108,\n",
       "  0.19706881046295166,\n",
       "  0.2162507176399231,\n",
       "  0.20422837138175964,\n",
       "  0.20781022310256958,\n",
       "  0.19657720625400543,\n",
       "  0.20515063405036926,\n",
       "  0.18720364570617676,\n",
       "  0.2151513397693634,\n",
       "  0.23648962378501892,\n",
       "  0.23765355348587036,\n",
       "  0.3327435851097107,\n",
       "  0.20800398290157318,\n",
       "  0.20257756114006042,\n",
       "  0.26493126153945923,\n",
       "  0.21858927607536316,\n",
       "  0.1903577744960785,\n",
       "  0.1865188181400299,\n",
       "  0.1852884292602539,\n",
       "  0.191438689827919,\n",
       "  0.2098381519317627,\n",
       "  0.19587342441082,\n",
       "  0.21104593575000763,\n",
       "  0.18258509039878845,\n",
       "  0.1937934309244156,\n",
       "  0.22839218378067017,\n",
       "  0.5079969763755798,\n",
       "  0.17165222764015198,\n",
       "  0.23801636695861816,\n",
       "  0.20342673361301422,\n",
       "  0.19495008885860443,\n",
       "  0.20118913054466248,\n",
       "  0.2969740927219391,\n",
       "  0.2435147762298584,\n",
       "  0.20164069533348083,\n",
       "  0.17651285231113434,\n",
       "  0.1734086275100708,\n",
       "  0.18782587349414825,\n",
       "  0.19471073150634766,\n",
       "  0.26741063594818115,\n",
       "  0.2016155868768692,\n",
       "  0.20375894010066986,\n",
       "  0.2071574330329895,\n",
       "  0.3158136010169983,\n",
       "  0.20359106361865997,\n",
       "  0.42310628294944763,\n",
       "  0.2715733051300049,\n",
       "  0.20794245600700378,\n",
       "  0.17284083366394043,\n",
       "  0.18597427010536194,\n",
       "  0.19137036800384521,\n",
       "  0.17669318616390228,\n",
       "  0.19967232644557953,\n",
       "  0.2331736981868744,\n",
       "  0.19160914421081543,\n",
       "  0.2115519940853119,\n",
       "  0.17488855123519897,\n",
       "  0.27708905935287476,\n",
       "  0.18430614471435547,\n",
       "  0.1764879673719406,\n",
       "  0.1883719265460968,\n",
       "  0.1676999032497406,\n",
       "  0.16593267023563385,\n",
       "  0.248806431889534,\n",
       "  0.17356866598129272,\n",
       "  0.25390514731407166,\n",
       "  0.19760945439338684,\n",
       "  0.1785009652376175,\n",
       "  0.18331754207611084,\n",
       "  0.17509034276008606,\n",
       "  0.18691962957382202,\n",
       "  0.17958980798721313,\n",
       "  0.1912946254014969,\n",
       "  0.19335989654064178,\n",
       "  0.19382299482822418,\n",
       "  0.17454968392848969,\n",
       "  0.18561504781246185,\n",
       "  0.1631152629852295,\n",
       "  0.1698051244020462,\n",
       "  0.18088391423225403,\n",
       "  0.16621294617652893,\n",
       "  0.20192284882068634,\n",
       "  0.1764485388994217,\n",
       "  0.20364435017108917,\n",
       "  0.208380788564682,\n",
       "  0.18271788954734802,\n",
       "  0.15642285346984863,\n",
       "  0.1996181458234787,\n",
       "  0.19138723611831665,\n",
       "  0.2865357995033264,\n",
       "  0.1560802310705185,\n",
       "  0.1648329347372055,\n",
       "  0.16971184313297272,\n",
       "  0.18857476115226746,\n",
       "  0.1859803944826126,\n",
       "  0.1942639946937561,\n",
       "  0.16074617207050323,\n",
       "  0.37337443232536316,\n",
       "  0.5463047623634338,\n",
       "  0.1928316205739975,\n",
       "  0.44046491384506226,\n",
       "  0.17117825150489807,\n",
       "  0.18524838984012604,\n",
       "  0.1782849282026291,\n",
       "  0.1899556666612625,\n",
       "  0.21004848182201385,\n",
       "  0.1827230602502823,\n",
       "  0.18873760104179382,\n",
       "  0.17714978754520416,\n",
       "  0.2566908299922943,\n",
       "  0.16766110062599182,\n",
       "  0.1914312243461609,\n",
       "  0.19556982815265656,\n",
       "  0.18636471033096313,\n",
       "  0.18908512592315674,\n",
       "  0.17204274237155914,\n",
       "  0.1617206335067749,\n",
       "  0.3683339059352875,\n",
       "  0.20351935923099518,\n",
       "  0.19251877069473267,\n",
       "  0.175886332988739,\n",
       "  0.33114534616470337,\n",
       "  0.19405369460582733,\n",
       "  0.18127864599227905,\n",
       "  0.18100067973136902,\n",
       "  0.22672413289546967,\n",
       "  0.16490285098552704,\n",
       "  0.20281141996383667,\n",
       "  0.1681615263223648,\n",
       "  0.19012333452701569,\n",
       "  0.1698216050863266,\n",
       "  0.2996067702770233,\n",
       "  0.15783239901065826,\n",
       "  0.24738872051239014,\n",
       "  0.2008555680513382,\n",
       "  0.20542068779468536,\n",
       "  0.20637597143650055,\n",
       "  0.16538555920124054,\n",
       "  0.17171691358089447,\n",
       "  0.16677390038967133,\n",
       "  0.17930898070335388,\n",
       "  0.1812051236629486,\n",
       "  0.2724097967147827,\n",
       "  0.17784692347049713,\n",
       "  0.2326612025499344,\n",
       "  0.1729668378829956,\n",
       "  0.1827010065317154,\n",
       "  0.26558560132980347,\n",
       "  0.26212239265441895,\n",
       "  0.1697222888469696,\n",
       "  0.17155063152313232,\n",
       "  0.1788734644651413,\n",
       "  0.1694961041212082,\n",
       "  0.2074303776025772,\n",
       "  0.6179946064949036,\n",
       "  0.17159855365753174,\n",
       "  0.1684361845254898,\n",
       "  0.23477597534656525,\n",
       "  0.17254194617271423,\n",
       "  0.17150482535362244,\n",
       "  0.17736077308654785,\n",
       "  0.15150637924671173,\n",
       "  0.17470288276672363,\n",
       "  0.15748979151248932,\n",
       "  0.19234949350357056,\n",
       "  0.15489526093006134,\n",
       "  0.423318475484848,\n",
       "  0.25623202323913574,\n",
       "  0.28094395995140076,\n",
       "  0.16791203618049622,\n",
       "  0.15437345206737518,\n",
       "  0.1718725711107254,\n",
       "  0.17978450655937195,\n",
       "  0.1725984811782837,\n",
       "  0.16605545580387115,\n",
       "  0.36679694056510925,\n",
       "  0.3126431703567505,\n",
       "  0.27942389249801636,\n",
       "  0.1903122216463089,\n",
       "  0.1587032526731491,\n",
       "  0.17539076507091522,\n",
       "  0.15983231365680695,\n",
       "  0.17451457679271698,\n",
       "  0.16921988129615784,\n",
       "  0.16258639097213745,\n",
       "  0.31078946590423584,\n",
       "  0.19440995156764984,\n",
       "  0.23860083520412445,\n",
       "  0.19484519958496094,\n",
       "  0.1721712350845337,\n",
       "  0.1663777381181717,\n",
       "  0.21106462180614471,\n",
       "  0.17317481338977814,\n",
       "  0.20937713980674744,\n",
       "  0.18887388706207275,\n",
       "  0.18678653240203857,\n",
       "  0.1665838658809662,\n",
       "  0.1867108941078186,\n",
       "  0.19083788990974426,\n",
       "  0.16537947952747345,\n",
       "  0.15079331398010254,\n",
       "  0.30621883273124695,\n",
       "  0.16215021908283234,\n",
       "  0.29232069849967957,\n",
       "  0.1523667573928833,\n",
       "  0.17061732709407806,\n",
       "  0.18118441104888916,\n",
       "  0.17214031517505646,\n",
       "  0.15893399715423584,\n",
       "  0.191366508603096,\n",
       "  0.1621137112379074,\n",
       "  0.16367056965827942,\n",
       "  0.17880934476852417,\n",
       "  0.20532935857772827,\n",
       "  0.1659759283065796,\n",
       "  0.19019170105457306,\n",
       "  0.16602429747581482,\n",
       "  0.1771836131811142,\n",
       "  0.40004733204841614,\n",
       "  0.15770098567008972,\n",
       "  0.17516903579235077,\n",
       "  0.16038253903388977,\n",
       "  0.25413617491722107,\n",
       "  0.20023302733898163,\n",
       "  0.32864511013031006,\n",
       "  0.13843578100204468,\n",
       "  0.17816723883152008,\n",
       "  0.19769683480262756,\n",
       "  0.1465432494878769,\n",
       "  0.1838153898715973,\n",
       "  0.20140329003334045,\n",
       "  0.7771470546722412,\n",
       "  0.2940278947353363,\n",
       "  0.1734134554862976,\n",
       "  0.1347387433052063,\n",
       "  0.18415726721286774,\n",
       "  0.14537976682186127,\n",
       "  0.16528792679309845,\n",
       "  0.815037727355957,\n",
       "  0.15167413651943207,\n",
       "  0.16382171213626862,\n",
       "  0.20748797059059143,\n",
       "  0.18718580901622772,\n",
       "  0.17621968686580658,\n",
       "  0.1708637923002243,\n",
       "  0.18160271644592285,\n",
       "  0.1841743439435959,\n",
       "  0.1608242243528366,\n",
       "  0.2055438905954361,\n",
       "  0.20135337114334106,\n",
       "  0.1421993523836136,\n",
       "  0.1903129369020462,\n",
       "  0.1858581304550171,\n",
       "  0.1661887764930725,\n",
       "  0.19487322866916656,\n",
       "  0.16646741330623627,\n",
       "  0.16873013973236084,\n",
       "  0.168082594871521,\n",
       "  0.1875564306974411,\n",
       "  0.13517017662525177,\n",
       "  0.17652598023414612,\n",
       "  0.20253723859786987,\n",
       "  0.16128644347190857,\n",
       "  0.1601700335741043,\n",
       "  0.1844761222600937,\n",
       "  0.1929377168416977,\n",
       "  0.14188066124916077,\n",
       "  0.7124675512313843,\n",
       "  0.15623360872268677,\n",
       "  0.21671783924102783,\n",
       "  0.1453455537557602,\n",
       "  0.15269486606121063,\n",
       "  0.17566274106502533,\n",
       "  0.15703201293945312,\n",
       "  0.20868360996246338,\n",
       "  0.19194014370441437,\n",
       "  0.14771206676959991,\n",
       "  0.1710176318883896,\n",
       "  0.19196902215480804,\n",
       "  0.1687002331018448,\n",
       "  0.17121532559394836,\n",
       "  0.15638841688632965,\n",
       "  0.3333378732204437,\n",
       "  0.1868603527545929,\n",
       "  0.15277494490146637,\n",
       "  0.1356271356344223,\n",
       "  0.14595867693424225,\n",
       "  0.13724207878112793,\n",
       "  0.13452470302581787,\n",
       "  0.1370173841714859,\n",
       "  0.13837634027004242,\n",
       "  0.15235352516174316,\n",
       "  0.3444410562515259,\n",
       "  0.16881610453128815,\n",
       "  0.1357976645231247,\n",
       "  0.15294982492923737,\n",
       "  0.22338663041591644,\n",
       "  0.15324249863624573,\n",
       "  0.3170253038406372,\n",
       "  0.1970340758562088,\n",
       "  0.17558497190475464,\n",
       "  0.3244015872478485,\n",
       "  0.1334473341703415,\n",
       "  0.18536314368247986,\n",
       "  0.1455744355916977,\n",
       "  0.21624702215194702,\n",
       "  0.15950356423854828,\n",
       "  0.35667189955711365,\n",
       "  0.17623716592788696,\n",
       "  0.22155873477458954,\n",
       "  0.18107037246227264,\n",
       "  0.14466655254364014,\n",
       "  0.18425865471363068,\n",
       "  0.17192143201828003,\n",
       "  0.14770863950252533,\n",
       "  0.9102975726127625,\n",
       "  0.13991262018680573,\n",
       "  0.13505002856254578,\n",
       "  0.21960793435573578,\n",
       "  0.1683032363653183,\n",
       "  0.16949966549873352,\n",
       "  5.087327480316162,\n",
       "  0.12987488508224487,\n",
       "  0.1264018714427948,\n",
       "  0.17317679524421692,\n",
       "  0.15425734221935272,\n",
       "  0.15405316650867462,\n",
       "  0.16022087633609772,\n",
       "  0.18531230092048645,\n",
       "  0.48869916796684265,\n",
       "  0.13703711330890656,\n",
       "  0.29961249232292175,\n",
       "  0.1409865915775299,\n",
       "  0.1642031967639923,\n",
       "  0.16553305089473724,\n",
       "  0.18247613310813904,\n",
       "  0.17479799687862396,\n",
       "  0.15669606626033783,\n",
       "  0.538618266582489,\n",
       "  0.1553923338651657,\n",
       "  0.17569047212600708,\n",
       "  0.13652403652668,\n",
       "  0.13987642526626587,\n",
       "  0.14942486584186554,\n",
       "  0.15411800146102905,\n",
       "  0.15900200605392456,\n",
       "  0.17991885542869568,\n",
       "  0.21153834462165833,\n",
       "  0.1469448357820511,\n",
       "  0.15381352603435516,\n",
       "  0.14719462394714355,\n",
       "  0.16324318945407867,\n",
       "  0.1418927162885666,\n",
       "  0.1380453109741211,\n",
       "  0.18796572089195251,\n",
       "  0.17635172605514526,\n",
       "  0.14655570685863495,\n",
       "  0.33597108721733093,\n",
       "  0.17434389889240265,\n",
       "  0.17661374807357788,\n",
       "  0.14477147161960602,\n",
       "  0.18662838637828827,\n",
       "  0.15487544238567352,\n",
       "  0.17484687268733978,\n",
       "  0.276015043258667,\n",
       "  0.1416226625442505,\n",
       "  0.15589898824691772,\n",
       "  0.1573847532272339,\n",
       "  0.22748446464538574,\n",
       "  0.2230347990989685,\n",
       "  0.15520800650119781,\n",
       "  0.18141555786132812,\n",
       "  0.16327713429927826,\n",
       "  0.21116632223129272,\n",
       "  0.16354061663150787,\n",
       "  0.15937379002571106,\n",
       "  0.17979282140731812,\n",
       "  0.151481494307518,\n",
       "  0.17175093293190002,\n",
       "  0.1632636934518814,\n",
       "  0.1462174952030182,\n",
       "  0.1592375636100769,\n",
       "  0.14778344333171844,\n",
       "  0.15217268466949463,\n",
       "  0.15189622342586517,\n",
       "  0.12890973687171936,\n",
       "  0.1583683043718338,\n",
       "  0.22993332147598267,\n",
       "  0.14562870562076569,\n",
       "  0.2550318241119385,\n",
       "  2.406146287918091,\n",
       "  0.16189229488372803,\n",
       "  0.17727428674697876,\n",
       "  0.14366954565048218,\n",
       "  0.1626913845539093,\n",
       "  0.3682377338409424,\n",
       "  0.17636355757713318,\n",
       "  0.1658308357000351,\n",
       "  0.14447052776813507,\n",
       "  0.13395951688289642,\n",
       "  0.16417762637138367,\n",
       "  0.18602421879768372,\n",
       "  0.15338996052742004,\n",
       "  0.15003950893878937,\n",
       "  0.1605600267648697,\n",
       "  0.1419820338487625,\n",
       "  0.2283499389886856,\n",
       "  0.1604231745004654,\n",
       "  0.13820292055606842,\n",
       "  0.171444371342659,\n",
       "  0.14723455905914307,\n",
       "  0.13899970054626465,\n",
       "  0.14567303657531738,\n",
       "  0.16239190101623535,\n",
       "  0.27871692180633545,\n",
       "  0.1736464649438858,\n",
       "  0.14970877766609192,\n",
       "  0.20698633790016174,\n",
       "  0.14403845369815826,\n",
       "  0.16492348909378052,\n",
       "  0.16647955775260925,\n",
       "  0.13563287258148193,\n",
       "  0.15992356836795807,\n",
       "  0.19655954837799072,\n",
       "  0.15062355995178223,\n",
       "  0.15269887447357178,\n",
       "  0.1739310622215271,\n",
       "  0.17363281548023224,\n",
       "  0.14230085909366608,\n",
       "  0.15478192269802094,\n",
       "  0.1359999030828476,\n",
       "  0.15846526622772217,\n",
       "  0.15471573173999786,\n",
       "  0.15822631120681763,\n",
       "  0.15698391199111938,\n",
       "  0.22398126125335693,\n",
       "  0.3040405809879303,\n",
       "  0.17068935930728912,\n",
       "  0.15686801075935364,\n",
       "  0.15074145793914795,\n",
       "  0.16263195872306824,\n",
       "  0.16796919703483582,\n",
       "  0.12795397639274597,\n",
       "  0.14226317405700684,\n",
       "  0.14426004886627197,\n",
       "  0.1416521668434143,\n",
       "  0.29153305292129517,\n",
       "  0.1621362566947937,\n",
       "  0.16810785233974457,\n",
       "  0.22103483974933624,\n",
       "  0.1685158908367157,\n",
       "  0.1453477442264557,\n",
       "  0.4579378366470337,\n",
       "  0.15023943781852722,\n",
       "  0.2430194765329361,\n",
       "  0.17987105250358582,\n",
       "  0.12435644865036011,\n",
       "  0.1666710376739502,\n",
       "  0.1460493505001068,\n",
       "  0.15462110936641693,\n",
       "  0.47274038195610046,\n",
       "  0.14724159240722656,\n",
       "  0.1529417335987091,\n",
       "  0.1336313933134079,\n",
       "  0.19507266581058502,\n",
       "  0.37329474091529846,\n",
       "  0.1509726643562317,\n",
       "  0.16282978653907776,\n",
       "  0.22488604485988617,\n",
       "  0.13928616046905518,\n",
       "  0.20553763210773468,\n",
       "  0.13998813927173615,\n",
       "  0.17110224068164825,\n",
       "  0.17071878910064697,\n",
       "  0.22059091925621033,\n",
       "  0.14428220689296722,\n",
       "  0.19779691100120544,\n",
       "  0.18560735881328583,\n",
       "  0.14018645882606506,\n",
       "  0.1565287858247757,\n",
       "  0.1772022247314453,\n",
       "  0.1762075424194336,\n",
       "  0.13843846321105957,\n",
       "  0.17533829808235168,\n",
       "  0.1753753423690796,\n",
       "  0.16208608448505402,\n",
       "  0.13436175882816315,\n",
       "  0.13776999711990356,\n",
       "  0.2425699383020401,\n",
       "  0.15455178916454315,\n",
       "  0.16741269826889038,\n",
       "  0.16895289719104767,\n",
       "  0.1668425053358078,\n",
       "  0.1660265475511551,\n",
       "  0.1394597887992859,\n",
       "  0.15380924940109253,\n",
       "  0.15822696685791016,\n",
       "  0.16918782889842987,\n",
       "  0.21145673096179962,\n",
       "  0.15668803453445435,\n",
       "  0.1511601358652115,\n",
       "  0.21171987056732178,\n",
       "  0.2986908555030823,\n",
       "  0.17786255478858948,\n",
       "  0.19275812804698944,\n",
       "  0.1665261685848236,\n",
       "  0.16487184166908264,\n",
       "  0.16647107899188995,\n",
       "  0.36410877108573914,\n",
       "  0.17387017607688904,\n",
       "  0.5330901145935059,\n",
       "  0.18418167531490326,\n",
       "  0.14702744781970978,\n",
       "  0.14899525046348572,\n",
       "  0.11388838291168213,\n",
       "  0.1311935931444168,\n",
       "  0.1367562860250473,\n",
       "  0.1610436886548996,\n",
       "  0.5778980851173401,\n",
       "  0.41052308678627014,\n",
       "  0.13895177841186523,\n",
       "  0.10954263061285019,\n",
       "  0.782494306564331,\n",
       "  0.18911373615264893,\n",
       "  0.17705757915973663,\n",
       "  0.12858158349990845,\n",
       "  0.13970568776130676,\n",
       "  0.18829554319381714,\n",
       "  0.15673977136611938,\n",
       "  0.17559808492660522,\n",
       "  0.15492624044418335,\n",
       "  0.20779606699943542,\n",
       "  0.7864346504211426,\n",
       "  0.18889616429805756,\n",
       "  0.24454553425312042,\n",
       "  0.1317492425441742,\n",
       "  0.19129988551139832,\n",
       "  0.21790748834609985,\n",
       "  0.18369713425636292,\n",
       "  0.2884901762008667,\n",
       "  0.16433101892471313,\n",
       "  0.15294116735458374,\n",
       "  0.15033338963985443,\n",
       "  0.20664405822753906,\n",
       "  0.13335992395877838,\n",
       "  0.1507997065782547,\n",
       "  0.15206597745418549,\n",
       "  0.15494880080223083,\n",
       "  0.17059916257858276,\n",
       "  0.12977637350559235,\n",
       "  0.15148301422595978,\n",
       "  0.1442343294620514,\n",
       "  0.16998408734798431,\n",
       "  0.13919714093208313,\n",
       "  0.1634122133255005,\n",
       "  0.22084137797355652,\n",
       "  0.17403410375118256,\n",
       "  0.13967959582805634,\n",
       "  0.14717063307762146,\n",
       "  0.2334664762020111,\n",
       "  0.30877265334129333,\n",
       "  0.14335335791110992,\n",
       "  0.17109103500843048,\n",
       "  0.17193804681301117,\n",
       "  0.13816608488559723,\n",
       "  0.182262122631073,\n",
       "  0.1354386955499649,\n",
       "  0.1470535844564438,\n",
       "  0.1414792388677597,\n",
       "  0.15876026451587677,\n",
       "  0.1521722674369812,\n",
       "  0.14046745002269745,\n",
       "  0.17073030769824982,\n",
       "  0.14758813381195068,\n",
       "  0.18157732486724854,\n",
       "  0.17551489174365997,\n",
       "  0.1457589864730835,\n",
       "  0.1215704008936882,\n",
       "  0.19023458659648895,\n",
       "  0.20404309034347534,\n",
       "  0.13843005895614624,\n",
       "  0.2185908555984497,\n",
       "  0.14957201480865479,\n",
       "  0.16034667193889618,\n",
       "  2.375732421875,\n",
       "  0.19644613564014435,\n",
       "  0.9631040692329407,\n",
       "  0.1668044626712799,\n",
       "  0.17739927768707275,\n",
       "  0.13012143969535828,\n",
       "  0.3655792474746704,\n",
       "  0.16149164736270905,\n",
       "  0.1861276477575302,\n",
       "  0.18223191797733307,\n",
       "  0.6624624729156494,\n",
       "  0.15599089860916138,\n",
       "  0.17707398533821106,\n",
       "  0.14432565867900848,\n",
       "  0.784600555896759,\n",
       "  0.1465827226638794,\n",
       "  0.17344486713409424,\n",
       "  0.1263502687215805,\n",
       "  0.11795088648796082,\n",
       "  0.19748835265636444,\n",
       "  0.18035656213760376,\n",
       "  0.11210650205612183,\n",
       "  0.1433844417333603,\n",
       "  0.1448771059513092,\n",
       "  0.12240499258041382,\n",
       "  0.17777392268180847,\n",
       "  0.13739468157291412,\n",
       "  0.26031365990638733,\n",
       "  0.23110291361808777,\n",
       "  0.1382889598608017,\n",
       "  0.1261458694934845,\n",
       "  0.1690637171268463,\n",
       "  0.15952257812023163,\n",
       "  0.14825809001922607,\n",
       "  0.15308497846126556,\n",
       "  0.18397076427936554,\n",
       "  0.23135727643966675,\n",
       "  0.19624623656272888,\n",
       "  0.17207077145576477,\n",
       "  0.1841304451227188,\n",
       "  0.16340696811676025,\n",
       "  0.49427664279937744,\n",
       "  0.46116700768470764,\n",
       "  0.1769389659166336,\n",
       "  0.18112826347351074,\n",
       "  0.17427781224250793,\n",
       "  0.1355561912059784,\n",
       "  0.13781750202178955,\n",
       "  0.13344740867614746,\n",
       "  0.15630801022052765,\n",
       "  0.19172360002994537,\n",
       "  0.22988736629486084,\n",
       "  0.24448856711387634,\n",
       "  0.19065596163272858,\n",
       "  0.15613333880901337,\n",
       "  0.19116811454296112,\n",
       "  0.1801006942987442,\n",
       "  0.13471508026123047,\n",
       "  0.21407879889011383,\n",
       "  0.15093404054641724,\n",
       "  0.1587400883436203,\n",
       "  0.17964254319667816,\n",
       "  0.16605046391487122,\n",
       "  0.1762780249118805,\n",
       "  0.1495036631822586,\n",
       "  0.21111148595809937,\n",
       "  0.18521085381507874,\n",
       "  0.15942971408367157,\n",
       "  0.18551567196846008,\n",
       "  0.17493504285812378,\n",
       "  0.20277033746242523,\n",
       "  0.15298312902450562,\n",
       "  0.16490110754966736,\n",
       "  0.15628211200237274,\n",
       "  0.1486128270626068,\n",
       "  0.16321027278900146,\n",
       "  0.14644230902194977,\n",
       "  0.15274475514888763,\n",
       "  0.13906943798065186,\n",
       "  0.15587982535362244,\n",
       "  0.3682137429714203,\n",
       "  0.17142482101917267,\n",
       "  0.16005444526672363,\n",
       "  0.2083740532398224,\n",
       "  0.21645167469978333,\n",
       "  0.17990240454673767,\n",
       "  0.1631832718849182,\n",
       "  0.1881943643093109,\n",
       "  0.15356844663619995,\n",
       "  0.16461744904518127,\n",
       "  0.14370371401309967,\n",
       "  0.15112459659576416,\n",
       "  0.15701711177825928,\n",
       "  0.23952192068099976,\n",
       "  0.16269735991954803,\n",
       "  0.19356289505958557,\n",
       "  0.2132663130760193,\n",
       "  0.18836577236652374,\n",
       "  0.4993947446346283,\n",
       "  0.1678926944732666,\n",
       "  0.13879600167274475,\n",
       "  0.1814889758825302,\n",
       "  0.14894326031208038,\n",
       "  0.13410380482673645,\n",
       "  0.15546932816505432,\n",
       "  0.13967444002628326,\n",
       "  0.18242329359054565,\n",
       "  0.15067021548748016,\n",
       "  0.19844812154769897,\n",
       "  0.13745465874671936,\n",
       "  0.1613912582397461,\n",
       "  0.1465746909379959],\n",
       " 'val_accuracy': [0.6110197305679321,\n",
       "  0.3856907784938812,\n",
       "  0.6658443212509155,\n",
       "  0.8358004093170166,\n",
       "  0.8377193212509155,\n",
       "  0.8390899300575256,\n",
       "  0.8555372953414917,\n",
       "  0.8412829041481018,\n",
       "  0.8382675647735596,\n",
       "  0.8854166865348816,\n",
       "  0.8950109481811523,\n",
       "  0.8782894611358643,\n",
       "  0.8333333134651184,\n",
       "  0.8662280440330505,\n",
       "  0.6381579041481018,\n",
       "  0.8555372953414917,\n",
       "  0.875,\n",
       "  0.8870614171028137,\n",
       "  0.8985745906829834,\n",
       "  0.8467653393745422,\n",
       "  0.8497806787490845,\n",
       "  0.6515899300575256,\n",
       "  0.8648574352264404,\n",
       "  0.8108552694320679,\n",
       "  0.8226425647735596,\n",
       "  0.8207237124443054,\n",
       "  0.8898026347160339,\n",
       "  0.8758223652839661,\n",
       "  0.8914473652839661,\n",
       "  0.8522478342056274,\n",
       "  0.8322368264198303,\n",
       "  0.8739035129547119,\n",
       "  0.8665021657943726,\n",
       "  0.8867872953414917,\n",
       "  0.8730811476707458,\n",
       "  0.8736293911933899,\n",
       "  0.8681469559669495,\n",
       "  0.8810306787490845,\n",
       "  0.8714364171028137,\n",
       "  0.890625,\n",
       "  0.9002193212509155,\n",
       "  0.8939144611358643,\n",
       "  0.8851425647735596,\n",
       "  0.8637609481811523,\n",
       "  0.8892543911933899,\n",
       "  0.8845943212509155,\n",
       "  0.9046052694320679,\n",
       "  0.827576756477356,\n",
       "  0.8972039222717285,\n",
       "  0.8782894611358643,\n",
       "  0.8739035129547119,\n",
       "  0.8807565569877625,\n",
       "  0.8821271657943726,\n",
       "  0.8985745906829834,\n",
       "  0.9111841917037964,\n",
       "  0.8966556787490845,\n",
       "  0.8810306787490845,\n",
       "  0.8856908082962036,\n",
       "  0.8837719559669495,\n",
       "  0.9013158082962036,\n",
       "  0.8941885828971863,\n",
       "  0.9128289222717285,\n",
       "  0.8711622953414917,\n",
       "  0.8629385828971863,\n",
       "  0.8741776347160339,\n",
       "  0.8360745906829834,\n",
       "  0.8854166865348816,\n",
       "  0.8878837823867798,\n",
       "  0.8253837823867798,\n",
       "  0.8692434430122375,\n",
       "  0.9037829041481018,\n",
       "  0.906798243522644,\n",
       "  0.9136512875556946,\n",
       "  0.9054276347160339,\n",
       "  0.8840460777282715,\n",
       "  0.8963815569877625,\n",
       "  0.8810306787490845,\n",
       "  0.9131030440330505,\n",
       "  0.8928179740905762,\n",
       "  0.8453947305679321,\n",
       "  0.828125,\n",
       "  0.9265350699424744,\n",
       "  0.8763706088066101,\n",
       "  0.8826754093170166,\n",
       "  0.9013158082962036,\n",
       "  0.8898026347160339,\n",
       "  0.8514254093170166,\n",
       "  0.8240131735801697,\n",
       "  0.8917214870452881,\n",
       "  0.9128289222717285,\n",
       "  0.9161184430122375,\n",
       "  0.8999451994895935,\n",
       "  0.8933662176132202,\n",
       "  0.8042762875556946,\n",
       "  0.8889802694320679,\n",
       "  0.90625,\n",
       "  0.8876096606254578,\n",
       "  0.8360745906829834,\n",
       "  0.8895285129547119,\n",
       "  0.844024121761322,\n",
       "  0.8634868264198303,\n",
       "  0.8834978342056274,\n",
       "  0.9210526347160339,\n",
       "  0.905701756477356,\n",
       "  0.9004934430122375,\n",
       "  0.9100877046585083,\n",
       "  0.8958333134651184,\n",
       "  0.8473135828971863,\n",
       "  0.8996710777282715,\n",
       "  0.8791118264198303,\n",
       "  0.9131030440330505,\n",
       "  0.8514254093170166,\n",
       "  0.9092653393745422,\n",
       "  0.9147478342056274,\n",
       "  0.9029605388641357,\n",
       "  0.921600878238678,\n",
       "  0.9240679740905762,\n",
       "  0.8733552694320679,\n",
       "  0.9210526347160339,\n",
       "  0.8344298005104065,\n",
       "  0.8884320259094238,\n",
       "  0.9078947305679321,\n",
       "  0.9024122953414917,\n",
       "  0.9133771657943726,\n",
       "  0.9081688523292542,\n",
       "  0.9166666865348816,\n",
       "  0.9026864171028137,\n",
       "  0.9081688523292542,\n",
       "  0.8972039222717285,\n",
       "  0.9152960777282715,\n",
       "  0.9046052694320679,\n",
       "  0.9265350699424744,\n",
       "  0.9194079041481018,\n",
       "  0.9111841917037964,\n",
       "  0.9205043911933899,\n",
       "  0.9046052694320679,\n",
       "  0.9144737124443054,\n",
       "  0.8983004093170166,\n",
       "  0.8840460777282715,\n",
       "  0.9141995906829834,\n",
       "  0.9317434430122375,\n",
       "  0.8966556787490845,\n",
       "  0.9150219559669495,\n",
       "  0.8314144611358643,\n",
       "  0.9339364171028137,\n",
       "  0.9248903393745422,\n",
       "  0.9174890518188477,\n",
       "  0.8952850699424744,\n",
       "  0.9051535129547119,\n",
       "  0.890625,\n",
       "  0.9300987124443054,\n",
       "  0.8297697305679321,\n",
       "  0.8155153393745422,\n",
       "  0.9002193212509155,\n",
       "  0.8070175647735596,\n",
       "  0.9174890518188477,\n",
       "  0.9037829041481018,\n",
       "  0.921326756477356,\n",
       "  0.90625,\n",
       "  0.8947368264198303,\n",
       "  0.9095394611358643,\n",
       "  0.9007675647735596,\n",
       "  0.9109100699424744,\n",
       "  0.8673245906829834,\n",
       "  0.9279056787490845,\n",
       "  0.9070723652839661,\n",
       "  0.9043311476707458,\n",
       "  0.9054276347160339,\n",
       "  0.9098135828971863,\n",
       "  0.9169408082962036,\n",
       "  0.9251644611358643,\n",
       "  0.8374451994895935,\n",
       "  0.8947368264198303,\n",
       "  0.9024122953414917,\n",
       "  0.9131030440330505,\n",
       "  0.8379934430122375,\n",
       "  0.9037829041481018,\n",
       "  0.9092653393745422,\n",
       "  0.906798243522644,\n",
       "  0.8856908082962036,\n",
       "  0.9309210777282715,\n",
       "  0.8980262875556946,\n",
       "  0.9169408082962036,\n",
       "  0.9037829041481018,\n",
       "  0.9180372953414917,\n",
       "  0.8294956088066101,\n",
       "  0.9273574352264404,\n",
       "  0.874451756477356,\n",
       "  0.8974780440330505,\n",
       "  0.9002193212509155,\n",
       "  0.8914473652839661,\n",
       "  0.9240679740905762,\n",
       "  0.9202302694320679,\n",
       "  0.922149121761322,\n",
       "  0.9177631735801697,\n",
       "  0.9092653393745422,\n",
       "  0.8569079041481018,\n",
       "  0.9133771657943726,\n",
       "  0.8788377046585083,\n",
       "  0.9191337823867798,\n",
       "  0.9081688523292542,\n",
       "  0.9004934430122375,\n",
       "  0.8637609481811523,\n",
       "  0.9166666865348816,\n",
       "  0.9185855388641357,\n",
       "  0.9191337823867798,\n",
       "  0.9246162176132202,\n",
       "  0.8958333134651184,\n",
       "  0.8114035129547119,\n",
       "  0.9114583134651184,\n",
       "  0.9232456088066101,\n",
       "  0.8519737124443054,\n",
       "  0.9172149300575256,\n",
       "  0.9169408082962036,\n",
       "  0.921600878238678,\n",
       "  0.9355811476707458,\n",
       "  0.9131030440330505,\n",
       "  0.9292762875556946,\n",
       "  0.9002193212509155,\n",
       "  0.9262609481811523,\n",
       "  0.8486841917037964,\n",
       "  0.8717105388641357,\n",
       "  0.8859649300575256,\n",
       "  0.9205043911933899,\n",
       "  0.9276315569877625,\n",
       "  0.9117324352264404,\n",
       "  0.9163925647735596,\n",
       "  0.9240679740905762,\n",
       "  0.9166666865348816,\n",
       "  0.8330591917037964,\n",
       "  0.8566337823867798,\n",
       "  0.8739035129547119,\n",
       "  0.9073464870452881,\n",
       "  0.9309210777282715,\n",
       "  0.9010416865348816,\n",
       "  0.9273574352264404,\n",
       "  0.9169408082962036,\n",
       "  0.9273574352264404,\n",
       "  0.9240679740905762,\n",
       "  0.8212719559669495,\n",
       "  0.906524121761322,\n",
       "  0.8453947305679321,\n",
       "  0.9004934430122375,\n",
       "  0.9207785129547119,\n",
       "  0.9183114171028137,\n",
       "  0.8922697305679321,\n",
       "  0.922149121761322,\n",
       "  0.8952850699424744,\n",
       "  0.9147478342056274,\n",
       "  0.9098135828971863,\n",
       "  0.9276315569877625,\n",
       "  0.9084429740905762,\n",
       "  0.8961074352264404,\n",
       "  0.9257127046585083,\n",
       "  0.9322916865348816,\n",
       "  0.8733552694320679,\n",
       "  0.9311951994895935,\n",
       "  0.8552631735801697,\n",
       "  0.9322916865348816,\n",
       "  0.9141995906829834,\n",
       "  0.9147478342056274,\n",
       "  0.9174890518188477,\n",
       "  0.9237938523292542,\n",
       "  0.9078947305679321,\n",
       "  0.9240679740905762,\n",
       "  0.9342105388641357,\n",
       "  0.9152960777282715,\n",
       "  0.8985745906829834,\n",
       "  0.921326756477356,\n",
       "  0.9117324352264404,\n",
       "  0.9262609481811523,\n",
       "  0.9155701994895935,\n",
       "  0.8322368264198303,\n",
       "  0.9254385828971863,\n",
       "  0.9158443212509155,\n",
       "  0.9248903393745422,\n",
       "  0.8788377046585083,\n",
       "  0.905975878238678,\n",
       "  0.8429276347160339,\n",
       "  0.9413377046585083,\n",
       "  0.9161184430122375,\n",
       "  0.9073464870452881,\n",
       "  0.9344846606254578,\n",
       "  0.9161184430122375,\n",
       "  0.9070723652839661,\n",
       "  0.8215460777282715,\n",
       "  0.8522478342056274,\n",
       "  0.9188596606254578,\n",
       "  0.9388706088066101,\n",
       "  0.9100877046585083,\n",
       "  0.937774121761322,\n",
       "  0.9210526347160339,\n",
       "  0.7039473652839661,\n",
       "  0.9333881735801697,\n",
       "  0.9292762875556946,\n",
       "  0.8648574352264404,\n",
       "  0.9131030440330505,\n",
       "  0.9147478342056274,\n",
       "  0.9205043911933899,\n",
       "  0.9163925647735596,\n",
       "  0.9141995906829834,\n",
       "  0.9259868264198303,\n",
       "  0.8939144611358643,\n",
       "  0.9015899300575256,\n",
       "  0.9342105388641357,\n",
       "  0.9076206088066101,\n",
       "  0.9103618264198303,\n",
       "  0.9257127046585083,\n",
       "  0.9029605388641357,\n",
       "  0.9226973652839661,\n",
       "  0.9183114171028137,\n",
       "  0.9180372953414917,\n",
       "  0.9281798005104065,\n",
       "  0.9410635828971863,\n",
       "  0.9169408082962036,\n",
       "  0.8947368264198303,\n",
       "  0.9287280440330505,\n",
       "  0.9306469559669495,\n",
       "  0.9161184430122375,\n",
       "  0.9051535129547119,\n",
       "  0.9449012875556946,\n",
       "  0.8448464870452881,\n",
       "  0.9265350699424744,\n",
       "  0.8958333134651184,\n",
       "  0.9407894611358643,\n",
       "  0.9350329041481018,\n",
       "  0.9188596606254578,\n",
       "  0.9279056787490845,\n",
       "  0.9018640518188477,\n",
       "  0.9098135828971863,\n",
       "  0.9355811476707458,\n",
       "  0.922423243522644,\n",
       "  0.8958333134651184,\n",
       "  0.9254385828971863,\n",
       "  0.9199561476707458,\n",
       "  0.9320175647735596,\n",
       "  0.8311403393745422,\n",
       "  0.9114583134651184,\n",
       "  0.9364035129547119,\n",
       "  0.9394188523292542,\n",
       "  0.9347587823867798,\n",
       "  0.9399670958518982,\n",
       "  0.9427083134651184,\n",
       "  0.9405153393745422,\n",
       "  0.9394188523292542,\n",
       "  0.9309210777282715,\n",
       "  0.8503289222717285,\n",
       "  0.9183114171028137,\n",
       "  0.9443530440330505,\n",
       "  0.9306469559669495,\n",
       "  0.8788377046585083,\n",
       "  0.9300987124443054,\n",
       "  0.8884320259094238,\n",
       "  0.9002193212509155,\n",
       "  0.9240679740905762,\n",
       "  0.8791118264198303,\n",
       "  0.9399670958518982,\n",
       "  0.9128289222717285,\n",
       "  0.9366776347160339,\n",
       "  0.8782894611358643,\n",
       "  0.9265350699424744,\n",
       "  0.8273026347160339,\n",
       "  0.9202302694320679,\n",
       "  0.9161184430122375,\n",
       "  0.9317434430122375,\n",
       "  0.9336622953414917,\n",
       "  0.9265350699424744,\n",
       "  0.9202302694320679,\n",
       "  0.9347587823867798,\n",
       "  0.6502193212509155,\n",
       "  0.9339364171028137,\n",
       "  0.9416118264198303,\n",
       "  0.9002193212509155,\n",
       "  0.9281798005104065,\n",
       "  0.9194079041481018,\n",
       "  0.5164473652839661,\n",
       "  0.9424341917037964,\n",
       "  0.9512061476707458,\n",
       "  0.9265350699424744,\n",
       "  0.9350329041481018,\n",
       "  0.9325658082962036,\n",
       "  0.9251644611358643,\n",
       "  0.9177631735801697,\n",
       "  0.7902960777282715,\n",
       "  0.9391447305679321,\n",
       "  0.8648574352264404,\n",
       "  0.9358552694320679,\n",
       "  0.921326756477356,\n",
       "  0.9257127046585083,\n",
       "  0.9174890518188477,\n",
       "  0.9309210777282715,\n",
       "  0.9281798005104065,\n",
       "  0.8226425647735596,\n",
       "  0.9251644611358643,\n",
       "  0.9114583134651184,\n",
       "  0.9451754093170166,\n",
       "  0.9429824352264404,\n",
       "  0.9394188523292542,\n",
       "  0.9311951994895935,\n",
       "  0.9320175647735596,\n",
       "  0.9169408082962036,\n",
       "  0.905975878238678,\n",
       "  0.936951756477356,\n",
       "  0.9295504093170166,\n",
       "  0.9399670958518982,\n",
       "  0.9295504093170166,\n",
       "  0.9383223652839661,\n",
       "  0.9410635828971863,\n",
       "  0.9133771657943726,\n",
       "  0.9076206088066101,\n",
       "  0.9388706088066101,\n",
       "  0.8382675647735596,\n",
       "  0.9169408082962036,\n",
       "  0.9237938523292542,\n",
       "  0.9418859481811523,\n",
       "  0.9158443212509155,\n",
       "  0.9311951994895935,\n",
       "  0.9188596606254578,\n",
       "  0.8837719559669495,\n",
       "  0.9383223652839661,\n",
       "  0.9331140518188477,\n",
       "  0.9292762875556946,\n",
       "  0.8739035129547119,\n",
       "  0.9078947305679321,\n",
       "  0.9336622953414917,\n",
       "  0.9169408082962036,\n",
       "  0.9270833134651184,\n",
       "  0.9004934430122375,\n",
       "  0.9292762875556946,\n",
       "  0.9325658082962036,\n",
       "  0.922149121761322,\n",
       "  0.9402412176132202,\n",
       "  0.9276315569877625,\n",
       "  0.9276315569877625,\n",
       "  0.9347587823867798,\n",
       "  0.9290021657943726,\n",
       "  0.9394188523292542,\n",
       "  0.9366776347160339,\n",
       "  0.9358552694320679,\n",
       "  0.9440789222717285,\n",
       "  0.9328399300575256,\n",
       "  0.9311951994895935,\n",
       "  0.9407894611358643,\n",
       "  0.8766447305679321,\n",
       "  0.6381579041481018,\n",
       "  0.9300987124443054,\n",
       "  0.922149121761322,\n",
       "  0.9440789222717285,\n",
       "  0.9273574352264404,\n",
       "  0.859923243522644,\n",
       "  0.9290021657943726,\n",
       "  0.9281798005104065,\n",
       "  0.9416118264198303,\n",
       "  0.9421600699424744,\n",
       "  0.9262609481811523,\n",
       "  0.9073464870452881,\n",
       "  0.9311951994895935,\n",
       "  0.9383223652839661,\n",
       "  0.9350329041481018,\n",
       "  0.9394188523292542,\n",
       "  0.8845943212509155,\n",
       "  0.9290021657943726,\n",
       "  0.9410635828971863,\n",
       "  0.922423243522644,\n",
       "  0.9443530440330505,\n",
       "  0.9424341917037964,\n",
       "  0.9385964870452881,\n",
       "  0.9246162176132202,\n",
       "  0.8686951994895935,\n",
       "  0.9287280440330505,\n",
       "  0.9355811476707458,\n",
       "  0.9375,\n",
       "  0.9416118264198303,\n",
       "  0.9320175647735596,\n",
       "  0.9306469559669495,\n",
       "  0.9432565569877625,\n",
       "  0.9366776347160339,\n",
       "  0.90625,\n",
       "  0.9394188523292542,\n",
       "  0.9388706088066101,\n",
       "  0.9235197305679321,\n",
       "  0.9284539222717285,\n",
       "  0.9391447305679321,\n",
       "  0.952576756477356,\n",
       "  0.9416118264198303,\n",
       "  0.9298245906829834,\n",
       "  0.9262609481811523,\n",
       "  0.936951756477356,\n",
       "  0.9268091917037964,\n",
       "  0.8873355388641357,\n",
       "  0.8516995906829834,\n",
       "  0.9226973652839661,\n",
       "  0.9290021657943726,\n",
       "  0.9314693212509155,\n",
       "  0.9279056787490845,\n",
       "  0.9262609481811523,\n",
       "  0.952850878238678,\n",
       "  0.9449012875556946,\n",
       "  0.9358552694320679,\n",
       "  0.9413377046585083,\n",
       "  0.8585526347160339,\n",
       "  0.9339364171028137,\n",
       "  0.9262609481811523,\n",
       "  0.8963815569877625,\n",
       "  0.9314693212509155,\n",
       "  0.9457237124443054,\n",
       "  0.8231908082962036,\n",
       "  0.9394188523292542,\n",
       "  0.906524121761322,\n",
       "  0.9265350699424744,\n",
       "  0.9481908082962036,\n",
       "  0.9418859481811523,\n",
       "  0.9440789222717285,\n",
       "  0.9391447305679321,\n",
       "  0.8552631735801697,\n",
       "  0.9416118264198303,\n",
       "  0.937774121761322,\n",
       "  0.9435306787490845,\n",
       "  0.9150219559669495,\n",
       "  0.8541666865348816,\n",
       "  0.937774121761322,\n",
       "  0.9325658082962036,\n",
       "  0.937225878238678,\n",
       "  0.9388706088066101,\n",
       "  0.905701756477356,\n",
       "  0.9446271657943726,\n",
       "  0.9251644611358643,\n",
       "  0.9295504093170166,\n",
       "  0.8966556787490845,\n",
       "  0.9394188523292542,\n",
       "  0.9254385828971863,\n",
       "  0.9237938523292542,\n",
       "  0.9424341917037964,\n",
       "  0.9333881735801697,\n",
       "  0.9257127046585083,\n",
       "  0.9284539222717285,\n",
       "  0.9424341917037964,\n",
       "  0.9251644611358643,\n",
       "  0.9237938523292542,\n",
       "  0.9306469559669495,\n",
       "  0.9410635828971863,\n",
       "  0.9443530440330505,\n",
       "  0.8919956088066101,\n",
       "  0.9457237124443054,\n",
       "  0.9325658082962036,\n",
       "  0.9259868264198303,\n",
       "  0.9257127046585083,\n",
       "  0.9287280440330505,\n",
       "  0.9410635828971863,\n",
       "  0.9402412176132202,\n",
       "  0.936951756477356,\n",
       "  0.9333881735801697,\n",
       "  0.9180372953414917,\n",
       "  0.9306469559669495,\n",
       "  0.937774121761322,\n",
       "  0.9295504093170166,\n",
       "  0.8327850699424744,\n",
       "  0.9306469559669495,\n",
       "  0.9270833134651184,\n",
       "  0.9413377046585083,\n",
       "  0.9432565569877625,\n",
       "  0.9259868264198303,\n",
       "  0.8665021657943726,\n",
       "  0.9438048005104065,\n",
       "  0.8199012875556946,\n",
       "  0.9259868264198303,\n",
       "  0.9490131735801697,\n",
       "  0.9325658082962036,\n",
       "  0.9506579041481018,\n",
       "  0.9588815569877625,\n",
       "  0.9427083134651184,\n",
       "  0.9290021657943726,\n",
       "  0.8503289222717285,\n",
       "  0.8327850699424744,\n",
       "  0.9435306787490845,\n",
       "  0.953125,\n",
       "  0.8377193212509155,\n",
       "  0.9133771657943726,\n",
       "  0.9284539222717285,\n",
       "  0.9487390518188477,\n",
       "  0.9503837823867798,\n",
       "  0.9177631735801697,\n",
       "  0.936951756477356,\n",
       "  0.9158443212509155,\n",
       "  0.9375,\n",
       "  0.9161184430122375,\n",
       "  0.8401864171028137,\n",
       "  0.9385964870452881,\n",
       "  0.9100877046585083,\n",
       "  0.9481908082962036,\n",
       "  0.9350329041481018,\n",
       "  0.9070723652839661,\n",
       "  0.937225878238678,\n",
       "  0.8777412176132202,\n",
       "  0.9424341917037964,\n",
       "  0.9410635828971863,\n",
       "  0.9443530440330505,\n",
       "  0.8991228342056274,\n",
       "  0.9429824352264404,\n",
       "  0.9344846606254578,\n",
       "  0.9498355388641357,\n",
       "  0.9353070259094238,\n",
       "  0.9355811476707458,\n",
       "  0.9438048005104065,\n",
       "  0.9328399300575256,\n",
       "  0.9413377046585083,\n",
       "  0.9407894611358643,\n",
       "  0.9443530440330505,\n",
       "  0.9355811476707458,\n",
       "  0.8977521657943726,\n",
       "  0.9333881735801697,\n",
       "  0.9443530440330505,\n",
       "  0.9484649300575256,\n",
       "  0.9161184430122375,\n",
       "  0.8952850699424744,\n",
       "  0.9407894611358643,\n",
       "  0.9292762875556946,\n",
       "  0.936951756477356,\n",
       "  0.9490131735801697,\n",
       "  0.9180372953414917,\n",
       "  0.9462719559669495,\n",
       "  0.937774121761322,\n",
       "  0.9462719559669495,\n",
       "  0.9248903393745422,\n",
       "  0.9402412176132202,\n",
       "  0.9413377046585083,\n",
       "  0.9246162176132202,\n",
       "  0.9399670958518982,\n",
       "  0.9487390518188477,\n",
       "  0.9438048005104065,\n",
       "  0.9421600699424744,\n",
       "  0.9517543911933899,\n",
       "  0.9418859481811523,\n",
       "  0.9317434430122375,\n",
       "  0.936951756477356,\n",
       "  0.9281798005104065,\n",
       "  0.9407894611358643,\n",
       "  0.937774121761322,\n",
       "  0.7642543911933899,\n",
       "  0.921600878238678,\n",
       "  0.796875,\n",
       "  0.9443530440330505,\n",
       "  0.9350329041481018,\n",
       "  0.9457237124443054,\n",
       "  0.8527960777282715,\n",
       "  0.9322916865348816,\n",
       "  0.9344846606254578,\n",
       "  0.9295504093170166,\n",
       "  0.828125,\n",
       "  0.9443530440330505,\n",
       "  0.9306469559669495,\n",
       "  0.9440789222717285,\n",
       "  0.8368969559669495,\n",
       "  0.9481908082962036,\n",
       "  0.922423243522644,\n",
       "  0.9443530440330505,\n",
       "  0.9495614171028137,\n",
       "  0.9314693212509155,\n",
       "  0.9311951994895935,\n",
       "  0.953125,\n",
       "  0.936951756477356,\n",
       "  0.9438048005104065,\n",
       "  0.9544956088066101,\n",
       "  0.8993969559669495,\n",
       "  0.9484649300575256,\n",
       "  0.8667762875556946,\n",
       "  0.8919956088066101,\n",
       "  0.9473684430122375,\n",
       "  0.9468201994895935,\n",
       "  0.9394188523292542,\n",
       "  0.9364035129547119,\n",
       "  0.9347587823867798,\n",
       "  0.9399670958518982,\n",
       "  0.9251644611358643,\n",
       "  0.9344846606254578,\n",
       "  0.9265350699424744,\n",
       "  0.9402412176132202,\n",
       "  0.9443530440330505,\n",
       "  0.9407894611358643,\n",
       "  0.8344298005104065,\n",
       "  0.844024121761322,\n",
       "  0.9248903393745422,\n",
       "  0.9407894611358643,\n",
       "  0.9273574352264404,\n",
       "  0.952850878238678,\n",
       "  0.9457237124443054,\n",
       "  0.9476425647735596,\n",
       "  0.9254385828971863,\n",
       "  0.9287280440330505,\n",
       "  0.9183114171028137,\n",
       "  0.8892543911933899,\n",
       "  0.9429824352264404,\n",
       "  0.9402412176132202,\n",
       "  0.9361293911933899,\n",
       "  0.9306469559669495,\n",
       "  0.9446271657943726,\n",
       "  0.9199561476707458,\n",
       "  0.9375,\n",
       "  0.9457237124443054,\n",
       "  0.9457237124443054,\n",
       "  0.937225878238678,\n",
       "  0.9479166865348816,\n",
       "  0.9457237124443054,\n",
       "  0.9276315569877625,\n",
       "  0.9298245906829834,\n",
       "  0.9470943212509155,\n",
       "  0.9350329041481018,\n",
       "  0.9462719559669495,\n",
       "  0.9446271657943726,\n",
       "  0.9407894611358643,\n",
       "  0.9416118264198303,\n",
       "  0.9512061476707458,\n",
       "  0.9498355388641357,\n",
       "  0.9410635828971863,\n",
       "  0.9375,\n",
       "  0.9366776347160339,\n",
       "  0.9413377046585083,\n",
       "  0.9418859481811523,\n",
       "  0.9199561476707458,\n",
       "  0.9435306787490845,\n",
       "  0.9358552694320679,\n",
       "  0.9235197305679321,\n",
       "  0.9202302694320679,\n",
       "  0.9344846606254578,\n",
       "  0.9407894611358643,\n",
       "  0.9405153393745422,\n",
       "  0.9454495906829834,\n",
       "  0.9358552694320679,\n",
       "  0.9432565569877625,\n",
       "  0.9427083134651184,\n",
       "  0.9451754093170166,\n",
       "  0.8972039222717285,\n",
       "  0.938048243522644,\n",
       "  0.9276315569877625,\n",
       "  0.9276315569877625,\n",
       "  0.9413377046585083,\n",
       "  0.8558114171028137,\n",
       "  0.9407894611358643,\n",
       "  0.953399121761322,\n",
       "  0.9350329041481018,\n",
       "  0.9509320259094238,\n",
       "  0.9506579041481018,\n",
       "  0.9446271657943726,\n",
       "  0.9454495906829834,\n",
       "  0.9273574352264404,\n",
       "  0.952576756477356,\n",
       "  0.9281798005104065,\n",
       "  0.9558662176132202,\n",
       "  0.9418859481811523,\n",
       "  0.9473684430122375]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "failing-encounter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3599 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1  2  3  4  5  6\n",
       "0     1  0  0  0  0  0\n",
       "1     1  0  0  0  0  0\n",
       "2     1  0  0  0  0  0\n",
       "3     1  0  0  0  0  0\n",
       "4     1  0  0  0  0  0\n",
       "...  .. .. .. .. .. ..\n",
       "3594  0  0  0  0  0  1\n",
       "3595  0  0  0  0  0  1\n",
       "3596  0  0  0  0  0  1\n",
       "3597  0  0  0  0  0  1\n",
       "3598  0  0  0  0  0  1\n",
       "\n",
       "[3599 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = df3.iloc[:,0:256]\n",
    "y1 = df3['target']\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X1 = scaler.fit_transform(X1)\n",
    "X1 = pd.DataFrame(X1)\n",
    "Y1 = pd.get_dummies(y1)\n",
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "better-blank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.25)\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "num_classes = len(np.unique(y_t))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "blind-methodology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(X[:])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, Y, test_size = 0.38,random_state = 120)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)\n",
    "\n",
    "input_shape=(x_train.shape[1], 1)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "federal-liberia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/25\n",
      "93/93 [==============================] - 63s 676ms/step - loss: 1.5381 - accuracy: 0.2887 - val_loss: 1.2293 - val_accuracy: 0.3405\n",
      "Epoch 2/25\n",
      "93/93 [==============================] - 60s 641ms/step - loss: 1.1753 - accuracy: 0.4025 - val_loss: 1.1317 - val_accuracy: 0.4400\n",
      "Epoch 3/25\n",
      "93/93 [==============================] - 60s 643ms/step - loss: 1.0614 - accuracy: 0.4650 - val_loss: 1.0040 - val_accuracy: 0.4825\n",
      "Epoch 4/25\n",
      "93/93 [==============================] - 60s 650ms/step - loss: 0.8960 - accuracy: 0.5318 - val_loss: 0.7712 - val_accuracy: 0.6009\n",
      "Epoch 5/25\n",
      "93/93 [==============================] - 59s 635ms/step - loss: 0.7793 - accuracy: 0.5951 - val_loss: 0.7347 - val_accuracy: 0.5743\n",
      "Epoch 6/25\n",
      "93/93 [==============================] - 60s 647ms/step - loss: 0.6791 - accuracy: 0.6563 - val_loss: 0.5366 - val_accuracy: 0.7670\n",
      "Epoch 7/25\n",
      "93/93 [==============================] - 59s 636ms/step - loss: 0.6010 - accuracy: 0.7069 - val_loss: 0.5908 - val_accuracy: 0.6774\n",
      "Epoch 8/25\n",
      "93/93 [==============================] - 59s 640ms/step - loss: 0.5499 - accuracy: 0.7286 - val_loss: 0.3882 - val_accuracy: 0.8388\n",
      "Epoch 9/25\n",
      "93/93 [==============================] - 60s 645ms/step - loss: 0.4788 - accuracy: 0.7765 - val_loss: 0.3290 - val_accuracy: 0.8964\n",
      "Epoch 10/25\n",
      "93/93 [==============================] - 61s 651ms/step - loss: 0.3877 - accuracy: 0.8323 - val_loss: 0.2955 - val_accuracy: 0.8909\n",
      "Epoch 11/25\n",
      "93/93 [==============================] - 57s 612ms/step - loss: 0.2992 - accuracy: 0.8822 - val_loss: 0.1659 - val_accuracy: 0.9583\n",
      "Epoch 12/25\n",
      "93/93 [==============================] - 59s 636ms/step - loss: 0.2407 - accuracy: 0.9139 - val_loss: 0.0960 - val_accuracy: 0.9726\n",
      "Epoch 13/25\n",
      "93/93 [==============================] - 60s 646ms/step - loss: 0.1859 - accuracy: 0.9311 - val_loss: 0.1047 - val_accuracy: 0.9619\n",
      "Epoch 14/25\n",
      "93/93 [==============================] - 66s 704ms/step - loss: 0.1717 - accuracy: 0.9415 - val_loss: 0.0757 - val_accuracy: 0.9770\n",
      "Epoch 15/25\n",
      "93/93 [==============================] - 60s 646ms/step - loss: 0.1169 - accuracy: 0.9590 - val_loss: 0.0813 - val_accuracy: 0.9778\n",
      "Epoch 16/25\n",
      "93/93 [==============================] - 60s 644ms/step - loss: 0.1055 - accuracy: 0.9652 - val_loss: 0.4394 - val_accuracy: 0.8734\n",
      "Epoch 17/25\n",
      "93/93 [==============================] - 60s 645ms/step - loss: 0.0980 - accuracy: 0.9699 - val_loss: 0.0433 - val_accuracy: 0.9888\n",
      "Epoch 18/25\n",
      "93/93 [==============================] - 60s 641ms/step - loss: 0.0858 - accuracy: 0.9708 - val_loss: 0.0567 - val_accuracy: 0.9841\n",
      "Epoch 19/25\n",
      "93/93 [==============================] - 60s 640ms/step - loss: 0.0843 - accuracy: 0.9746 - val_loss: 0.0344 - val_accuracy: 0.9912\n",
      "Epoch 20/25\n",
      "93/93 [==============================] - 59s 630ms/step - loss: 0.0786 - accuracy: 0.9768 - val_loss: 0.1210 - val_accuracy: 0.9638\n",
      "Epoch 21/25\n",
      "93/93 [==============================] - 59s 638ms/step - loss: 0.0604 - accuracy: 0.9797 - val_loss: 0.0660 - val_accuracy: 0.9827\n",
      "Epoch 22/25\n",
      "93/93 [==============================] - 59s 634ms/step - loss: 0.0587 - accuracy: 0.9812 - val_loss: 0.0444 - val_accuracy: 0.9877\n",
      "Epoch 23/25\n",
      "93/93 [==============================] - 59s 638ms/step - loss: 0.0584 - accuracy: 0.9832 - val_loss: 0.1210 - val_accuracy: 0.9627\n",
      "Epoch 24/25\n",
      "93/93 [==============================] - 60s 646ms/step - loss: 0.0503 - accuracy: 0.9822 - val_loss: 0.0429 - val_accuracy: 0.9893\n",
      "Epoch 25/25\n",
      "93/93 [==============================] - 59s 635ms/step - loss: 0.0533 - accuracy: 0.9829 - val_loss: 0.0255 - val_accuracy: 0.9942\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Conv1D(filters=32, kernel_size=3, input_shape=input_shape,activation='relu'))\n",
    "model4.add(MaxPooling1D(pool_size=2))\n",
    "model4.add(Conv1D(filters=32, kernel_size=10, padding='same', activation='relu'))\n",
    "model4.add(MaxPooling1D(pool_size=2))\n",
    "model4.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model4.add(Dense(32, activation='relu'))\n",
    "model4.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model4.fit(x_train, y_train, epochs=25, batch_size=64, validation_data=(x_test, y_test), callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "behind-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 254, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 127, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 127, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 63, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 67,030\n",
      "Trainable params: 67,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "periodic-harmony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34046053886413574,\n",
       " 0.4399670958518982,\n",
       " 0.48245614767074585,\n",
       " 0.6008771657943726,\n",
       " 0.5742872953414917,\n",
       " 0.7669956088066101,\n",
       " 0.6773574352264404,\n",
       " 0.8388158082962036,\n",
       " 0.8963815569877625,\n",
       " 0.890899121761322,\n",
       " 0.9583333134651184,\n",
       " 0.9725877046585083,\n",
       " 0.9618969559669495,\n",
       " 0.9769737124443054,\n",
       " 0.9777960777282715,\n",
       " 0.8733552694320679,\n",
       " 0.9887609481811523,\n",
       " 0.984100878238678,\n",
       " 0.9912280440330505,\n",
       " 0.9638158082962036,\n",
       " 0.9827302694320679,\n",
       " 0.9876644611358643,\n",
       " 0.9627193212509155,\n",
       " 0.9893091917037964,\n",
       " 0.9942434430122375]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "stuck-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "raising-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save(\"dataset2_model_02.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "searching-lafayette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.340639\n",
       "1      0.330110\n",
       "2      0.348269\n",
       "3      0.365160\n",
       "4      0.403843\n",
       "         ...   \n",
       "251    0.297630\n",
       "252    0.332080\n",
       "253    0.308504\n",
       "254    0.430463\n",
       "255    0.460746\n",
       "Name: 10, Length: 256, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "mediterranean-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34063853 0.33011035 0.34826892 0.36515992 0.40384296 0.39626589\n",
      "  0.39109334 0.39809021 0.38180488 0.39602334 0.37898977 0.35498589\n",
      "  0.44226363 0.38630667 0.4845772  0.45373149 0.49233897 0.47908132\n",
      "  0.54430341 0.48476895 0.57099941 0.48772668 0.51875065 0.48823564\n",
      "  0.47371193 0.44103919 0.4434554  0.47455567 0.40757766 0.48832206\n",
      "  0.46036441 0.46502866 0.38333671 0.4103904  0.35387508 0.35737256\n",
      "  0.27601677 0.28289307 0.25402367 0.19454737 0.13436359 0.08666306\n",
      "  0.08516352 0.03339238 0.02468999 0.0306051  0.06444829 0.06741453\n",
      "  0.11339657 0.14096606 0.16501333 0.18414779 0.19253059 0.2151681\n",
      "  0.22352349 0.28818034 0.24753775 0.29688503 0.27738963 0.30432573\n",
      "  0.30680502 0.34732708 0.33474407 0.34297939 0.34170355 0.3532642\n",
      "  0.40268606 0.3513168  0.36337804 0.38209761 0.38579482 0.36940509\n",
      "  0.37956968 0.40166954 0.39414668 0.37831889 0.41431842 0.37239522\n",
      "  0.45487028 0.46038122 0.51261692 0.56947777 0.52273285 0.48886256\n",
      "  0.49930205 0.52499338 0.57002806 0.48157125 0.46892104 0.48966713\n",
      "  0.45684522 0.44604969 0.4369048  0.41774612 0.3903093  0.44530334\n",
      "  0.39317955 0.33613235 0.36775763 0.32712147 0.29665487 0.27877562\n",
      "  0.23875832 0.17038718 0.11389192 0.10306466 0.03602943 0.03504564\n",
      "  0.03159795 0.02781251 0.03317062 0.06095671 0.0934438  0.12889774\n",
      "  0.18926448 0.20357609 0.19995623 0.26528971 0.25459396 0.23014434\n",
      "  0.24750826 0.27537232 0.27869833 0.30673647 0.29132396 0.31567056\n",
      "  0.31159034 0.32508615 0.3573972  0.3411279  0.34510836 0.38449896\n",
      "  0.37587873 0.38376001 0.40057341 0.39123876 0.40692329 0.39285235\n",
      "  0.38584362 0.3866976  0.43899761 0.39765342 0.4544634  0.45627161\n",
      "  0.48100715 0.48963965 0.49455984 0.56759856 0.46921524 0.49406671\n",
      "  0.50467577 0.47705069 0.48535452 0.45187768 0.43054339 0.47590645\n",
      "  0.49257493 0.44858778 0.40452736 0.41484237 0.43151946 0.36560722\n",
      "  0.36896024 0.29867716 0.30312615 0.35219668 0.18711244 0.16663447\n",
      "  0.11389324 0.07611122 0.03897077 0.02721316 0.0233174  0.02184403\n",
      "  0.04949743 0.08051664 0.11326146 0.13542955 0.16524375 0.17004632\n",
      "  0.21142702 0.23689393 0.22096872 0.29061782 0.29597774 0.30726363\n",
      "  0.28810344 0.28158642 0.30259065 0.2955958  0.3318239  0.35119341\n",
      "  0.29210434 0.33455142 0.37038184 0.36451928 0.3663302  0.35916257\n",
      "  0.378228   0.39227267 0.40515496 0.38564702 0.38645166 0.34969048\n",
      "  0.40091718 0.44451316 0.44519521 0.47044718 0.47810469 0.51570761\n",
      "  0.48322496 0.5010147  0.51384142 0.47972561 0.50537619 0.4728477\n",
      "  0.52404448 0.58677392 0.42556354 0.48506892 0.42763106 0.40764362\n",
      "  0.41791053 0.46541621 0.41997097 0.3563487  0.37313276 0.31814101\n",
      "  0.32147069 0.26426826 0.22351109 0.17464895 0.12252754 0.0913246\n",
      "  0.03653784 0.03441058 0.02304957 0.0238001  0.0334982  0.06306766\n",
      "  0.10226471 0.14943374 0.17059013 0.18511181 0.25933563 0.22738976\n",
      "  0.2225389  0.25660568 0.26074042 0.26860766 0.31072308 0.29763013\n",
      "  0.33207977 0.30850396 0.43046262 0.46074586]]\n",
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "single_test = X.iloc[10,:]\n",
    "single_test = single_test.values.reshape(1,256)\n",
    "print(single_test)\n",
    "print(single_test.shape)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "immune-eligibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 20,934\n",
      "Trainable params: 20,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "modelFromFile1 = load_model('dataset2_model_0.h5')\n",
    "modelFromFile1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "secondary-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-63-92743ebbd387>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "[0]\n",
      "Time taken was 0.29725193977355957 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(modelFromFile1.predict_classes(single_test))\n",
    "t2 = time.time()\n",
    "print( 'Time taken was {} seconds'.format( t2 - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "short-address",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 254, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 127, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 127, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 63, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 67,030\n",
      "Trainable params: 67,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "modelFromFile2 = load_model('dataset2_model_02.h5')\n",
    "modelFromFile2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "colored-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "X_train[1].shape\n",
    "single_test1 = x_train[1]\n",
    "single_test1 = single_test1.reshape(1,256)\n",
    "print(single_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "numeric-level",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = x_train[12].reshape(1,256,1)\n",
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "initial-original",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "Time taken was 0.3078761100769043 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(modelFromFile2.predict_classes(i))\n",
    "t2 = time.time()\n",
    "print( 'Time taken was {} seconds'.format( t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-albania",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
